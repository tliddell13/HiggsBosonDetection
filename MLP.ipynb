{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP testing notebook\n",
    "I trained the smaller models on a smaller amount of data to find optimum hyperparameters. Once the best settings were found I used this on bigger models trained with more data using a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(563200, 28)\n",
      "(140800, 28)\n",
      "(176000, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import MLPfunctions as mlp\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a dataframe from the csv file\n",
    "dataset = pd.read_csv('HIGGS_train.csv')\n",
    "# Make the dataset much smaller\n",
    "dataset_small = dataset.sample(frac=0.1)\n",
    "# Set aside a validation set\n",
    "data_val = dataset_small.sample(frac=0.2)\n",
    "dataset_small.drop(data_val.index, inplace=True)\n",
    "X_val = data_val.iloc[:, 1:].values\n",
    "y_val = data_val.iloc[:, 0].values\n",
    "# Split the data into features and labels\n",
    "X = dataset_small.iloc[:, 1:].values\n",
    "y = dataset_small.iloc[:, 0].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Print all the sizes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train,)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.6920670866966248, Test Loss: 0.738484799861908\n",
      "Epoch 2/1000, Train Loss: 0.7375934720039368, Test Loss: 0.784781813621521\n",
      "Epoch 3/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 4/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 5/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 6/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 7/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 8/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 9/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 10/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 11/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 12/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 13/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 14/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 15/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 16/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 17/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 18/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 19/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 20/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 21/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 22/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 23/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 24/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 25/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 26/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 27/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 28/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 29/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 30/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 31/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 32/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 33/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 34/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 35/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 36/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 37/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 38/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 39/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 40/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 41/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 42/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 43/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 44/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 45/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 46/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 47/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 48/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 49/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 50/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 51/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 52/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 53/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 54/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 55/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 56/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 57/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 58/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 59/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 60/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 61/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 62/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 63/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 64/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 65/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 66/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 67/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 68/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 69/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 70/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 71/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 72/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 73/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 74/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 75/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 76/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 77/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 78/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 79/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 80/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 81/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 82/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 83/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 84/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 85/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 86/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 87/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 88/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 89/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 90/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 91/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 92/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 93/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 94/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 95/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 96/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 97/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 98/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 99/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 100/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 101/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 102/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 103/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 104/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 105/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 106/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 107/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 108/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 109/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 110/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 111/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 112/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 113/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 114/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 115/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 116/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 117/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 118/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 119/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 120/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 121/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 122/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 123/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 124/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 125/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 126/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 127/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 128/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 129/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 130/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 131/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 132/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 133/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 134/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 135/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 136/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 137/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 138/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 139/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 140/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 141/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 142/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 143/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 144/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 145/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 146/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 147/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 148/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 149/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 150/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 151/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 152/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 153/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 154/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 155/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 156/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 157/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 158/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 159/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 160/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 161/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 162/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 163/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 164/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 165/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 166/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 167/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 168/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 169/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 170/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 171/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 172/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 173/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 174/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 175/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 176/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 177/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 178/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 179/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 180/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 181/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 182/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 183/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 184/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 185/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 186/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 187/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 188/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 189/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 190/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 191/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 192/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 193/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 194/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 195/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 196/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 197/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 198/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 199/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 200/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 201/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 202/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 203/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 204/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 205/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 206/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 207/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 208/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 209/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 210/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 211/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 212/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 213/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 214/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 215/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 216/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 217/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 218/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 219/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 220/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 221/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 222/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 223/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 224/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 225/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 226/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 227/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 228/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 229/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 230/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 231/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 232/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 233/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 234/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 235/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 236/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 237/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 238/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 239/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 240/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 241/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 242/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 243/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 244/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 245/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 246/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 247/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 248/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 249/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 250/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 251/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 252/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 253/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 254/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 255/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 256/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 257/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 258/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 259/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 260/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 261/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 262/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 263/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 264/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 265/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 266/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 267/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 268/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 269/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 270/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 271/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 272/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 273/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 274/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 275/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 276/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 277/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 278/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 279/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 280/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 281/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 282/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 283/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 284/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 285/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 286/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 287/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 288/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 289/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 290/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 291/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 292/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 293/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 294/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 295/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 296/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 297/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 298/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 299/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 300/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 301/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 302/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 303/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 304/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 305/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 306/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 307/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 308/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 309/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 310/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 311/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 312/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 313/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 314/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 315/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 316/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 317/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 318/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 319/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 320/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 321/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 322/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 323/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 324/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 325/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 326/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 327/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 328/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 329/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 330/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 331/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 332/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 333/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 334/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 335/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 336/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 337/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 338/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 339/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 340/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 341/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 342/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 343/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 344/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 345/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 346/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 347/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 348/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 349/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 350/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 351/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 352/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 353/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 354/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 355/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 356/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 357/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 358/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 359/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 360/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 361/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 362/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 363/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 364/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 365/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 366/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 367/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 368/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 369/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 370/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 371/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 372/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 373/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 374/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 375/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 376/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 377/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 378/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 379/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 380/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 381/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 382/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 383/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 384/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 385/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 386/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 387/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 388/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 389/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 390/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 391/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 392/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 393/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 394/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 395/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 396/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 397/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 398/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 399/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 400/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 401/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 402/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 403/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 404/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 405/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 406/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 407/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 408/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 409/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 410/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 411/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 412/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 413/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 414/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 415/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 416/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 417/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 418/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 419/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 420/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 421/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 422/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 423/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 424/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 425/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 426/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 427/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 428/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 429/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 430/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 431/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 432/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 433/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 434/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 435/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 436/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 437/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 438/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 439/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 440/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 441/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 442/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 443/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 444/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 445/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 446/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 447/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 448/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 449/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 450/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 451/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 452/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 453/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 454/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 455/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 456/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 457/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 458/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 459/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 460/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 461/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 462/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 463/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 464/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 465/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 466/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 467/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 468/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 469/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 470/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 471/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 472/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 473/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 474/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 475/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 476/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 477/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 478/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 479/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 480/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 481/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 482/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 483/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 484/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 485/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 486/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 487/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 488/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 489/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 490/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 491/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 492/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 493/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 494/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 495/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 496/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 497/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 498/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 499/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 500/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 501/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 502/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 503/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 504/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 505/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 506/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 507/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 508/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 509/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 510/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 511/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 512/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 513/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 514/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 515/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 516/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 517/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 518/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 519/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 520/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 521/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 522/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 523/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 524/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 525/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 526/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 527/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 528/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 529/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 530/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 531/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 532/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 533/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 534/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 535/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 536/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 537/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 538/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 539/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 540/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 541/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 542/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 543/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 544/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 545/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 546/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 547/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 548/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 549/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 550/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 551/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 552/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 553/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 554/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 555/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 556/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 557/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 558/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 559/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 560/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 561/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 562/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 563/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 564/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 565/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 566/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 567/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 568/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 569/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 570/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 571/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 572/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 573/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 574/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 575/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 576/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 577/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 578/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 579/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 580/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 581/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 582/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 583/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 584/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 585/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 586/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 587/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 588/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 589/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 590/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 591/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 592/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 593/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 594/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 595/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 596/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 597/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 598/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 599/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 600/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 601/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 602/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 603/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 604/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 605/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 606/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 607/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 608/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 609/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 610/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 611/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 612/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 613/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 614/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 615/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 616/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 617/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 618/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 619/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 620/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 621/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 622/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 623/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 624/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 625/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 626/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 627/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 628/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 629/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 630/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 631/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 632/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 633/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 634/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 635/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 636/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 637/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 638/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 639/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 640/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 641/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 642/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 643/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 644/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 645/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 646/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 647/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 648/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 649/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 650/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 651/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 652/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 653/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 654/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 655/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 656/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 657/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 658/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 659/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 660/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 661/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 662/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 663/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 664/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 665/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 666/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 667/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 668/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 669/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 670/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 671/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 672/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 673/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 674/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 675/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 676/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 677/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 678/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 679/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 680/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 681/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 682/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 683/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 684/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 685/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 686/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 687/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 688/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 689/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 690/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 691/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 692/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 693/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 694/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 695/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 696/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 697/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 698/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 699/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 700/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 701/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 702/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 703/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 704/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 705/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 706/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 707/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 708/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 709/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 710/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 711/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 712/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 713/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 714/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 715/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 716/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 717/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 718/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 719/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 720/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 721/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 722/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 723/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 724/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 725/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 726/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 727/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 728/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 729/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 730/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 731/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 732/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 733/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 734/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 735/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 736/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 737/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 738/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 739/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 740/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 741/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 742/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 743/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 744/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 745/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 746/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 747/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 748/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 749/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 750/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 751/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 752/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 753/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 754/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 755/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 756/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 757/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 758/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 759/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 760/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 761/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 762/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 763/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 764/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 765/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 766/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 767/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 768/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 769/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 770/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 771/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 772/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 773/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 774/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 775/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 776/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 777/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 778/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 779/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 780/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 781/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 782/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 783/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 784/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 785/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 786/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 787/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 788/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 789/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 790/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 791/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 792/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 793/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 794/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 795/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 796/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 797/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 798/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 799/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 800/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 801/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 802/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 803/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 804/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 805/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 806/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 807/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 808/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 809/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 810/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 811/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 812/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 813/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 814/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 815/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 816/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 817/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 818/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 819/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 820/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 821/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 822/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 823/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 824/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 825/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 826/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 827/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 828/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 829/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 830/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 831/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 832/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 833/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 834/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 835/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 836/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 837/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 838/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 839/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 840/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 841/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 842/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 843/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 844/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 845/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 846/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 847/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 848/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 849/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 850/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 851/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 852/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 853/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 854/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 855/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 856/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 857/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 858/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 859/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 860/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 861/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 862/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 863/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 864/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 865/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 866/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 867/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 868/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 869/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 870/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 871/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 872/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 873/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 874/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 875/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 876/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 877/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 878/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 879/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 880/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 881/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 882/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 883/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 884/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 885/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 886/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 887/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 888/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 889/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 890/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 891/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 892/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 893/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 894/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 895/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 896/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 897/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 898/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 899/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 900/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 901/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 902/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 903/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 904/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 905/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 906/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 907/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 908/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 909/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 910/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 911/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 912/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 913/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 914/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 915/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 916/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 917/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 918/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 919/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 920/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 921/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 922/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 923/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 924/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 925/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 926/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 927/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 928/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 929/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 930/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 931/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 932/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 933/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 934/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 935/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 936/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 937/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 938/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 939/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 940/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 941/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 942/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 943/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 944/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 945/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 946/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 947/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 948/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 949/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 950/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 951/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 952/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 953/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 954/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 955/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 956/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 957/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 958/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 959/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 960/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 961/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 962/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 963/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 964/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 965/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 966/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 967/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 968/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 969/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 970/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 971/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 972/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 973/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 974/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 975/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 976/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 977/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 978/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 979/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 980/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 981/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 982/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 983/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 984/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 985/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 986/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 987/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 988/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 989/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 990/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 991/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 992/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 993/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 994/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 995/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 996/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 997/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 998/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 999/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n",
      "Epoch 1000/1000, Train Loss: 0.783041775226593, Test Loss: 0.784781813621521\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:0.6920624540178205\n",
      "Accuracy:0.529125\n",
      "Confusion Matrix:\n",
      "[[    0 82874]\n",
      " [    0 93126]]\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "hidden_size = 50\n",
    "model = mlp.MLP_mach1(28, hidden_size)\n",
    "# Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = .1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 1000\n",
    "# Train the model using our function\n",
    "train_losses, test_losses = mlp.train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n",
    "# Make predictions on the validation set\n",
    "f1, acc, cm = mlp.getResults(train_losses, test_losses, model, X_val, y_val)\n",
    "# Print the results\n",
    "print(\"F1:\" + str(f1))\n",
    "print(\"Accuracy:\" + str(acc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.6964541077613831, Test Loss: 0.6961902976036072\n",
      "Epoch 2/1000, Train Loss: 0.6964044570922852, Test Loss: 0.6961429715156555\n",
      "Epoch 3/1000, Train Loss: 0.6963553428649902, Test Loss: 0.6960961222648621\n",
      "Epoch 4/1000, Train Loss: 0.6963067650794983, Test Loss: 0.696049690246582\n",
      "Epoch 5/1000, Train Loss: 0.6962586641311646, Test Loss: 0.6960037350654602\n",
      "Epoch 6/1000, Train Loss: 0.6962108612060547, Test Loss: 0.6959580779075623\n",
      "Epoch 7/1000, Train Loss: 0.696163535118103, Test Loss: 0.6959128975868225\n",
      "Epoch 8/1000, Train Loss: 0.6961166858673096, Test Loss: 0.6958682537078857\n",
      "Epoch 9/1000, Train Loss: 0.6960702538490295, Test Loss: 0.6958239674568176\n",
      "Epoch 10/1000, Train Loss: 0.6960242986679077, Test Loss: 0.6957800388336182\n",
      "Epoch 11/1000, Train Loss: 0.6959787011146545, Test Loss: 0.6957366466522217\n",
      "Epoch 12/1000, Train Loss: 0.6959335803985596, Test Loss: 0.6956934928894043\n",
      "Epoch 13/1000, Train Loss: 0.6958888173103333, Test Loss: 0.6956508159637451\n",
      "Epoch 14/1000, Train Loss: 0.6958445310592651, Test Loss: 0.6956085562705994\n",
      "Epoch 15/1000, Train Loss: 0.6958005428314209, Test Loss: 0.6955666542053223\n",
      "Epoch 16/1000, Train Loss: 0.6957570314407349, Test Loss: 0.6955252289772034\n",
      "Epoch 17/1000, Train Loss: 0.6957139372825623, Test Loss: 0.6954841017723083\n",
      "Epoch 18/1000, Train Loss: 0.6956712007522583, Test Loss: 0.6954433917999268\n",
      "Epoch 19/1000, Train Loss: 0.6956288814544678, Test Loss: 0.695402979850769\n",
      "Epoch 20/1000, Train Loss: 0.6955868601799011, Test Loss: 0.6953631043434143\n",
      "Epoch 21/1000, Train Loss: 0.6955453157424927, Test Loss: 0.6953234672546387\n",
      "Epoch 22/1000, Train Loss: 0.6955040693283081, Test Loss: 0.6952842473983765\n",
      "Epoch 23/1000, Train Loss: 0.695463240146637, Test Loss: 0.6952454447746277\n",
      "Epoch 24/1000, Train Loss: 0.6954228281974792, Test Loss: 0.695206880569458\n",
      "Epoch 25/1000, Train Loss: 0.6953827738761902, Test Loss: 0.6951687335968018\n",
      "Epoch 26/1000, Train Loss: 0.695343017578125, Test Loss: 0.6951309442520142\n",
      "Epoch 27/1000, Train Loss: 0.6953036785125732, Test Loss: 0.6950935125350952\n",
      "Epoch 28/1000, Train Loss: 0.6952646374702454, Test Loss: 0.6950563788414001\n",
      "Epoch 29/1000, Train Loss: 0.6952260732650757, Test Loss: 0.6950197219848633\n",
      "Epoch 30/1000, Train Loss: 0.6951877474784851, Test Loss: 0.6949832439422607\n",
      "Epoch 31/1000, Train Loss: 0.6951497197151184, Test Loss: 0.6949471831321716\n",
      "Epoch 32/1000, Train Loss: 0.6951121091842651, Test Loss: 0.6949113607406616\n",
      "Epoch 33/1000, Train Loss: 0.6950747966766357, Test Loss: 0.694875955581665\n",
      "Epoch 34/1000, Train Loss: 0.6950379014015198, Test Loss: 0.6948407888412476\n",
      "Epoch 35/1000, Train Loss: 0.6950012445449829, Test Loss: 0.6948060393333435\n",
      "Epoch 36/1000, Train Loss: 0.6949649453163147, Test Loss: 0.6947715878486633\n",
      "Epoch 37/1000, Train Loss: 0.6949290037155151, Test Loss: 0.694737434387207\n",
      "Epoch 38/1000, Train Loss: 0.6948933005332947, Test Loss: 0.6947035193443298\n",
      "Epoch 39/1000, Train Loss: 0.6948580145835876, Test Loss: 0.6946700215339661\n",
      "Epoch 40/1000, Train Loss: 0.6948230266571045, Test Loss: 0.6946367621421814\n",
      "Epoch 41/1000, Train Loss: 0.6947883367538452, Test Loss: 0.6946038007736206\n",
      "Epoch 42/1000, Train Loss: 0.6947538256645203, Test Loss: 0.6945711374282837\n",
      "Epoch 43/1000, Train Loss: 0.6947197914123535, Test Loss: 0.6945388913154602\n",
      "Epoch 44/1000, Train Loss: 0.6946860551834106, Test Loss: 0.6945067644119263\n",
      "Epoch 45/1000, Train Loss: 0.6946525573730469, Test Loss: 0.6944751143455505\n",
      "Epoch 46/1000, Train Loss: 0.694619357585907, Test Loss: 0.6944436430931091\n",
      "Epoch 47/1000, Train Loss: 0.694586455821991, Test Loss: 0.6944125294685364\n",
      "Epoch 48/1000, Train Loss: 0.6945539116859436, Test Loss: 0.694381594657898\n",
      "Epoch 49/1000, Train Loss: 0.6945216059684753, Test Loss: 0.6943510174751282\n",
      "Epoch 50/1000, Train Loss: 0.6944896578788757, Test Loss: 0.6943206787109375\n",
      "Epoch 51/1000, Train Loss: 0.6944578886032104, Test Loss: 0.6942905783653259\n",
      "Epoch 52/1000, Train Loss: 0.694426417350769, Test Loss: 0.694260835647583\n",
      "Epoch 53/1000, Train Loss: 0.6943952441215515, Test Loss: 0.6942312717437744\n",
      "Epoch 54/1000, Train Loss: 0.6943643093109131, Test Loss: 0.6942020058631897\n",
      "Epoch 55/1000, Train Loss: 0.6943337917327881, Test Loss: 0.6941730380058289\n",
      "Epoch 56/1000, Train Loss: 0.6943033933639526, Test Loss: 0.6941443085670471\n",
      "Epoch 57/1000, Train Loss: 0.6942732930183411, Test Loss: 0.6941158175468445\n",
      "Epoch 58/1000, Train Loss: 0.6942434310913086, Test Loss: 0.6940876245498657\n",
      "Epoch 59/1000, Train Loss: 0.6942138671875, Test Loss: 0.6940596699714661\n",
      "Epoch 60/1000, Train Loss: 0.6941845417022705, Test Loss: 0.6940319538116455\n",
      "Epoch 61/1000, Train Loss: 0.6941555142402649, Test Loss: 0.694004476070404\n",
      "Epoch 62/1000, Train Loss: 0.6941267251968384, Test Loss: 0.6939773559570312\n",
      "Epoch 63/1000, Train Loss: 0.6940982341766357, Test Loss: 0.693950355052948\n",
      "Epoch 64/1000, Train Loss: 0.6940698623657227, Test Loss: 0.6939235925674438\n",
      "Epoch 65/1000, Train Loss: 0.6940417885780334, Test Loss: 0.6938971877098083\n",
      "Epoch 66/1000, Train Loss: 0.6940140724182129, Test Loss: 0.6938709020614624\n",
      "Epoch 67/1000, Train Loss: 0.6939864754676819, Test Loss: 0.6938448548316956\n",
      "Epoch 68/1000, Train Loss: 0.6939591765403748, Test Loss: 0.6938191652297974\n",
      "Epoch 69/1000, Train Loss: 0.6939321756362915, Test Loss: 0.6937935948371887\n",
      "Epoch 70/1000, Train Loss: 0.693905234336853, Test Loss: 0.6937682628631592\n",
      "Epoch 71/1000, Train Loss: 0.6938785910606384, Test Loss: 0.6937431693077087\n",
      "Epoch 72/1000, Train Loss: 0.6938523054122925, Test Loss: 0.6937182545661926\n",
      "Epoch 73/1000, Train Loss: 0.6938261389732361, Test Loss: 0.6936936378479004\n",
      "Epoch 74/1000, Train Loss: 0.6938002109527588, Test Loss: 0.6936690807342529\n",
      "Epoch 75/1000, Train Loss: 0.6937745213508606, Test Loss: 0.6936448812484741\n",
      "Epoch 76/1000, Train Loss: 0.6937490701675415, Test Loss: 0.6936208605766296\n",
      "Epoch 77/1000, Train Loss: 0.693723738193512, Test Loss: 0.6935971975326538\n",
      "Epoch 78/1000, Train Loss: 0.6936987042427063, Test Loss: 0.693573534488678\n",
      "Epoch 79/1000, Train Loss: 0.6936737895011902, Test Loss: 0.6935501098632812\n",
      "Epoch 80/1000, Train Loss: 0.6936492323875427, Test Loss: 0.6935269832611084\n",
      "Epoch 81/1000, Train Loss: 0.6936247944831848, Test Loss: 0.6935040950775146\n",
      "Epoch 82/1000, Train Loss: 0.6936006546020508, Test Loss: 0.6934812664985657\n",
      "Epoch 83/1000, Train Loss: 0.6935766935348511, Test Loss: 0.6934586763381958\n",
      "Epoch 84/1000, Train Loss: 0.6935529112815857, Test Loss: 0.693436324596405\n",
      "Epoch 85/1000, Train Loss: 0.6935292482376099, Test Loss: 0.6934142112731934\n",
      "Epoch 86/1000, Train Loss: 0.6935059428215027, Test Loss: 0.6933921575546265\n",
      "Epoch 87/1000, Train Loss: 0.6934827566146851, Test Loss: 0.6933704018592834\n",
      "Epoch 88/1000, Train Loss: 0.6934597492218018, Test Loss: 0.6933488249778748\n",
      "Epoch 89/1000, Train Loss: 0.6934369802474976, Test Loss: 0.6933273673057556\n",
      "Epoch 90/1000, Train Loss: 0.6934143900871277, Test Loss: 0.6933062076568604\n",
      "Epoch 91/1000, Train Loss: 0.6933919191360474, Test Loss: 0.6932851076126099\n",
      "Epoch 92/1000, Train Loss: 0.6933698058128357, Test Loss: 0.6932641863822937\n",
      "Epoch 93/1000, Train Loss: 0.6933477520942688, Test Loss: 0.693243682384491\n",
      "Epoch 94/1000, Train Loss: 0.693325936794281, Test Loss: 0.6932231187820435\n",
      "Epoch 95/1000, Train Loss: 0.6933042407035828, Test Loss: 0.693202793598175\n",
      "Epoch 96/1000, Train Loss: 0.6932827830314636, Test Loss: 0.693182647228241\n",
      "Epoch 97/1000, Train Loss: 0.6932615041732788, Test Loss: 0.693162739276886\n",
      "Epoch 98/1000, Train Loss: 0.6932403445243835, Test Loss: 0.6931429505348206\n",
      "Epoch 99/1000, Train Loss: 0.6932195425033569, Test Loss: 0.6931232810020447\n",
      "Epoch 100/1000, Train Loss: 0.6931987404823303, Test Loss: 0.6931038498878479\n",
      "Epoch 101/1000, Train Loss: 0.693178117275238, Test Loss: 0.6930845379829407\n",
      "Epoch 102/1000, Train Loss: 0.6931577324867249, Test Loss: 0.6930654644966125\n",
      "Epoch 103/1000, Train Loss: 0.6931375861167908, Test Loss: 0.6930465698242188\n",
      "Epoch 104/1000, Train Loss: 0.6931173801422119, Test Loss: 0.6930277943611145\n",
      "Epoch 105/1000, Train Loss: 0.6930975914001465, Test Loss: 0.6930091381072998\n",
      "Epoch 106/1000, Train Loss: 0.6930778622627258, Test Loss: 0.6929907202720642\n",
      "Epoch 107/1000, Train Loss: 0.6930583715438843, Test Loss: 0.6929724216461182\n",
      "Epoch 108/1000, Train Loss: 0.6930389404296875, Test Loss: 0.6929543018341064\n",
      "Epoch 109/1000, Train Loss: 0.6930195689201355, Test Loss: 0.6929363012313843\n",
      "Epoch 110/1000, Train Loss: 0.6930004954338074, Test Loss: 0.6929185390472412\n",
      "Epoch 111/1000, Train Loss: 0.6929816007614136, Test Loss: 0.6929007172584534\n",
      "Epoch 112/1000, Train Loss: 0.6929629445075989, Test Loss: 0.6928831934928894\n",
      "Epoch 113/1000, Train Loss: 0.692944347858429, Test Loss: 0.6928658485412598\n",
      "Epoch 114/1000, Train Loss: 0.6929258704185486, Test Loss: 0.6928486824035645\n",
      "Epoch 115/1000, Train Loss: 0.6929076313972473, Test Loss: 0.6928314566612244\n",
      "Epoch 116/1000, Train Loss: 0.6928895115852356, Test Loss: 0.6928145885467529\n",
      "Epoch 117/1000, Train Loss: 0.6928715109825134, Test Loss: 0.6927977800369263\n",
      "Epoch 118/1000, Train Loss: 0.6928536295890808, Test Loss: 0.6927811503410339\n",
      "Epoch 119/1000, Train Loss: 0.6928359270095825, Test Loss: 0.6927646994590759\n",
      "Epoch 120/1000, Train Loss: 0.6928184628486633, Test Loss: 0.6927482485771179\n",
      "Epoch 121/1000, Train Loss: 0.6928009986877441, Test Loss: 0.692732036113739\n",
      "Epoch 122/1000, Train Loss: 0.692783772945404, Test Loss: 0.6927158832550049\n",
      "Epoch 123/1000, Train Loss: 0.6927666664123535, Test Loss: 0.6927000284194946\n",
      "Epoch 124/1000, Train Loss: 0.6927496790885925, Test Loss: 0.6926841735839844\n",
      "Epoch 125/1000, Train Loss: 0.6927328705787659, Test Loss: 0.6926684975624084\n",
      "Epoch 126/1000, Train Loss: 0.6927162408828735, Test Loss: 0.6926529407501221\n",
      "Epoch 127/1000, Train Loss: 0.6926996111869812, Test Loss: 0.6926375031471252\n",
      "Epoch 128/1000, Train Loss: 0.692683219909668, Test Loss: 0.6926222443580627\n",
      "Epoch 129/1000, Train Loss: 0.6926670074462891, Test Loss: 0.692607045173645\n",
      "Epoch 130/1000, Train Loss: 0.6926507949829102, Test Loss: 0.6925920248031616\n",
      "Epoch 131/1000, Train Loss: 0.6926348209381104, Test Loss: 0.692577064037323\n",
      "Epoch 132/1000, Train Loss: 0.6926189661026001, Test Loss: 0.6925623416900635\n",
      "Epoch 133/1000, Train Loss: 0.6926031708717346, Test Loss: 0.6925476789474487\n",
      "Epoch 134/1000, Train Loss: 0.6925875544548035, Test Loss: 0.6925331950187683\n",
      "Epoch 135/1000, Train Loss: 0.6925720572471619, Test Loss: 0.6925188302993774\n",
      "Epoch 136/1000, Train Loss: 0.6925567388534546, Test Loss: 0.6925044655799866\n",
      "Epoch 137/1000, Train Loss: 0.6925414204597473, Test Loss: 0.6924902200698853\n",
      "Epoch 138/1000, Train Loss: 0.6925263404846191, Test Loss: 0.692476212978363\n",
      "Epoch 139/1000, Train Loss: 0.6925113201141357, Test Loss: 0.6924623250961304\n",
      "Epoch 140/1000, Train Loss: 0.6924964785575867, Test Loss: 0.6924485564231873\n",
      "Epoch 141/1000, Train Loss: 0.6924816966056824, Test Loss: 0.6924349069595337\n",
      "Epoch 142/1000, Train Loss: 0.6924671530723572, Test Loss: 0.6924212574958801\n",
      "Epoch 143/1000, Train Loss: 0.692452609539032, Test Loss: 0.6924077868461609\n",
      "Epoch 144/1000, Train Loss: 0.6924383044242859, Test Loss: 0.6923944354057312\n",
      "Epoch 145/1000, Train Loss: 0.6924239993095398, Test Loss: 0.6923811435699463\n",
      "Epoch 146/1000, Train Loss: 0.6924098134040833, Test Loss: 0.6923680901527405\n",
      "Epoch 147/1000, Train Loss: 0.692395806312561, Test Loss: 0.6923550963401794\n",
      "Epoch 148/1000, Train Loss: 0.6923818588256836, Test Loss: 0.6923421621322632\n",
      "Epoch 149/1000, Train Loss: 0.6923680305480957, Test Loss: 0.6923292875289917\n",
      "Epoch 150/1000, Train Loss: 0.6923543214797974, Test Loss: 0.6923166513442993\n",
      "Epoch 151/1000, Train Loss: 0.6923407912254333, Test Loss: 0.6923040747642517\n",
      "Epoch 152/1000, Train Loss: 0.6923272013664246, Test Loss: 0.6922914981842041\n",
      "Epoch 153/1000, Train Loss: 0.6923139095306396, Test Loss: 0.6922791004180908\n",
      "Epoch 154/1000, Train Loss: 0.6923006176948547, Test Loss: 0.6922668814659119\n",
      "Epoch 155/1000, Train Loss: 0.6922875046730042, Test Loss: 0.6922547221183777\n",
      "Epoch 156/1000, Train Loss: 0.6922744512557983, Test Loss: 0.6922425031661987\n",
      "Epoch 157/1000, Train Loss: 0.6922614574432373, Test Loss: 0.6922305822372437\n",
      "Epoch 158/1000, Train Loss: 0.6922486424446106, Test Loss: 0.6922186017036438\n",
      "Epoch 159/1000, Train Loss: 0.6922358870506287, Test Loss: 0.6922067403793335\n",
      "Epoch 160/1000, Train Loss: 0.692223310470581, Test Loss: 0.6921950578689575\n",
      "Epoch 161/1000, Train Loss: 0.6922107338905334, Test Loss: 0.6921834945678711\n",
      "Epoch 162/1000, Train Loss: 0.6921982765197754, Test Loss: 0.6921719908714294\n",
      "Epoch 163/1000, Train Loss: 0.6921858787536621, Test Loss: 0.6921606063842773\n",
      "Epoch 164/1000, Train Loss: 0.6921736598014832, Test Loss: 0.69214928150177\n",
      "Epoch 165/1000, Train Loss: 0.6921615600585938, Test Loss: 0.6921380758285522\n",
      "Epoch 166/1000, Train Loss: 0.6921495199203491, Test Loss: 0.6921268701553345\n",
      "Epoch 167/1000, Train Loss: 0.6921375393867493, Test Loss: 0.692115843296051\n",
      "Epoch 168/1000, Train Loss: 0.6921256184577942, Test Loss: 0.6921049356460571\n",
      "Epoch 169/1000, Train Loss: 0.6921138763427734, Test Loss: 0.692094087600708\n",
      "Epoch 170/1000, Train Loss: 0.6921021342277527, Test Loss: 0.6920832395553589\n",
      "Epoch 171/1000, Train Loss: 0.692090630531311, Test Loss: 0.6920725703239441\n",
      "Epoch 172/1000, Train Loss: 0.6920790672302246, Test Loss: 0.6920619606971741\n",
      "Epoch 173/1000, Train Loss: 0.6920677423477173, Test Loss: 0.6920514702796936\n",
      "Epoch 174/1000, Train Loss: 0.69205641746521, Test Loss: 0.6920410990715027\n",
      "Epoch 175/1000, Train Loss: 0.6920450925827026, Test Loss: 0.692030668258667\n",
      "Epoch 176/1000, Train Loss: 0.6920340061187744, Test Loss: 0.6920204162597656\n",
      "Epoch 177/1000, Train Loss: 0.692022979259491, Test Loss: 0.6920102834701538\n",
      "Epoch 178/1000, Train Loss: 0.6920120120048523, Test Loss: 0.692000150680542\n",
      "Epoch 179/1000, Train Loss: 0.6920011639595032, Test Loss: 0.6919901967048645\n",
      "Epoch 180/1000, Train Loss: 0.6919904351234436, Test Loss: 0.6919801831245422\n",
      "Epoch 181/1000, Train Loss: 0.6919796466827393, Test Loss: 0.6919704079627991\n",
      "Epoch 182/1000, Train Loss: 0.6919690370559692, Test Loss: 0.6919606328010559\n",
      "Epoch 183/1000, Train Loss: 0.6919584274291992, Test Loss: 0.6919509172439575\n",
      "Epoch 184/1000, Train Loss: 0.6919479966163635, Test Loss: 0.6919413208961487\n",
      "Epoch 185/1000, Train Loss: 0.6919375658035278, Test Loss: 0.6919317245483398\n",
      "Epoch 186/1000, Train Loss: 0.6919273138046265, Test Loss: 0.6919223070144653\n",
      "Epoch 187/1000, Train Loss: 0.6919171810150146, Test Loss: 0.6919129490852356\n",
      "Epoch 188/1000, Train Loss: 0.6919069886207581, Test Loss: 0.6919037103652954\n",
      "Epoch 189/1000, Train Loss: 0.6918968558311462, Test Loss: 0.6918943524360657\n",
      "Epoch 190/1000, Train Loss: 0.6918869018554688, Test Loss: 0.691885232925415\n",
      "Epoch 191/1000, Train Loss: 0.6918770670890808, Test Loss: 0.6918761730194092\n",
      "Epoch 192/1000, Train Loss: 0.6918671727180481, Test Loss: 0.6918670535087585\n",
      "Epoch 193/1000, Train Loss: 0.6918574571609497, Test Loss: 0.6918581128120422\n",
      "Epoch 194/1000, Train Loss: 0.6918476819992065, Test Loss: 0.6918492913246155\n",
      "Epoch 195/1000, Train Loss: 0.6918381452560425, Test Loss: 0.6918404698371887\n",
      "Epoch 196/1000, Train Loss: 0.6918286681175232, Test Loss: 0.6918317675590515\n",
      "Epoch 197/1000, Train Loss: 0.6918191909790039, Test Loss: 0.6918230652809143\n",
      "Epoch 198/1000, Train Loss: 0.6918098330497742, Test Loss: 0.6918146014213562\n",
      "Epoch 199/1000, Train Loss: 0.6918004155158997, Test Loss: 0.6918060183525085\n",
      "Epoch 200/1000, Train Loss: 0.6917912364006042, Test Loss: 0.6917976140975952\n",
      "Epoch 201/1000, Train Loss: 0.6917819976806641, Test Loss: 0.6917891502380371\n",
      "Epoch 202/1000, Train Loss: 0.691772997379303, Test Loss: 0.6917808651924133\n",
      "Epoch 203/1000, Train Loss: 0.6917639374732971, Test Loss: 0.6917726397514343\n",
      "Epoch 204/1000, Train Loss: 0.6917549967765808, Test Loss: 0.6917644143104553\n",
      "Epoch 205/1000, Train Loss: 0.6917461156845093, Test Loss: 0.6917562484741211\n",
      "Epoch 206/1000, Train Loss: 0.6917372941970825, Test Loss: 0.6917482018470764\n",
      "Epoch 207/1000, Train Loss: 0.6917285323143005, Test Loss: 0.6917402744293213\n",
      "Epoch 208/1000, Train Loss: 0.6917198300361633, Test Loss: 0.6917322874069214\n",
      "Epoch 209/1000, Train Loss: 0.6917111873626709, Test Loss: 0.6917243599891663\n",
      "Epoch 210/1000, Train Loss: 0.6917025446891785, Test Loss: 0.6917165517807007\n",
      "Epoch 211/1000, Train Loss: 0.6916940808296204, Test Loss: 0.6917088031768799\n",
      "Epoch 212/1000, Train Loss: 0.691685676574707, Test Loss: 0.6917011141777039\n",
      "Epoch 213/1000, Train Loss: 0.6916772723197937, Test Loss: 0.6916934251785278\n",
      "Epoch 214/1000, Train Loss: 0.6916690468788147, Test Loss: 0.6916859149932861\n",
      "Epoch 215/1000, Train Loss: 0.6916607618331909, Test Loss: 0.6916784048080444\n",
      "Epoch 216/1000, Train Loss: 0.6916525959968567, Test Loss: 0.6916708946228027\n",
      "Epoch 217/1000, Train Loss: 0.6916444897651672, Test Loss: 0.6916635036468506\n",
      "Epoch 218/1000, Train Loss: 0.6916363835334778, Test Loss: 0.6916561126708984\n",
      "Epoch 219/1000, Train Loss: 0.6916283369064331, Test Loss: 0.6916489005088806\n",
      "Epoch 220/1000, Train Loss: 0.6916204690933228, Test Loss: 0.6916415691375732\n",
      "Epoch 221/1000, Train Loss: 0.6916126012802124, Test Loss: 0.691634476184845\n",
      "Epoch 222/1000, Train Loss: 0.6916047930717468, Test Loss: 0.6916272640228271\n",
      "Epoch 223/1000, Train Loss: 0.6915969848632812, Test Loss: 0.6916202902793884\n",
      "Epoch 224/1000, Train Loss: 0.69158935546875, Test Loss: 0.6916132569313049\n",
      "Epoch 225/1000, Train Loss: 0.691581666469574, Test Loss: 0.6916062831878662\n",
      "Epoch 226/1000, Train Loss: 0.6915740370750427, Test Loss: 0.6915993690490723\n",
      "Epoch 227/1000, Train Loss: 0.691566526889801, Test Loss: 0.6915925741195679\n",
      "Epoch 228/1000, Train Loss: 0.6915590763092041, Test Loss: 0.6915857791900635\n",
      "Epoch 229/1000, Train Loss: 0.6915516257286072, Test Loss: 0.6915790438652039\n",
      "Epoch 230/1000, Train Loss: 0.6915441751480103, Test Loss: 0.691572368144989\n",
      "Epoch 231/1000, Train Loss: 0.6915369629859924, Test Loss: 0.6915656924247742\n",
      "Epoch 232/1000, Train Loss: 0.6915296316146851, Test Loss: 0.6915590763092041\n",
      "Epoch 233/1000, Train Loss: 0.691522479057312, Test Loss: 0.6915525794029236\n",
      "Epoch 234/1000, Train Loss: 0.6915152072906494, Test Loss: 0.6915460228919983\n",
      "Epoch 235/1000, Train Loss: 0.6915081739425659, Test Loss: 0.6915395855903625\n",
      "Epoch 236/1000, Train Loss: 0.6915011405944824, Test Loss: 0.6915332078933716\n",
      "Epoch 237/1000, Train Loss: 0.6914941668510437, Test Loss: 0.6915269494056702\n",
      "Epoch 238/1000, Train Loss: 0.691487193107605, Test Loss: 0.691520631313324\n",
      "Epoch 239/1000, Train Loss: 0.6914803385734558, Test Loss: 0.6915143132209778\n",
      "Epoch 240/1000, Train Loss: 0.6914734840393066, Test Loss: 0.6915081143379211\n",
      "Epoch 241/1000, Train Loss: 0.6914666891098022, Test Loss: 0.6915019750595093\n",
      "Epoch 242/1000, Train Loss: 0.6914599537849426, Test Loss: 0.691495954990387\n",
      "Epoch 243/1000, Train Loss: 0.691453218460083, Test Loss: 0.6914898157119751\n",
      "Epoch 244/1000, Train Loss: 0.6914465427398682, Test Loss: 0.6914838552474976\n",
      "Epoch 245/1000, Train Loss: 0.6914399862289429, Test Loss: 0.6914778351783752\n",
      "Epoch 246/1000, Train Loss: 0.6914334297180176, Test Loss: 0.6914718747138977\n",
      "Epoch 247/1000, Train Loss: 0.6914268732070923, Test Loss: 0.6914660334587097\n",
      "Epoch 248/1000, Train Loss: 0.6914204359054565, Test Loss: 0.6914601922035217\n",
      "Epoch 249/1000, Train Loss: 0.6914140582084656, Test Loss: 0.6914544105529785\n",
      "Epoch 250/1000, Train Loss: 0.6914075613021851, Test Loss: 0.6914486289024353\n",
      "Epoch 251/1000, Train Loss: 0.6914013624191284, Test Loss: 0.6914429068565369\n",
      "Epoch 252/1000, Train Loss: 0.6913950443267822, Test Loss: 0.691437304019928\n",
      "Epoch 253/1000, Train Loss: 0.6913888454437256, Test Loss: 0.6914317011833191\n",
      "Epoch 254/1000, Train Loss: 0.6913825869560242, Test Loss: 0.691426157951355\n",
      "Epoch 255/1000, Train Loss: 0.6913765072822571, Test Loss: 0.6914205551147461\n",
      "Epoch 256/1000, Train Loss: 0.69137042760849, Test Loss: 0.6914150714874268\n",
      "Epoch 257/1000, Train Loss: 0.6913642287254333, Test Loss: 0.6914095878601074\n",
      "Epoch 258/1000, Train Loss: 0.6913583278656006, Test Loss: 0.6914042234420776\n",
      "Epoch 259/1000, Train Loss: 0.6913523077964783, Test Loss: 0.6913988590240479\n",
      "Epoch 260/1000, Train Loss: 0.6913464069366455, Test Loss: 0.6913935542106628\n",
      "Epoch 261/1000, Train Loss: 0.6913405060768127, Test Loss: 0.6913881897926331\n",
      "Epoch 262/1000, Train Loss: 0.69133460521698, Test Loss: 0.6913829445838928\n",
      "Epoch 263/1000, Train Loss: 0.6913288235664368, Test Loss: 0.6913776993751526\n",
      "Epoch 264/1000, Train Loss: 0.6913231015205383, Test Loss: 0.6913725733757019\n",
      "Epoch 265/1000, Train Loss: 0.6913173794746399, Test Loss: 0.6913673281669617\n",
      "Epoch 266/1000, Train Loss: 0.6913116574287415, Test Loss: 0.6913623213768005\n",
      "Epoch 267/1000, Train Loss: 0.6913059949874878, Test Loss: 0.6913571953773499\n",
      "Epoch 268/1000, Train Loss: 0.6913003921508789, Test Loss: 0.6913521885871887\n",
      "Epoch 269/1000, Train Loss: 0.6912948489189148, Test Loss: 0.6913472414016724\n",
      "Epoch 270/1000, Train Loss: 0.6912893652915955, Test Loss: 0.6913421750068665\n",
      "Epoch 271/1000, Train Loss: 0.6912838220596313, Test Loss: 0.6913373470306396\n",
      "Epoch 272/1000, Train Loss: 0.691278338432312, Test Loss: 0.6913323998451233\n",
      "Epoch 273/1000, Train Loss: 0.6912729740142822, Test Loss: 0.6913275718688965\n",
      "Epoch 274/1000, Train Loss: 0.6912676095962524, Test Loss: 0.6913227438926697\n",
      "Epoch 275/1000, Train Loss: 0.6912622451782227, Test Loss: 0.6913179755210876\n",
      "Epoch 276/1000, Train Loss: 0.6912569403648376, Test Loss: 0.6913132667541504\n",
      "Epoch 277/1000, Train Loss: 0.6912516355514526, Test Loss: 0.6913084387779236\n",
      "Epoch 278/1000, Train Loss: 0.6912464499473572, Test Loss: 0.6913037896156311\n",
      "Epoch 279/1000, Train Loss: 0.6912412047386169, Test Loss: 0.6912992000579834\n",
      "Epoch 280/1000, Train Loss: 0.691236138343811, Test Loss: 0.6912945508956909\n",
      "Epoch 281/1000, Train Loss: 0.6912310123443604, Test Loss: 0.6912899613380432\n",
      "Epoch 282/1000, Train Loss: 0.6912258863449097, Test Loss: 0.6912853717803955\n",
      "Epoch 283/1000, Train Loss: 0.6912208795547485, Test Loss: 0.6912809610366821\n",
      "Epoch 284/1000, Train Loss: 0.6912158131599426, Test Loss: 0.691276490688324\n",
      "Epoch 285/1000, Train Loss: 0.6912107467651367, Test Loss: 0.6912720203399658\n",
      "Epoch 286/1000, Train Loss: 0.6912058591842651, Test Loss: 0.6912676095962524\n",
      "Epoch 287/1000, Train Loss: 0.6912010312080383, Test Loss: 0.6912631988525391\n",
      "Epoch 288/1000, Train Loss: 0.691196084022522, Test Loss: 0.6912589073181152\n",
      "Epoch 289/1000, Train Loss: 0.6911913156509399, Test Loss: 0.6912544965744019\n",
      "Epoch 290/1000, Train Loss: 0.6911864280700684, Test Loss: 0.6912502646446228\n",
      "Epoch 291/1000, Train Loss: 0.6911816596984863, Test Loss: 0.6912460327148438\n",
      "Epoch 292/1000, Train Loss: 0.6911769509315491, Test Loss: 0.6912416815757751\n",
      "Epoch 293/1000, Train Loss: 0.691172182559967, Test Loss: 0.6912374496459961\n",
      "Epoch 294/1000, Train Loss: 0.6911674737930298, Test Loss: 0.691233217716217\n",
      "Epoch 295/1000, Train Loss: 0.6911628246307373, Test Loss: 0.6912291049957275\n",
      "Epoch 296/1000, Train Loss: 0.6911582350730896, Test Loss: 0.6912250518798828\n",
      "Epoch 297/1000, Train Loss: 0.6911536455154419, Test Loss: 0.6912209391593933\n",
      "Epoch 298/1000, Train Loss: 0.6911490559577942, Test Loss: 0.6912168860435486\n",
      "Epoch 299/1000, Train Loss: 0.6911444664001465, Test Loss: 0.6912127733230591\n",
      "Epoch 300/1000, Train Loss: 0.6911399960517883, Test Loss: 0.6912087798118591\n",
      "Epoch 301/1000, Train Loss: 0.691135585308075, Test Loss: 0.691204845905304\n",
      "Epoch 302/1000, Train Loss: 0.691131055355072, Test Loss: 0.691200852394104\n",
      "Epoch 303/1000, Train Loss: 0.6911265850067139, Test Loss: 0.6911969184875488\n",
      "Epoch 304/1000, Train Loss: 0.69112229347229, Test Loss: 0.6911930441856384\n",
      "Epoch 305/1000, Train Loss: 0.6911179423332214, Test Loss: 0.6911891102790833\n",
      "Epoch 306/1000, Train Loss: 0.6911135911941528, Test Loss: 0.6911852955818176\n",
      "Epoch 307/1000, Train Loss: 0.6911092400550842, Test Loss: 0.691181480884552\n",
      "Epoch 308/1000, Train Loss: 0.69110506772995, Test Loss: 0.6911776661872864\n",
      "Epoch 309/1000, Train Loss: 0.6911007761955261, Test Loss: 0.6911739110946655\n",
      "Epoch 310/1000, Train Loss: 0.6910966038703918, Test Loss: 0.6911701560020447\n",
      "Epoch 311/1000, Train Loss: 0.691092312335968, Test Loss: 0.6911665201187134\n",
      "Epoch 312/1000, Train Loss: 0.6910881400108337, Test Loss: 0.6911627650260925\n",
      "Epoch 313/1000, Train Loss: 0.6910840272903442, Test Loss: 0.6911591291427612\n",
      "Epoch 314/1000, Train Loss: 0.6910799145698547, Test Loss: 0.6911554932594299\n",
      "Epoch 315/1000, Train Loss: 0.69107586145401, Test Loss: 0.6911518573760986\n",
      "Epoch 316/1000, Train Loss: 0.6910718083381653, Test Loss: 0.6911482810974121\n",
      "Epoch 317/1000, Train Loss: 0.6910677552223206, Test Loss: 0.691144585609436\n",
      "Epoch 318/1000, Train Loss: 0.6910637021064758, Test Loss: 0.6911411285400391\n",
      "Epoch 319/1000, Train Loss: 0.6910597681999207, Test Loss: 0.6911376118659973\n",
      "Epoch 320/1000, Train Loss: 0.6910558938980103, Test Loss: 0.6911340355873108\n",
      "Epoch 321/1000, Train Loss: 0.6910519003868103, Test Loss: 0.6911306977272034\n",
      "Epoch 322/1000, Train Loss: 0.6910479664802551, Test Loss: 0.6911271810531616\n",
      "Epoch 323/1000, Train Loss: 0.6910441517829895, Test Loss: 0.6911237239837646\n",
      "Epoch 324/1000, Train Loss: 0.6910403370857239, Test Loss: 0.6911203861236572\n",
      "Epoch 325/1000, Train Loss: 0.6910364031791687, Test Loss: 0.691116988658905\n",
      "Epoch 326/1000, Train Loss: 0.6910326480865479, Test Loss: 0.6911135911941528\n",
      "Epoch 327/1000, Train Loss: 0.691028892993927, Test Loss: 0.6911103129386902\n",
      "Epoch 328/1000, Train Loss: 0.6910251975059509, Test Loss: 0.6911069750785828\n",
      "Epoch 329/1000, Train Loss: 0.6910213232040405, Test Loss: 0.6911036968231201\n",
      "Epoch 330/1000, Train Loss: 0.6910176277160645, Test Loss: 0.6911004185676575\n",
      "Epoch 331/1000, Train Loss: 0.6910139918327332, Test Loss: 0.6910971403121948\n",
      "Epoch 332/1000, Train Loss: 0.6910102963447571, Test Loss: 0.6910938620567322\n",
      "Epoch 333/1000, Train Loss: 0.6910066604614258, Test Loss: 0.6910905838012695\n",
      "Epoch 334/1000, Train Loss: 0.6910030245780945, Test Loss: 0.6910874843597412\n",
      "Epoch 335/1000, Train Loss: 0.690999448299408, Test Loss: 0.6910843253135681\n",
      "Epoch 336/1000, Train Loss: 0.6909959316253662, Test Loss: 0.6910812258720398\n",
      "Epoch 337/1000, Train Loss: 0.6909922361373901, Test Loss: 0.6910781264305115\n",
      "Epoch 338/1000, Train Loss: 0.6909887790679932, Test Loss: 0.6910749077796936\n",
      "Epoch 339/1000, Train Loss: 0.6909852623939514, Test Loss: 0.6910718679428101\n",
      "Epoch 340/1000, Train Loss: 0.6909817457199097, Test Loss: 0.6910688281059265\n",
      "Epoch 341/1000, Train Loss: 0.6909783482551575, Test Loss: 0.6910657286643982\n",
      "Epoch 342/1000, Train Loss: 0.6909748315811157, Test Loss: 0.6910626888275146\n",
      "Epoch 343/1000, Train Loss: 0.6909713745117188, Test Loss: 0.6910597085952759\n",
      "Epoch 344/1000, Train Loss: 0.6909680366516113, Test Loss: 0.6910566687583923\n",
      "Epoch 345/1000, Train Loss: 0.6909646391868591, Test Loss: 0.6910536885261536\n",
      "Epoch 346/1000, Train Loss: 0.6909613013267517, Test Loss: 0.6910507678985596\n",
      "Epoch 347/1000, Train Loss: 0.6909579038619995, Test Loss: 0.6910477876663208\n",
      "Epoch 348/1000, Train Loss: 0.6909545660018921, Test Loss: 0.6910449266433716\n",
      "Epoch 349/1000, Train Loss: 0.6909512877464294, Test Loss: 0.6910420656204224\n",
      "Epoch 350/1000, Train Loss: 0.6909480094909668, Test Loss: 0.6910390853881836\n",
      "Epoch 351/1000, Train Loss: 0.6909447908401489, Test Loss: 0.6910362243652344\n",
      "Epoch 352/1000, Train Loss: 0.6909414529800415, Test Loss: 0.6910334229469299\n",
      "Epoch 353/1000, Train Loss: 0.6909382343292236, Test Loss: 0.6910305023193359\n",
      "Epoch 354/1000, Train Loss: 0.6909350156784058, Test Loss: 0.6910276412963867\n",
      "Epoch 355/1000, Train Loss: 0.6909318566322327, Test Loss: 0.6910249590873718\n",
      "Epoch 356/1000, Train Loss: 0.6909286975860596, Test Loss: 0.6910221576690674\n",
      "Epoch 357/1000, Train Loss: 0.6909255385398865, Test Loss: 0.6910194158554077\n",
      "Epoch 358/1000, Train Loss: 0.6909223794937134, Test Loss: 0.691016674041748\n",
      "Epoch 359/1000, Train Loss: 0.6909192800521851, Test Loss: 0.6910138726234436\n",
      "Epoch 360/1000, Train Loss: 0.690916121006012, Test Loss: 0.6910113096237183\n",
      "Epoch 361/1000, Train Loss: 0.6909130811691284, Test Loss: 0.6910085678100586\n",
      "Epoch 362/1000, Train Loss: 0.6909099817276001, Test Loss: 0.6910058856010437\n",
      "Epoch 363/1000, Train Loss: 0.6909069418907166, Test Loss: 0.6910032629966736\n",
      "Epoch 364/1000, Train Loss: 0.690903902053833, Test Loss: 0.6910005807876587\n",
      "Epoch 365/1000, Train Loss: 0.6909008026123047, Test Loss: 0.6909978985786438\n",
      "Epoch 366/1000, Train Loss: 0.6908978819847107, Test Loss: 0.6909953951835632\n",
      "Epoch 367/1000, Train Loss: 0.6908948421478271, Test Loss: 0.6909927129745483\n",
      "Epoch 368/1000, Train Loss: 0.6908919811248779, Test Loss: 0.6909900307655334\n",
      "Epoch 369/1000, Train Loss: 0.6908890604972839, Test Loss: 0.6909876465797424\n",
      "Epoch 370/1000, Train Loss: 0.6908861398696899, Test Loss: 0.6909850239753723\n",
      "Epoch 371/1000, Train Loss: 0.690883219242096, Test Loss: 0.690982460975647\n",
      "Epoch 372/1000, Train Loss: 0.6908803582191467, Test Loss: 0.6909799575805664\n",
      "Epoch 373/1000, Train Loss: 0.6908774375915527, Test Loss: 0.6909774541854858\n",
      "Epoch 374/1000, Train Loss: 0.6908746361732483, Test Loss: 0.6909749507904053\n",
      "Epoch 375/1000, Train Loss: 0.6908717751502991, Test Loss: 0.6909724473953247\n",
      "Epoch 376/1000, Train Loss: 0.6908689737319946, Test Loss: 0.6909700036048889\n",
      "Epoch 377/1000, Train Loss: 0.6908661723136902, Test Loss: 0.6909675002098083\n",
      "Epoch 378/1000, Train Loss: 0.6908633708953857, Test Loss: 0.6909650564193726\n",
      "Epoch 379/1000, Train Loss: 0.6908605694770813, Test Loss: 0.6909627318382263\n",
      "Epoch 380/1000, Train Loss: 0.6908578276634216, Test Loss: 0.6909602284431458\n",
      "Epoch 381/1000, Train Loss: 0.6908550262451172, Test Loss: 0.6909579038619995\n",
      "Epoch 382/1000, Train Loss: 0.6908523440361023, Test Loss: 0.6909554600715637\n",
      "Epoch 383/1000, Train Loss: 0.6908496022224426, Test Loss: 0.6909530758857727\n",
      "Epoch 384/1000, Train Loss: 0.690846860408783, Test Loss: 0.6909508109092712\n",
      "Epoch 385/1000, Train Loss: 0.6908441781997681, Test Loss: 0.690948486328125\n",
      "Epoch 386/1000, Train Loss: 0.6908414363861084, Test Loss: 0.6909462213516235\n",
      "Epoch 387/1000, Train Loss: 0.6908388733863831, Test Loss: 0.6909437775611877\n",
      "Epoch 388/1000, Train Loss: 0.6908361911773682, Test Loss: 0.6909416317939758\n",
      "Epoch 389/1000, Train Loss: 0.6908336281776428, Test Loss: 0.69093918800354\n",
      "Epoch 390/1000, Train Loss: 0.6908309459686279, Test Loss: 0.6909369230270386\n",
      "Epoch 391/1000, Train Loss: 0.6908283829689026, Test Loss: 0.6909346580505371\n",
      "Epoch 392/1000, Train Loss: 0.6908257603645325, Test Loss: 0.6909323334693909\n",
      "Epoch 393/1000, Train Loss: 0.6908231973648071, Test Loss: 0.690930187702179\n",
      "Epoch 394/1000, Train Loss: 0.690820574760437, Test Loss: 0.6909279823303223\n",
      "Epoch 395/1000, Train Loss: 0.6908180713653564, Test Loss: 0.6909257769584656\n",
      "Epoch 396/1000, Train Loss: 0.6908155679702759, Test Loss: 0.6909234523773193\n",
      "Epoch 397/1000, Train Loss: 0.6908130049705505, Test Loss: 0.6909213662147522\n",
      "Epoch 398/1000, Train Loss: 0.69081050157547, Test Loss: 0.6909191608428955\n",
      "Epoch 399/1000, Train Loss: 0.6908079385757446, Test Loss: 0.6909170150756836\n",
      "Epoch 400/1000, Train Loss: 0.6908054947853088, Test Loss: 0.6909148097038269\n",
      "Epoch 401/1000, Train Loss: 0.6908031105995178, Test Loss: 0.6909127235412598\n",
      "Epoch 402/1000, Train Loss: 0.690800666809082, Test Loss: 0.6909106373786926\n",
      "Epoch 403/1000, Train Loss: 0.6907981634140015, Test Loss: 0.6909084916114807\n",
      "Epoch 404/1000, Train Loss: 0.6907957792282104, Test Loss: 0.6909063458442688\n",
      "Epoch 405/1000, Train Loss: 0.6907933354377747, Test Loss: 0.6909041404724121\n",
      "Epoch 406/1000, Train Loss: 0.6907908916473389, Test Loss: 0.6909021735191345\n",
      "Epoch 407/1000, Train Loss: 0.6907884478569031, Test Loss: 0.6908999681472778\n",
      "Epoch 408/1000, Train Loss: 0.6907860636711121, Test Loss: 0.6908979415893555\n",
      "Epoch 409/1000, Train Loss: 0.6907837390899658, Test Loss: 0.6908959150314331\n",
      "Epoch 410/1000, Train Loss: 0.6907814145088196, Test Loss: 0.6908939480781555\n",
      "Epoch 411/1000, Train Loss: 0.6907789707183838, Test Loss: 0.6908918619155884\n",
      "Epoch 412/1000, Train Loss: 0.6907767057418823, Test Loss: 0.690889835357666\n",
      "Epoch 413/1000, Train Loss: 0.6907743215560913, Test Loss: 0.6908878087997437\n",
      "Epoch 414/1000, Train Loss: 0.6907719969749451, Test Loss: 0.6908858418464661\n",
      "Epoch 415/1000, Train Loss: 0.6907697319984436, Test Loss: 0.6908838152885437\n",
      "Epoch 416/1000, Train Loss: 0.6907674074172974, Test Loss: 0.6908817887306213\n",
      "Epoch 417/1000, Train Loss: 0.6907650828361511, Test Loss: 0.6908798217773438\n",
      "Epoch 418/1000, Train Loss: 0.6907628774642944, Test Loss: 0.6908778548240662\n",
      "Epoch 419/1000, Train Loss: 0.6907606720924377, Test Loss: 0.6908758878707886\n",
      "Epoch 420/1000, Train Loss: 0.690758466720581, Test Loss: 0.6908739805221558\n",
      "Epoch 421/1000, Train Loss: 0.6907561421394348, Test Loss: 0.690872073173523\n",
      "Epoch 422/1000, Train Loss: 0.6907539963722229, Test Loss: 0.6908701658248901\n",
      "Epoch 423/1000, Train Loss: 0.6907517313957214, Test Loss: 0.6908681988716125\n",
      "Epoch 424/1000, Train Loss: 0.6907496452331543, Test Loss: 0.690866231918335\n",
      "Epoch 425/1000, Train Loss: 0.6907473206520081, Test Loss: 0.6908644437789917\n",
      "Epoch 426/1000, Train Loss: 0.6907451748847961, Test Loss: 0.6908624172210693\n",
      "Epoch 427/1000, Train Loss: 0.6907430291175842, Test Loss: 0.6908606290817261\n",
      "Epoch 428/1000, Train Loss: 0.6907408833503723, Test Loss: 0.690858781337738\n",
      "Epoch 429/1000, Train Loss: 0.6907386779785156, Test Loss: 0.69085693359375\n",
      "Epoch 430/1000, Train Loss: 0.6907365918159485, Test Loss: 0.6908551454544067\n",
      "Epoch 431/1000, Train Loss: 0.6907344460487366, Test Loss: 0.6908532381057739\n",
      "Epoch 432/1000, Train Loss: 0.6907322406768799, Test Loss: 0.6908513903617859\n",
      "Epoch 433/1000, Train Loss: 0.6907301545143127, Test Loss: 0.6908495426177979\n",
      "Epoch 434/1000, Train Loss: 0.6907281279563904, Test Loss: 0.6908477544784546\n",
      "Epoch 435/1000, Train Loss: 0.6907259225845337, Test Loss: 0.6908459067344666\n",
      "Epoch 436/1000, Train Loss: 0.6907239556312561, Test Loss: 0.6908441185951233\n",
      "Epoch 437/1000, Train Loss: 0.6907218098640442, Test Loss: 0.69084233045578\n",
      "Epoch 438/1000, Train Loss: 0.6907198429107666, Test Loss: 0.6908405423164368\n",
      "Epoch 439/1000, Train Loss: 0.6907176971435547, Test Loss: 0.6908388137817383\n",
      "Epoch 440/1000, Train Loss: 0.6907157301902771, Test Loss: 0.6908370852470398\n",
      "Epoch 441/1000, Train Loss: 0.6907137036323547, Test Loss: 0.6908352375030518\n",
      "Epoch 442/1000, Train Loss: 0.6907115578651428, Test Loss: 0.6908335089683533\n",
      "Epoch 443/1000, Train Loss: 0.69070965051651, Test Loss: 0.6908317804336548\n",
      "Epoch 444/1000, Train Loss: 0.6907076239585876, Test Loss: 0.6908300518989563\n",
      "Epoch 445/1000, Train Loss: 0.6907055377960205, Test Loss: 0.6908283233642578\n",
      "Epoch 446/1000, Train Loss: 0.6907035708427429, Test Loss: 0.6908266544342041\n",
      "Epoch 447/1000, Train Loss: 0.6907016038894653, Test Loss: 0.6908249258995056\n",
      "Epoch 448/1000, Train Loss: 0.6906996965408325, Test Loss: 0.6908231377601624\n",
      "Epoch 449/1000, Train Loss: 0.6906977891921997, Test Loss: 0.6908215284347534\n",
      "Epoch 450/1000, Train Loss: 0.6906957626342773, Test Loss: 0.6908197999000549\n",
      "Epoch 451/1000, Train Loss: 0.6906938552856445, Test Loss: 0.6908180713653564\n",
      "Epoch 452/1000, Train Loss: 0.6906918883323669, Test Loss: 0.6908164620399475\n",
      "Epoch 453/1000, Train Loss: 0.6906899213790894, Test Loss: 0.6908147931098938\n",
      "Epoch 454/1000, Train Loss: 0.6906880140304565, Test Loss: 0.6908131241798401\n",
      "Epoch 455/1000, Train Loss: 0.690686047077179, Test Loss: 0.6908115148544312\n",
      "Epoch 456/1000, Train Loss: 0.6906841993331909, Test Loss: 0.6908098459243774\n",
      "Epoch 457/1000, Train Loss: 0.6906822323799133, Test Loss: 0.6908082365989685\n",
      "Epoch 458/1000, Train Loss: 0.6906804442405701, Test Loss: 0.6908066272735596\n",
      "Epoch 459/1000, Train Loss: 0.6906784772872925, Test Loss: 0.6908050179481506\n",
      "Epoch 460/1000, Train Loss: 0.6906766891479492, Test Loss: 0.6908033490180969\n",
      "Epoch 461/1000, Train Loss: 0.6906748414039612, Test Loss: 0.6908016800880432\n",
      "Epoch 462/1000, Train Loss: 0.6906729340553284, Test Loss: 0.6908001899719238\n",
      "Epoch 463/1000, Train Loss: 0.6906710863113403, Test Loss: 0.6907985806465149\n",
      "Epoch 464/1000, Train Loss: 0.6906692981719971, Test Loss: 0.690796971321106\n",
      "Epoch 465/1000, Train Loss: 0.690667450428009, Test Loss: 0.690795361995697\n",
      "Epoch 466/1000, Train Loss: 0.690665602684021, Test Loss: 0.6907937526702881\n",
      "Epoch 467/1000, Train Loss: 0.690663754940033, Test Loss: 0.6907922029495239\n",
      "Epoch 468/1000, Train Loss: 0.6906619668006897, Test Loss: 0.6907906532287598\n",
      "Epoch 469/1000, Train Loss: 0.6906601190567017, Test Loss: 0.6907891035079956\n",
      "Epoch 470/1000, Train Loss: 0.6906582713127136, Test Loss: 0.6907874941825867\n",
      "Epoch 471/1000, Train Loss: 0.6906565427780151, Test Loss: 0.6907859444618225\n",
      "Epoch 472/1000, Train Loss: 0.6906547546386719, Test Loss: 0.6907843947410583\n",
      "Epoch 473/1000, Train Loss: 0.6906529664993286, Test Loss: 0.690782904624939\n",
      "Epoch 474/1000, Train Loss: 0.6906511187553406, Test Loss: 0.6907813549041748\n",
      "Epoch 475/1000, Train Loss: 0.6906494498252869, Test Loss: 0.6907798647880554\n",
      "Epoch 476/1000, Train Loss: 0.6906477212905884, Test Loss: 0.690778374671936\n",
      "Epoch 477/1000, Train Loss: 0.6906459927558899, Test Loss: 0.6907768249511719\n",
      "Epoch 478/1000, Train Loss: 0.6906442046165466, Test Loss: 0.6907753944396973\n",
      "Epoch 479/1000, Train Loss: 0.6906424760818481, Test Loss: 0.6907737851142883\n",
      "Epoch 480/1000, Train Loss: 0.6906406879425049, Test Loss: 0.690772294998169\n",
      "Epoch 481/1000, Train Loss: 0.690639078617096, Test Loss: 0.6907708048820496\n",
      "Epoch 482/1000, Train Loss: 0.6906372904777527, Test Loss: 0.690769374370575\n",
      "Epoch 483/1000, Train Loss: 0.6906356811523438, Test Loss: 0.6907678842544556\n",
      "Epoch 484/1000, Train Loss: 0.69063401222229, Test Loss: 0.6907663941383362\n",
      "Epoch 485/1000, Train Loss: 0.6906322240829468, Test Loss: 0.6907649636268616\n",
      "Epoch 486/1000, Train Loss: 0.6906304955482483, Test Loss: 0.6907634139060974\n",
      "Epoch 487/1000, Train Loss: 0.6906288266181946, Test Loss: 0.6907620429992676\n",
      "Epoch 488/1000, Train Loss: 0.6906270980834961, Test Loss: 0.6907605528831482\n",
      "Epoch 489/1000, Train Loss: 0.6906254887580872, Test Loss: 0.6907591819763184\n",
      "Epoch 490/1000, Train Loss: 0.6906238198280334, Test Loss: 0.690757691860199\n",
      "Epoch 491/1000, Train Loss: 0.6906221508979797, Test Loss: 0.6907562017440796\n",
      "Epoch 492/1000, Train Loss: 0.6906205415725708, Test Loss: 0.6907548308372498\n",
      "Epoch 493/1000, Train Loss: 0.6906188726425171, Test Loss: 0.6907533407211304\n",
      "Epoch 494/1000, Train Loss: 0.6906172037124634, Test Loss: 0.6907519698143005\n",
      "Epoch 495/1000, Train Loss: 0.6906155347824097, Test Loss: 0.6907505393028259\n",
      "Epoch 496/1000, Train Loss: 0.6906139254570007, Test Loss: 0.6907491683959961\n",
      "Epoch 497/1000, Train Loss: 0.6906123161315918, Test Loss: 0.6907477378845215\n",
      "Epoch 498/1000, Train Loss: 0.6906107664108276, Test Loss: 0.6907463073730469\n",
      "Epoch 499/1000, Train Loss: 0.6906091570854187, Test Loss: 0.690744936466217\n",
      "Epoch 500/1000, Train Loss: 0.6906075477600098, Test Loss: 0.6907435655593872\n",
      "Epoch 501/1000, Train Loss: 0.6906059384346008, Test Loss: 0.6907421946525574\n",
      "Epoch 502/1000, Train Loss: 0.6906042695045471, Test Loss: 0.6907408237457275\n",
      "Epoch 503/1000, Train Loss: 0.6906027793884277, Test Loss: 0.6907393932342529\n",
      "Epoch 504/1000, Train Loss: 0.6906011700630188, Test Loss: 0.6907379627227783\n",
      "Epoch 505/1000, Train Loss: 0.6905995607376099, Test Loss: 0.690736711025238\n",
      "Epoch 506/1000, Train Loss: 0.6905979514122009, Test Loss: 0.6907352805137634\n",
      "Epoch 507/1000, Train Loss: 0.6905964016914368, Test Loss: 0.6907340288162231\n",
      "Epoch 508/1000, Train Loss: 0.6905947327613831, Test Loss: 0.6907326579093933\n",
      "Epoch 509/1000, Train Loss: 0.6905932426452637, Test Loss: 0.6907312273979187\n",
      "Epoch 510/1000, Train Loss: 0.6905916929244995, Test Loss: 0.6907299160957336\n",
      "Epoch 511/1000, Train Loss: 0.6905901432037354, Test Loss: 0.6907286047935486\n",
      "Epoch 512/1000, Train Loss: 0.6905885934829712, Test Loss: 0.690727174282074\n",
      "Epoch 513/1000, Train Loss: 0.6905871033668518, Test Loss: 0.6907258033752441\n",
      "Epoch 514/1000, Train Loss: 0.6905855536460876, Test Loss: 0.6907246112823486\n",
      "Epoch 515/1000, Train Loss: 0.6905840635299683, Test Loss: 0.6907232403755188\n",
      "Epoch 516/1000, Train Loss: 0.6905825138092041, Test Loss: 0.6907219290733337\n",
      "Epoch 517/1000, Train Loss: 0.6905809640884399, Test Loss: 0.6907206177711487\n",
      "Epoch 518/1000, Train Loss: 0.6905794739723206, Test Loss: 0.6907192468643188\n",
      "Epoch 519/1000, Train Loss: 0.6905779838562012, Test Loss: 0.690717875957489\n",
      "Epoch 520/1000, Train Loss: 0.690576434135437, Test Loss: 0.6907166242599487\n",
      "Epoch 521/1000, Train Loss: 0.6905750036239624, Test Loss: 0.6907153725624084\n",
      "Epoch 522/1000, Train Loss: 0.690573513507843, Test Loss: 0.6907141208648682\n",
      "Epoch 523/1000, Train Loss: 0.6905720233917236, Test Loss: 0.6907128095626831\n",
      "Epoch 524/1000, Train Loss: 0.6905704736709595, Test Loss: 0.6907114386558533\n",
      "Epoch 525/1000, Train Loss: 0.6905690431594849, Test Loss: 0.690710186958313\n",
      "Epoch 526/1000, Train Loss: 0.6905675530433655, Test Loss: 0.6907088756561279\n",
      "Epoch 527/1000, Train Loss: 0.6905660629272461, Test Loss: 0.6907076835632324\n",
      "Epoch 528/1000, Train Loss: 0.6905646920204163, Test Loss: 0.6907063722610474\n",
      "Epoch 529/1000, Train Loss: 0.6905631422996521, Test Loss: 0.6907051205635071\n",
      "Epoch 530/1000, Train Loss: 0.6905617117881775, Test Loss: 0.6907038688659668\n",
      "Epoch 531/1000, Train Loss: 0.6905601620674133, Test Loss: 0.6907025575637817\n",
      "Epoch 532/1000, Train Loss: 0.6905587315559387, Test Loss: 0.6907013058662415\n",
      "Epoch 533/1000, Train Loss: 0.6905573010444641, Test Loss: 0.6907000541687012\n",
      "Epoch 534/1000, Train Loss: 0.6905558705329895, Test Loss: 0.6906988024711609\n",
      "Epoch 535/1000, Train Loss: 0.6905544400215149, Test Loss: 0.6906975507736206\n",
      "Epoch 536/1000, Train Loss: 0.6905529499053955, Test Loss: 0.6906964182853699\n",
      "Epoch 537/1000, Train Loss: 0.6905515193939209, Test Loss: 0.69069504737854\n",
      "Epoch 538/1000, Train Loss: 0.6905501484870911, Test Loss: 0.6906937956809998\n",
      "Epoch 539/1000, Train Loss: 0.6905486583709717, Test Loss: 0.6906926035881042\n",
      "Epoch 540/1000, Train Loss: 0.6905473470687866, Test Loss: 0.690691351890564\n",
      "Epoch 541/1000, Train Loss: 0.6905457973480225, Test Loss: 0.6906902194023132\n",
      "Epoch 542/1000, Train Loss: 0.6905444860458374, Test Loss: 0.6906888484954834\n",
      "Epoch 543/1000, Train Loss: 0.6905431151390076, Test Loss: 0.6906877160072327\n",
      "Epoch 544/1000, Train Loss: 0.6905417442321777, Test Loss: 0.6906864643096924\n",
      "Epoch 545/1000, Train Loss: 0.6905401945114136, Test Loss: 0.6906852126121521\n",
      "Epoch 546/1000, Train Loss: 0.6905388832092285, Test Loss: 0.6906839609146118\n",
      "Epoch 547/1000, Train Loss: 0.6905375123023987, Test Loss: 0.6906828284263611\n",
      "Epoch 548/1000, Train Loss: 0.6905360817909241, Test Loss: 0.6906815767288208\n",
      "Epoch 549/1000, Train Loss: 0.6905346512794495, Test Loss: 0.6906803846359253\n",
      "Epoch 550/1000, Train Loss: 0.6905332803726196, Test Loss: 0.6906791925430298\n",
      "Epoch 551/1000, Train Loss: 0.6905319690704346, Test Loss: 0.690678060054779\n",
      "Epoch 552/1000, Train Loss: 0.6905305981636047, Test Loss: 0.6906768083572388\n",
      "Epoch 553/1000, Train Loss: 0.6905291676521301, Test Loss: 0.6906756162643433\n",
      "Epoch 554/1000, Train Loss: 0.6905278563499451, Test Loss: 0.690674364566803\n",
      "Epoch 555/1000, Train Loss: 0.6905264854431152, Test Loss: 0.6906732320785522\n",
      "Epoch 556/1000, Train Loss: 0.6905250549316406, Test Loss: 0.6906720995903015\n",
      "Epoch 557/1000, Train Loss: 0.6905238032341003, Test Loss: 0.6906708478927612\n",
      "Epoch 558/1000, Train Loss: 0.6905224323272705, Test Loss: 0.6906696557998657\n",
      "Epoch 559/1000, Train Loss: 0.6905210018157959, Test Loss: 0.690668523311615\n",
      "Epoch 560/1000, Train Loss: 0.6905196905136108, Test Loss: 0.6906673908233643\n",
      "Epoch 561/1000, Train Loss: 0.6905183792114258, Test Loss: 0.6906661987304688\n",
      "Epoch 562/1000, Train Loss: 0.6905170679092407, Test Loss: 0.6906649470329285\n",
      "Epoch 563/1000, Train Loss: 0.6905157566070557, Test Loss: 0.6906638145446777\n",
      "Epoch 564/1000, Train Loss: 0.6905143857002258, Test Loss: 0.690662682056427\n",
      "Epoch 565/1000, Train Loss: 0.690513014793396, Test Loss: 0.6906614899635315\n",
      "Epoch 566/1000, Train Loss: 0.6905117034912109, Test Loss: 0.6906604170799255\n",
      "Epoch 567/1000, Train Loss: 0.6905103921890259, Test Loss: 0.6906591653823853\n",
      "Epoch 568/1000, Train Loss: 0.690509021282196, Test Loss: 0.6906580328941345\n",
      "Epoch 569/1000, Train Loss: 0.6905077695846558, Test Loss: 0.6906569004058838\n",
      "Epoch 570/1000, Train Loss: 0.6905063986778259, Test Loss: 0.6906557679176331\n",
      "Epoch 571/1000, Train Loss: 0.6905051469802856, Test Loss: 0.6906545162200928\n",
      "Epoch 572/1000, Train Loss: 0.6905038356781006, Test Loss: 0.6906534433364868\n",
      "Epoch 573/1000, Train Loss: 0.6905024647712708, Test Loss: 0.6906523108482361\n",
      "Epoch 574/1000, Train Loss: 0.6905012726783752, Test Loss: 0.6906511187553406\n",
      "Epoch 575/1000, Train Loss: 0.6904999613761902, Test Loss: 0.6906500458717346\n",
      "Epoch 576/1000, Train Loss: 0.6904985904693604, Test Loss: 0.6906488537788391\n",
      "Epoch 577/1000, Train Loss: 0.6904973387718201, Test Loss: 0.6906477808952332\n",
      "Epoch 578/1000, Train Loss: 0.690496027469635, Test Loss: 0.6906466484069824\n",
      "Epoch 579/1000, Train Loss: 0.6904948353767395, Test Loss: 0.6906454563140869\n",
      "Epoch 580/1000, Train Loss: 0.6904935240745544, Test Loss: 0.6906444430351257\n",
      "Epoch 581/1000, Train Loss: 0.6904922127723694, Test Loss: 0.690643310546875\n",
      "Epoch 582/1000, Train Loss: 0.6904909014701843, Test Loss: 0.6906421184539795\n",
      "Epoch 583/1000, Train Loss: 0.690489649772644, Test Loss: 0.6906409859657288\n",
      "Epoch 584/1000, Train Loss: 0.6904884576797485, Test Loss: 0.6906399130821228\n",
      "Epoch 585/1000, Train Loss: 0.6904870867729187, Test Loss: 0.6906388401985168\n",
      "Epoch 586/1000, Train Loss: 0.6904858946800232, Test Loss: 0.6906376481056213\n",
      "Epoch 587/1000, Train Loss: 0.6904845833778381, Test Loss: 0.6906365752220154\n",
      "Epoch 588/1000, Train Loss: 0.6904832124710083, Test Loss: 0.6906354427337646\n",
      "Epoch 589/1000, Train Loss: 0.6904820799827576, Test Loss: 0.6906343102455139\n",
      "Epoch 590/1000, Train Loss: 0.6904808282852173, Test Loss: 0.6906331777572632\n",
      "Epoch 591/1000, Train Loss: 0.690479576587677, Test Loss: 0.6906321048736572\n",
      "Epoch 592/1000, Train Loss: 0.6904782652854919, Test Loss: 0.6906309127807617\n",
      "Epoch 593/1000, Train Loss: 0.6904770731925964, Test Loss: 0.6906298995018005\n",
      "Epoch 594/1000, Train Loss: 0.6904758214950562, Test Loss: 0.690628707408905\n",
      "Epoch 595/1000, Train Loss: 0.6904745697975159, Test Loss: 0.6906276345252991\n",
      "Epoch 596/1000, Train Loss: 0.6904733180999756, Test Loss: 0.6906265616416931\n",
      "Epoch 597/1000, Train Loss: 0.6904720664024353, Test Loss: 0.6906255483627319\n",
      "Epoch 598/1000, Train Loss: 0.6904708743095398, Test Loss: 0.6906244158744812\n",
      "Epoch 599/1000, Train Loss: 0.6904696226119995, Test Loss: 0.6906232833862305\n",
      "Epoch 600/1000, Train Loss: 0.6904683709144592, Test Loss: 0.6906222105026245\n",
      "Epoch 601/1000, Train Loss: 0.6904671788215637, Test Loss: 0.6906211376190186\n",
      "Epoch 602/1000, Train Loss: 0.6904658675193787, Test Loss: 0.6906200647354126\n",
      "Epoch 603/1000, Train Loss: 0.6904647350311279, Test Loss: 0.6906189322471619\n",
      "Epoch 604/1000, Train Loss: 0.6904636025428772, Test Loss: 0.6906178593635559\n",
      "Epoch 605/1000, Train Loss: 0.6904622316360474, Test Loss: 0.69061678647995\n",
      "Epoch 606/1000, Train Loss: 0.6904610991477966, Test Loss: 0.6906157732009888\n",
      "Epoch 607/1000, Train Loss: 0.6904597878456116, Test Loss: 0.690614640712738\n",
      "Epoch 608/1000, Train Loss: 0.6904586553573608, Test Loss: 0.6906135678291321\n",
      "Epoch 609/1000, Train Loss: 0.6904574036598206, Test Loss: 0.6906124353408813\n",
      "Epoch 610/1000, Train Loss: 0.690456211566925, Test Loss: 0.6906114220619202\n",
      "Epoch 611/1000, Train Loss: 0.6904549598693848, Test Loss: 0.6906103491783142\n",
      "Epoch 612/1000, Train Loss: 0.6904537677764893, Test Loss: 0.6906092762947083\n",
      "Epoch 613/1000, Train Loss: 0.6904525756835938, Test Loss: 0.6906082630157471\n",
      "Epoch 614/1000, Train Loss: 0.690451443195343, Test Loss: 0.6906072497367859\n",
      "Epoch 615/1000, Train Loss: 0.6904501914978027, Test Loss: 0.6906060576438904\n",
      "Epoch 616/1000, Train Loss: 0.690449059009552, Test Loss: 0.6906050443649292\n",
      "Epoch 617/1000, Train Loss: 0.6904477477073669, Test Loss: 0.690604031085968\n",
      "Epoch 618/1000, Train Loss: 0.6904466152191162, Test Loss: 0.6906029582023621\n",
      "Epoch 619/1000, Train Loss: 0.6904453635215759, Test Loss: 0.6906018257141113\n",
      "Epoch 620/1000, Train Loss: 0.69044429063797, Test Loss: 0.6906008720397949\n",
      "Epoch 621/1000, Train Loss: 0.6904430985450745, Test Loss: 0.6905996799468994\n",
      "Epoch 622/1000, Train Loss: 0.690441906452179, Test Loss: 0.690598726272583\n",
      "Epoch 623/1000, Train Loss: 0.6904405951499939, Test Loss: 0.690597653388977\n",
      "Epoch 624/1000, Train Loss: 0.6904394626617432, Test Loss: 0.6905965805053711\n",
      "Epoch 625/1000, Train Loss: 0.6904383301734924, Test Loss: 0.6905955672264099\n",
      "Epoch 626/1000, Train Loss: 0.6904371976852417, Test Loss: 0.6905945539474487\n",
      "Epoch 627/1000, Train Loss: 0.6904360055923462, Test Loss: 0.6905934810638428\n",
      "Epoch 628/1000, Train Loss: 0.6904348731040955, Test Loss: 0.6905924081802368\n",
      "Epoch 629/1000, Train Loss: 0.6904336214065552, Test Loss: 0.6905913949012756\n",
      "Epoch 630/1000, Train Loss: 0.6904324889183044, Test Loss: 0.6905903816223145\n",
      "Epoch 631/1000, Train Loss: 0.6904312968254089, Test Loss: 0.6905893087387085\n",
      "Epoch 632/1000, Train Loss: 0.6904301047325134, Test Loss: 0.6905882954597473\n",
      "Epoch 633/1000, Train Loss: 0.6904289722442627, Test Loss: 0.6905872821807861\n",
      "Epoch 634/1000, Train Loss: 0.6904277801513672, Test Loss: 0.6905862092971802\n",
      "Epoch 635/1000, Train Loss: 0.6904267072677612, Test Loss: 0.6905851364135742\n",
      "Epoch 636/1000, Train Loss: 0.690425455570221, Test Loss: 0.690584123134613\n",
      "Epoch 637/1000, Train Loss: 0.6904243230819702, Test Loss: 0.6905831098556519\n",
      "Epoch 638/1000, Train Loss: 0.6904231905937195, Test Loss: 0.6905820369720459\n",
      "Epoch 639/1000, Train Loss: 0.690421998500824, Test Loss: 0.6905810236930847\n",
      "Epoch 640/1000, Train Loss: 0.6904208660125732, Test Loss: 0.6905799508094788\n",
      "Epoch 641/1000, Train Loss: 0.6904196739196777, Test Loss: 0.6905789971351624\n",
      "Epoch 642/1000, Train Loss: 0.6904186010360718, Test Loss: 0.6905778646469116\n",
      "Epoch 643/1000, Train Loss: 0.6904174089431763, Test Loss: 0.69057697057724\n",
      "Epoch 644/1000, Train Loss: 0.6904163360595703, Test Loss: 0.690575897693634\n",
      "Epoch 645/1000, Train Loss: 0.6904151439666748, Test Loss: 0.6905748844146729\n",
      "Epoch 646/1000, Train Loss: 0.6904140114784241, Test Loss: 0.6905738711357117\n",
      "Epoch 647/1000, Train Loss: 0.6904128193855286, Test Loss: 0.6905728578567505\n",
      "Epoch 648/1000, Train Loss: 0.6904116868972778, Test Loss: 0.6905717253684998\n",
      "Epoch 649/1000, Train Loss: 0.6904105544090271, Test Loss: 0.6905708312988281\n",
      "Epoch 650/1000, Train Loss: 0.6904094219207764, Test Loss: 0.6905697584152222\n",
      "Epoch 651/1000, Train Loss: 0.6904083490371704, Test Loss: 0.6905686855316162\n",
      "Epoch 652/1000, Train Loss: 0.6904071569442749, Test Loss: 0.6905677318572998\n",
      "Epoch 653/1000, Train Loss: 0.6904060244560242, Test Loss: 0.6905667185783386\n",
      "Epoch 654/1000, Train Loss: 0.6904049515724182, Test Loss: 0.6905656456947327\n",
      "Epoch 655/1000, Train Loss: 0.6904037594795227, Test Loss: 0.6905646920204163\n",
      "Epoch 656/1000, Train Loss: 0.6904028058052063, Test Loss: 0.6905636191368103\n",
      "Epoch 657/1000, Train Loss: 0.690401554107666, Test Loss: 0.6905626058578491\n",
      "Epoch 658/1000, Train Loss: 0.6904004216194153, Test Loss: 0.6905617117881775\n",
      "Epoch 659/1000, Train Loss: 0.6903993487358093, Test Loss: 0.6905606389045715\n",
      "Epoch 660/1000, Train Loss: 0.6903982758522034, Test Loss: 0.6905595660209656\n",
      "Epoch 661/1000, Train Loss: 0.6903970837593079, Test Loss: 0.6905586123466492\n",
      "Epoch 662/1000, Train Loss: 0.6903960108757019, Test Loss: 0.6905576586723328\n",
      "Epoch 663/1000, Train Loss: 0.6903947591781616, Test Loss: 0.6905565857887268\n",
      "Epoch 664/1000, Train Loss: 0.6903937458992004, Test Loss: 0.6905555725097656\n",
      "Epoch 665/1000, Train Loss: 0.6903924942016602, Test Loss: 0.6905545592308044\n",
      "Epoch 666/1000, Train Loss: 0.690391480922699, Test Loss: 0.690553605556488\n",
      "Epoch 667/1000, Train Loss: 0.690390408039093, Test Loss: 0.6905525922775269\n",
      "Epoch 668/1000, Train Loss: 0.6903892159461975, Test Loss: 0.6905515789985657\n",
      "Epoch 669/1000, Train Loss: 0.6903882026672363, Test Loss: 0.6905505657196045\n",
      "Epoch 670/1000, Train Loss: 0.6903870105743408, Test Loss: 0.6905496716499329\n",
      "Epoch 671/1000, Train Loss: 0.6903859376907349, Test Loss: 0.6905485987663269\n",
      "Epoch 672/1000, Train Loss: 0.6903848648071289, Test Loss: 0.6905476450920105\n",
      "Epoch 673/1000, Train Loss: 0.6903837323188782, Test Loss: 0.6905465126037598\n",
      "Epoch 674/1000, Train Loss: 0.690382719039917, Test Loss: 0.6905455589294434\n",
      "Epoch 675/1000, Train Loss: 0.6903815269470215, Test Loss: 0.6905445456504822\n",
      "Epoch 676/1000, Train Loss: 0.6903803944587708, Test Loss: 0.6905435919761658\n",
      "Epoch 677/1000, Train Loss: 0.6903793811798096, Test Loss: 0.6905425786972046\n",
      "Epoch 678/1000, Train Loss: 0.6903782486915588, Test Loss: 0.6905415654182434\n",
      "Epoch 679/1000, Train Loss: 0.6903771162033081, Test Loss: 0.6905406713485718\n",
      "Epoch 680/1000, Train Loss: 0.6903760433197021, Test Loss: 0.6905396580696106\n",
      "Epoch 681/1000, Train Loss: 0.6903749704360962, Test Loss: 0.6905385851860046\n",
      "Epoch 682/1000, Train Loss: 0.6903738975524902, Test Loss: 0.6905375719070435\n",
      "Epoch 683/1000, Train Loss: 0.6903727650642395, Test Loss: 0.6905366778373718\n",
      "Epoch 684/1000, Train Loss: 0.6903716921806335, Test Loss: 0.6905356049537659\n",
      "Epoch 685/1000, Train Loss: 0.6903706192970276, Test Loss: 0.6905346512794495\n",
      "Epoch 686/1000, Train Loss: 0.6903695464134216, Test Loss: 0.6905336976051331\n",
      "Epoch 687/1000, Train Loss: 0.6903684139251709, Test Loss: 0.6905327439308167\n",
      "Epoch 688/1000, Train Loss: 0.6903673410415649, Test Loss: 0.6905316710472107\n",
      "Epoch 689/1000, Train Loss: 0.690366268157959, Test Loss: 0.6905307173728943\n",
      "Epoch 690/1000, Train Loss: 0.6903651356697083, Test Loss: 0.6905297636985779\n",
      "Epoch 691/1000, Train Loss: 0.6903641223907471, Test Loss: 0.6905288100242615\n",
      "Epoch 692/1000, Train Loss: 0.6903629899024963, Test Loss: 0.6905277967453003\n",
      "Epoch 693/1000, Train Loss: 0.6903618574142456, Test Loss: 0.6905267834663391\n",
      "Epoch 694/1000, Train Loss: 0.6903609037399292, Test Loss: 0.6905257701873779\n",
      "Epoch 695/1000, Train Loss: 0.6903598308563232, Test Loss: 0.6905248165130615\n",
      "Epoch 696/1000, Train Loss: 0.6903586983680725, Test Loss: 0.6905238628387451\n",
      "Epoch 697/1000, Train Loss: 0.6903576254844666, Test Loss: 0.6905227899551392\n",
      "Epoch 698/1000, Train Loss: 0.6903566122055054, Test Loss: 0.6905218362808228\n",
      "Epoch 699/1000, Train Loss: 0.6903554797172546, Test Loss: 0.6905208230018616\n",
      "Epoch 700/1000, Train Loss: 0.6903544664382935, Test Loss: 0.6905199885368347\n",
      "Epoch 701/1000, Train Loss: 0.690353274345398, Test Loss: 0.6905189156532288\n",
      "Epoch 702/1000, Train Loss: 0.6903522610664368, Test Loss: 0.6905178427696228\n",
      "Epoch 703/1000, Train Loss: 0.6903511881828308, Test Loss: 0.6905168890953064\n",
      "Epoch 704/1000, Train Loss: 0.6903501152992249, Test Loss: 0.6905159950256348\n",
      "Epoch 705/1000, Train Loss: 0.6903490424156189, Test Loss: 0.6905150413513184\n",
      "Epoch 706/1000, Train Loss: 0.6903479695320129, Test Loss: 0.6905140280723572\n",
      "Epoch 707/1000, Train Loss: 0.690346896648407, Test Loss: 0.6905130743980408\n",
      "Epoch 708/1000, Train Loss: 0.6903457641601562, Test Loss: 0.6905120015144348\n",
      "Epoch 709/1000, Train Loss: 0.6903447508811951, Test Loss: 0.690511167049408\n",
      "Epoch 710/1000, Train Loss: 0.6903437376022339, Test Loss: 0.690510094165802\n",
      "Epoch 711/1000, Train Loss: 0.6903425455093384, Test Loss: 0.6905091404914856\n",
      "Epoch 712/1000, Train Loss: 0.6903416514396667, Test Loss: 0.6905080676078796\n",
      "Epoch 713/1000, Train Loss: 0.690340518951416, Test Loss: 0.690507173538208\n",
      "Epoch 714/1000, Train Loss: 0.6903394460678101, Test Loss: 0.6905062794685364\n",
      "Epoch 715/1000, Train Loss: 0.6903384327888489, Test Loss: 0.6905052065849304\n",
      "Epoch 716/1000, Train Loss: 0.6903373003005981, Test Loss: 0.690504252910614\n",
      "Epoch 717/1000, Train Loss: 0.6903362274169922, Test Loss: 0.6905032992362976\n",
      "Epoch 718/1000, Train Loss: 0.690335214138031, Test Loss: 0.6905022859573364\n",
      "Epoch 719/1000, Train Loss: 0.6903342008590698, Test Loss: 0.6905013918876648\n",
      "Epoch 720/1000, Train Loss: 0.6903330683708191, Test Loss: 0.6905004382133484\n",
      "Epoch 721/1000, Train Loss: 0.6903320550918579, Test Loss: 0.6904993653297424\n",
      "Epoch 722/1000, Train Loss: 0.6903310418128967, Test Loss: 0.690498411655426\n",
      "Epoch 723/1000, Train Loss: 0.6903299689292908, Test Loss: 0.6904975175857544\n",
      "Epoch 724/1000, Train Loss: 0.6903288960456848, Test Loss: 0.6904966235160828\n",
      "Epoch 725/1000, Train Loss: 0.6903278231620789, Test Loss: 0.6904956102371216\n",
      "Epoch 726/1000, Train Loss: 0.6903266906738281, Test Loss: 0.6904945969581604\n",
      "Epoch 727/1000, Train Loss: 0.6903257369995117, Test Loss: 0.690493643283844\n",
      "Epoch 728/1000, Train Loss: 0.6903247237205505, Test Loss: 0.6904926896095276\n",
      "Epoch 729/1000, Train Loss: 0.690323531627655, Test Loss: 0.6904917359352112\n",
      "Epoch 730/1000, Train Loss: 0.6903225183486938, Test Loss: 0.6904907822608948\n",
      "Epoch 731/1000, Train Loss: 0.6903215050697327, Test Loss: 0.6904897093772888\n",
      "Epoch 732/1000, Train Loss: 0.6903205513954163, Test Loss: 0.6904886960983276\n",
      "Epoch 733/1000, Train Loss: 0.6903194189071655, Test Loss: 0.6904879212379456\n",
      "Epoch 734/1000, Train Loss: 0.6903183460235596, Test Loss: 0.6904869079589844\n",
      "Epoch 735/1000, Train Loss: 0.6903173923492432, Test Loss: 0.690485954284668\n",
      "Epoch 736/1000, Train Loss: 0.6903163194656372, Test Loss: 0.6904849410057068\n",
      "Epoch 737/1000, Train Loss: 0.6903152465820312, Test Loss: 0.6904838681221008\n",
      "Epoch 738/1000, Train Loss: 0.6903142333030701, Test Loss: 0.690483033657074\n",
      "Epoch 739/1000, Train Loss: 0.6903132200241089, Test Loss: 0.6904820799827576\n",
      "Epoch 740/1000, Train Loss: 0.6903120875358582, Test Loss: 0.6904811263084412\n",
      "Epoch 741/1000, Train Loss: 0.6903111338615417, Test Loss: 0.6904800534248352\n",
      "Epoch 742/1000, Train Loss: 0.690310001373291, Test Loss: 0.6904790997505188\n",
      "Epoch 743/1000, Train Loss: 0.6903090476989746, Test Loss: 0.6904782652854919\n",
      "Epoch 744/1000, Train Loss: 0.6903079152107239, Test Loss: 0.6904772520065308\n",
      "Epoch 745/1000, Train Loss: 0.6903070211410522, Test Loss: 0.6904762983322144\n",
      "Epoch 746/1000, Train Loss: 0.6903058886528015, Test Loss: 0.6904752850532532\n",
      "Epoch 747/1000, Train Loss: 0.6903048157691956, Test Loss: 0.6904743313789368\n",
      "Epoch 748/1000, Train Loss: 0.6903038024902344, Test Loss: 0.6904733180999756\n",
      "Epoch 749/1000, Train Loss: 0.6903027892112732, Test Loss: 0.690472424030304\n",
      "Epoch 750/1000, Train Loss: 0.6903017163276672, Test Loss: 0.6904714703559875\n",
      "Epoch 751/1000, Train Loss: 0.690300703048706, Test Loss: 0.6904705166816711\n",
      "Epoch 752/1000, Train Loss: 0.6902996301651001, Test Loss: 0.6904695630073547\n",
      "Epoch 753/1000, Train Loss: 0.6902986168861389, Test Loss: 0.6904685497283936\n",
      "Epoch 754/1000, Train Loss: 0.6902976036071777, Test Loss: 0.6904676556587219\n",
      "Epoch 755/1000, Train Loss: 0.6902965307235718, Test Loss: 0.6904667615890503\n",
      "Epoch 756/1000, Train Loss: 0.6902955174446106, Test Loss: 0.6904657483100891\n",
      "Epoch 757/1000, Train Loss: 0.6902944445610046, Test Loss: 0.6904646754264832\n",
      "Epoch 758/1000, Train Loss: 0.6902934908866882, Test Loss: 0.6904637813568115\n",
      "Epoch 759/1000, Train Loss: 0.6902924180030823, Test Loss: 0.6904628276824951\n",
      "Epoch 760/1000, Train Loss: 0.6902912855148315, Test Loss: 0.6904619336128235\n",
      "Epoch 761/1000, Train Loss: 0.6902903318405151, Test Loss: 0.6904609203338623\n",
      "Epoch 762/1000, Train Loss: 0.6902894377708435, Test Loss: 0.6904599070549011\n",
      "Epoch 763/1000, Train Loss: 0.6902883052825928, Test Loss: 0.6904590129852295\n",
      "Epoch 764/1000, Train Loss: 0.690287172794342, Test Loss: 0.6904579997062683\n",
      "Epoch 765/1000, Train Loss: 0.6902862787246704, Test Loss: 0.6904571652412415\n",
      "Epoch 766/1000, Train Loss: 0.6902852058410645, Test Loss: 0.6904560923576355\n",
      "Epoch 767/1000, Train Loss: 0.6902841329574585, Test Loss: 0.6904551982879639\n",
      "Epoch 768/1000, Train Loss: 0.6902831196784973, Test Loss: 0.6904541850090027\n",
      "Epoch 769/1000, Train Loss: 0.6902821660041809, Test Loss: 0.690453290939331\n",
      "Epoch 770/1000, Train Loss: 0.6902811527252197, Test Loss: 0.6904523372650146\n",
      "Epoch 771/1000, Train Loss: 0.690280020236969, Test Loss: 0.6904513239860535\n",
      "Epoch 772/1000, Train Loss: 0.6902790665626526, Test Loss: 0.6904503107070923\n",
      "Epoch 773/1000, Train Loss: 0.6902781128883362, Test Loss: 0.6904494166374207\n",
      "Epoch 774/1000, Train Loss: 0.6902769804000854, Test Loss: 0.690448522567749\n",
      "Epoch 775/1000, Train Loss: 0.6902759075164795, Test Loss: 0.6904475688934326\n",
      "Epoch 776/1000, Train Loss: 0.6902750134468079, Test Loss: 0.6904466152191162\n",
      "Epoch 777/1000, Train Loss: 0.6902739405632019, Test Loss: 0.690445601940155\n",
      "Epoch 778/1000, Train Loss: 0.690272867679596, Test Loss: 0.6904446482658386\n",
      "Epoch 779/1000, Train Loss: 0.6902718544006348, Test Loss: 0.690443754196167\n",
      "Epoch 780/1000, Train Loss: 0.6902708411216736, Test Loss: 0.6904428005218506\n",
      "Epoch 781/1000, Train Loss: 0.6902698278427124, Test Loss: 0.6904417872428894\n",
      "Epoch 782/1000, Train Loss: 0.6902688145637512, Test Loss: 0.690440833568573\n",
      "Epoch 783/1000, Train Loss: 0.6902677416801453, Test Loss: 0.6904398798942566\n",
      "Epoch 784/1000, Train Loss: 0.6902667880058289, Test Loss: 0.6904389262199402\n",
      "Epoch 785/1000, Train Loss: 0.6902657747268677, Test Loss: 0.6904380321502686\n",
      "Epoch 786/1000, Train Loss: 0.6902647018432617, Test Loss: 0.6904370784759521\n",
      "Epoch 787/1000, Train Loss: 0.6902636885643005, Test Loss: 0.6904360055923462\n",
      "Epoch 788/1000, Train Loss: 0.6902626752853394, Test Loss: 0.6904351711273193\n",
      "Epoch 789/1000, Train Loss: 0.6902616024017334, Test Loss: 0.6904341578483582\n",
      "Epoch 790/1000, Train Loss: 0.690260648727417, Test Loss: 0.6904332637786865\n",
      "Epoch 791/1000, Train Loss: 0.6902597546577454, Test Loss: 0.6904323101043701\n",
      "Epoch 792/1000, Train Loss: 0.6902585029602051, Test Loss: 0.6904312372207642\n",
      "Epoch 793/1000, Train Loss: 0.6902575492858887, Test Loss: 0.6904302835464478\n",
      "Epoch 794/1000, Train Loss: 0.6902565956115723, Test Loss: 0.6904293894767761\n",
      "Epoch 795/1000, Train Loss: 0.6902555823326111, Test Loss: 0.6904284358024597\n",
      "Epoch 796/1000, Train Loss: 0.6902545094490051, Test Loss: 0.6904275417327881\n",
      "Epoch 797/1000, Train Loss: 0.6902534365653992, Test Loss: 0.6904264688491821\n",
      "Epoch 798/1000, Train Loss: 0.6902525424957275, Test Loss: 0.6904255151748657\n",
      "Epoch 799/1000, Train Loss: 0.6902514696121216, Test Loss: 0.6904246211051941\n",
      "Epoch 800/1000, Train Loss: 0.6902503967285156, Test Loss: 0.6904236674308777\n",
      "Epoch 801/1000, Train Loss: 0.6902493834495544, Test Loss: 0.690422773361206\n",
      "Epoch 802/1000, Train Loss: 0.6902484893798828, Test Loss: 0.6904218196868896\n",
      "Epoch 803/1000, Train Loss: 0.6902474761009216, Test Loss: 0.6904208660125732\n",
      "Epoch 804/1000, Train Loss: 0.6902464032173157, Test Loss: 0.6904198527336121\n",
      "Epoch 805/1000, Train Loss: 0.6902453899383545, Test Loss: 0.6904188990592957\n",
      "Epoch 806/1000, Train Loss: 0.6902443766593933, Test Loss: 0.6904179453849792\n",
      "Epoch 807/1000, Train Loss: 0.6902434229850769, Test Loss: 0.6904170513153076\n",
      "Epoch 808/1000, Train Loss: 0.6902424097061157, Test Loss: 0.6904160380363464\n",
      "Epoch 809/1000, Train Loss: 0.6902413368225098, Test Loss: 0.69041508436203\n",
      "Epoch 810/1000, Train Loss: 0.6902403235435486, Test Loss: 0.6904141306877136\n",
      "Epoch 811/1000, Train Loss: 0.6902393102645874, Test Loss: 0.6904131770133972\n",
      "Epoch 812/1000, Train Loss: 0.690238356590271, Test Loss: 0.6904123425483704\n",
      "Epoch 813/1000, Train Loss: 0.6902373433113098, Test Loss: 0.6904113292694092\n",
      "Epoch 814/1000, Train Loss: 0.6902362704277039, Test Loss: 0.690410315990448\n",
      "Epoch 815/1000, Train Loss: 0.6902352571487427, Test Loss: 0.6904093623161316\n",
      "Epoch 816/1000, Train Loss: 0.6902342438697815, Test Loss: 0.6904084086418152\n",
      "Epoch 817/1000, Train Loss: 0.6902333498001099, Test Loss: 0.6904074549674988\n",
      "Epoch 818/1000, Train Loss: 0.6902322769165039, Test Loss: 0.6904065012931824\n",
      "Epoch 819/1000, Train Loss: 0.690231204032898, Test Loss: 0.690405547618866\n",
      "Epoch 820/1000, Train Loss: 0.6902301907539368, Test Loss: 0.6904045939445496\n",
      "Epoch 821/1000, Train Loss: 0.6902292966842651, Test Loss: 0.6904036998748779\n",
      "Epoch 822/1000, Train Loss: 0.6902282238006592, Test Loss: 0.6904026865959167\n",
      "Epoch 823/1000, Train Loss: 0.690227210521698, Test Loss: 0.6904017329216003\n",
      "Epoch 824/1000, Train Loss: 0.6902261972427368, Test Loss: 0.6904008388519287\n",
      "Epoch 825/1000, Train Loss: 0.6902251243591309, Test Loss: 0.6903998255729675\n",
      "Epoch 826/1000, Train Loss: 0.6902241706848145, Test Loss: 0.6903987526893616\n",
      "Epoch 827/1000, Train Loss: 0.6902231574058533, Test Loss: 0.6903979778289795\n",
      "Epoch 828/1000, Train Loss: 0.6902220845222473, Test Loss: 0.6903969645500183\n",
      "Epoch 829/1000, Train Loss: 0.6902210712432861, Test Loss: 0.6903960108757019\n",
      "Epoch 830/1000, Train Loss: 0.6902201771736145, Test Loss: 0.6903950572013855\n",
      "Epoch 831/1000, Train Loss: 0.6902192234992981, Test Loss: 0.6903941035270691\n",
      "Epoch 832/1000, Train Loss: 0.6902180910110474, Test Loss: 0.6903931498527527\n",
      "Epoch 833/1000, Train Loss: 0.6902170777320862, Test Loss: 0.6903921365737915\n",
      "Epoch 834/1000, Train Loss: 0.6902161240577698, Test Loss: 0.6903911828994751\n",
      "Epoch 835/1000, Train Loss: 0.6902150511741638, Test Loss: 0.6903902292251587\n",
      "Epoch 836/1000, Train Loss: 0.6902140974998474, Test Loss: 0.6903892755508423\n",
      "Epoch 837/1000, Train Loss: 0.6902130246162415, Test Loss: 0.6903883218765259\n",
      "Epoch 838/1000, Train Loss: 0.690212070941925, Test Loss: 0.6903873682022095\n",
      "Epoch 839/1000, Train Loss: 0.6902109980583191, Test Loss: 0.6903864145278931\n",
      "Epoch 840/1000, Train Loss: 0.6902100443840027, Test Loss: 0.6903854608535767\n",
      "Epoch 841/1000, Train Loss: 0.6902090907096863, Test Loss: 0.6903845071792603\n",
      "Epoch 842/1000, Train Loss: 0.6902080178260803, Test Loss: 0.6903835535049438\n",
      "Epoch 843/1000, Train Loss: 0.6902070641517639, Test Loss: 0.6903826594352722\n",
      "Epoch 844/1000, Train Loss: 0.690205991268158, Test Loss: 0.6903815865516663\n",
      "Epoch 845/1000, Train Loss: 0.6902050375938416, Test Loss: 0.6903806924819946\n",
      "Epoch 846/1000, Train Loss: 0.6902040243148804, Test Loss: 0.6903797388076782\n",
      "Epoch 847/1000, Train Loss: 0.6902029514312744, Test Loss: 0.6903787851333618\n",
      "Epoch 848/1000, Train Loss: 0.690201997756958, Test Loss: 0.6903778910636902\n",
      "Epoch 849/1000, Train Loss: 0.6902009844779968, Test Loss: 0.6903769373893738\n",
      "Epoch 850/1000, Train Loss: 0.6901999711990356, Test Loss: 0.6903758645057678\n",
      "Epoch 851/1000, Train Loss: 0.6901988983154297, Test Loss: 0.690375030040741\n",
      "Epoch 852/1000, Train Loss: 0.6901979446411133, Test Loss: 0.690373957157135\n",
      "Epoch 853/1000, Train Loss: 0.6901969313621521, Test Loss: 0.6903730034828186\n",
      "Epoch 854/1000, Train Loss: 0.6901959180831909, Test Loss: 0.690372109413147\n",
      "Epoch 855/1000, Train Loss: 0.6901949048042297, Test Loss: 0.6903710961341858\n",
      "Epoch 856/1000, Train Loss: 0.6901939511299133, Test Loss: 0.6903701424598694\n",
      "Epoch 857/1000, Train Loss: 0.6901928782463074, Test Loss: 0.690369188785553\n",
      "Epoch 858/1000, Train Loss: 0.6901918649673462, Test Loss: 0.6903682351112366\n",
      "Epoch 859/1000, Train Loss: 0.690190851688385, Test Loss: 0.6903673410415649\n",
      "Epoch 860/1000, Train Loss: 0.6901898980140686, Test Loss: 0.690366268157959\n",
      "Epoch 861/1000, Train Loss: 0.6901889443397522, Test Loss: 0.6903653740882874\n",
      "Epoch 862/1000, Train Loss: 0.690187931060791, Test Loss: 0.6903643608093262\n",
      "Epoch 863/1000, Train Loss: 0.6901867985725403, Test Loss: 0.6903634667396545\n",
      "Epoch 864/1000, Train Loss: 0.6901859045028687, Test Loss: 0.6903625130653381\n",
      "Epoch 865/1000, Train Loss: 0.6901848912239075, Test Loss: 0.6903615593910217\n",
      "Epoch 866/1000, Train Loss: 0.6901838183403015, Test Loss: 0.6903606653213501\n",
      "Epoch 867/1000, Train Loss: 0.6901828646659851, Test Loss: 0.6903595924377441\n",
      "Epoch 868/1000, Train Loss: 0.6901818513870239, Test Loss: 0.6903586387634277\n",
      "Epoch 869/1000, Train Loss: 0.6901808381080627, Test Loss: 0.6903577446937561\n",
      "Epoch 870/1000, Train Loss: 0.6901798248291016, Test Loss: 0.6903566718101501\n",
      "Epoch 871/1000, Train Loss: 0.6901788115501404, Test Loss: 0.6903557777404785\n",
      "Epoch 872/1000, Train Loss: 0.6901779174804688, Test Loss: 0.6903547644615173\n",
      "Epoch 873/1000, Train Loss: 0.690176784992218, Test Loss: 0.6903537511825562\n",
      "Epoch 874/1000, Train Loss: 0.6901757717132568, Test Loss: 0.6903529763221741\n",
      "Epoch 875/1000, Train Loss: 0.6901747584342957, Test Loss: 0.6903519630432129\n",
      "Epoch 876/1000, Train Loss: 0.6901738047599792, Test Loss: 0.6903509497642517\n",
      "Epoch 877/1000, Train Loss: 0.6901727914810181, Test Loss: 0.6903499960899353\n",
      "Epoch 878/1000, Train Loss: 0.6901718378067017, Test Loss: 0.6903490424156189\n",
      "Epoch 879/1000, Train Loss: 0.6901707053184509, Test Loss: 0.6903480887413025\n",
      "Epoch 880/1000, Train Loss: 0.6901696920394897, Test Loss: 0.6903470754623413\n",
      "Epoch 881/1000, Train Loss: 0.6901687979698181, Test Loss: 0.6903461217880249\n",
      "Epoch 882/1000, Train Loss: 0.6901677250862122, Test Loss: 0.6903451681137085\n",
      "Epoch 883/1000, Train Loss: 0.6901667714118958, Test Loss: 0.6903442144393921\n",
      "Epoch 884/1000, Train Loss: 0.6901657581329346, Test Loss: 0.6903432607650757\n",
      "Epoch 885/1000, Train Loss: 0.6901646256446838, Test Loss: 0.6903422474861145\n",
      "Epoch 886/1000, Train Loss: 0.6901636719703674, Test Loss: 0.6903413534164429\n",
      "Epoch 887/1000, Train Loss: 0.690162718296051, Test Loss: 0.6903403401374817\n",
      "Epoch 888/1000, Train Loss: 0.6901617646217346, Test Loss: 0.6903393864631653\n",
      "Epoch 889/1000, Train Loss: 0.6901606917381287, Test Loss: 0.6903385519981384\n",
      "Epoch 890/1000, Train Loss: 0.6901597380638123, Test Loss: 0.6903374791145325\n",
      "Epoch 891/1000, Train Loss: 0.6901587247848511, Test Loss: 0.6903365254402161\n",
      "Epoch 892/1000, Train Loss: 0.6901576519012451, Test Loss: 0.6903355121612549\n",
      "Epoch 893/1000, Train Loss: 0.6901566982269287, Test Loss: 0.6903344988822937\n",
      "Epoch 894/1000, Train Loss: 0.6901557445526123, Test Loss: 0.6903336644172668\n",
      "Epoch 895/1000, Train Loss: 0.6901546120643616, Test Loss: 0.6903326511383057\n",
      "Epoch 896/1000, Train Loss: 0.6901537179946899, Test Loss: 0.6903316974639893\n",
      "Epoch 897/1000, Train Loss: 0.6901527047157288, Test Loss: 0.6903306245803833\n",
      "Epoch 898/1000, Train Loss: 0.6901516318321228, Test Loss: 0.6903297305107117\n",
      "Epoch 899/1000, Train Loss: 0.6901506185531616, Test Loss: 0.6903287172317505\n",
      "Epoch 900/1000, Train Loss: 0.6901496648788452, Test Loss: 0.6903278827667236\n",
      "Epoch 901/1000, Train Loss: 0.690148651599884, Test Loss: 0.6903269290924072\n",
      "Epoch 902/1000, Train Loss: 0.6901476979255676, Test Loss: 0.6903259754180908\n",
      "Epoch 903/1000, Train Loss: 0.6901466250419617, Test Loss: 0.6903249025344849\n",
      "Epoch 904/1000, Train Loss: 0.69014573097229, Test Loss: 0.6903240084648132\n",
      "Epoch 905/1000, Train Loss: 0.6901445984840393, Test Loss: 0.6903230547904968\n",
      "Epoch 906/1000, Train Loss: 0.6901436448097229, Test Loss: 0.6903219819068909\n",
      "Epoch 907/1000, Train Loss: 0.6901426315307617, Test Loss: 0.6903210282325745\n",
      "Epoch 908/1000, Train Loss: 0.6901416778564453, Test Loss: 0.6903200745582581\n",
      "Epoch 909/1000, Train Loss: 0.6901406645774841, Test Loss: 0.6903191208839417\n",
      "Epoch 910/1000, Train Loss: 0.6901395916938782, Test Loss: 0.6903181672096252\n",
      "Epoch 911/1000, Train Loss: 0.690138578414917, Test Loss: 0.6903170943260193\n",
      "Epoch 912/1000, Train Loss: 0.6901376247406006, Test Loss: 0.6903162598609924\n",
      "Epoch 913/1000, Train Loss: 0.6901366114616394, Test Loss: 0.6903151869773865\n",
      "Epoch 914/1000, Train Loss: 0.690135657787323, Test Loss: 0.6903142929077148\n",
      "Epoch 915/1000, Train Loss: 0.6901345252990723, Test Loss: 0.6903132200241089\n",
      "Epoch 916/1000, Train Loss: 0.6901335120201111, Test Loss: 0.6903123259544373\n",
      "Epoch 917/1000, Train Loss: 0.6901325583457947, Test Loss: 0.6903113722801208\n",
      "Epoch 918/1000, Train Loss: 0.6901314854621887, Test Loss: 0.6903104186058044\n",
      "Epoch 919/1000, Train Loss: 0.6901305317878723, Test Loss: 0.690309464931488\n",
      "Epoch 920/1000, Train Loss: 0.6901295185089111, Test Loss: 0.6903084516525269\n",
      "Epoch 921/1000, Train Loss: 0.6901285648345947, Test Loss: 0.6903074383735657\n",
      "Epoch 922/1000, Train Loss: 0.6901274919509888, Test Loss: 0.6903064846992493\n",
      "Epoch 923/1000, Train Loss: 0.6901265382766724, Test Loss: 0.6903055906295776\n",
      "Epoch 924/1000, Train Loss: 0.690125584602356, Test Loss: 0.6903046369552612\n",
      "Epoch 925/1000, Train Loss: 0.69012451171875, Test Loss: 0.6903036832809448\n",
      "Epoch 926/1000, Train Loss: 0.6901234984397888, Test Loss: 0.6903026700019836\n",
      "Epoch 927/1000, Train Loss: 0.6901224851608276, Test Loss: 0.6903016567230225\n",
      "Epoch 928/1000, Train Loss: 0.6901215314865112, Test Loss: 0.6903006434440613\n",
      "Epoch 929/1000, Train Loss: 0.69012051820755, Test Loss: 0.6902996897697449\n",
      "Epoch 930/1000, Train Loss: 0.6901195049285889, Test Loss: 0.6902986764907837\n",
      "Epoch 931/1000, Train Loss: 0.6901184320449829, Test Loss: 0.6902976632118225\n",
      "Epoch 932/1000, Train Loss: 0.6901174783706665, Test Loss: 0.6902967691421509\n",
      "Epoch 933/1000, Train Loss: 0.6901165246963501, Test Loss: 0.6902958154678345\n",
      "Epoch 934/1000, Train Loss: 0.6901154518127441, Test Loss: 0.6902948617935181\n",
      "Epoch 935/1000, Train Loss: 0.6901144981384277, Test Loss: 0.6902939677238464\n",
      "Epoch 936/1000, Train Loss: 0.6901134848594666, Test Loss: 0.6902929544448853\n",
      "Epoch 937/1000, Train Loss: 0.6901124715805054, Test Loss: 0.6902919411659241\n",
      "Epoch 938/1000, Train Loss: 0.6901113986968994, Test Loss: 0.6902910470962524\n",
      "Epoch 939/1000, Train Loss: 0.690110445022583, Test Loss: 0.6902900338172913\n",
      "Epoch 940/1000, Train Loss: 0.6901094317436218, Test Loss: 0.6902890205383301\n",
      "Epoch 941/1000, Train Loss: 0.6901084184646606, Test Loss: 0.6902880072593689\n",
      "Epoch 942/1000, Train Loss: 0.6901074051856995, Test Loss: 0.6902871131896973\n",
      "Epoch 943/1000, Train Loss: 0.6901063919067383, Test Loss: 0.6902860999107361\n",
      "Epoch 944/1000, Train Loss: 0.6901053786277771, Test Loss: 0.6902851462364197\n",
      "Epoch 945/1000, Train Loss: 0.6901043653488159, Test Loss: 0.6902841925621033\n",
      "Epoch 946/1000, Train Loss: 0.69010329246521, Test Loss: 0.6902831196784973\n",
      "Epoch 947/1000, Train Loss: 0.6901023983955383, Test Loss: 0.6902822256088257\n",
      "Epoch 948/1000, Train Loss: 0.6901013851165771, Test Loss: 0.6902812123298645\n",
      "Epoch 949/1000, Train Loss: 0.690100371837616, Test Loss: 0.6902801990509033\n",
      "Epoch 950/1000, Train Loss: 0.69009929895401, Test Loss: 0.6902792453765869\n",
      "Epoch 951/1000, Train Loss: 0.6900983452796936, Test Loss: 0.6902782917022705\n",
      "Epoch 952/1000, Train Loss: 0.6900973320007324, Test Loss: 0.6902772784233093\n",
      "Epoch 953/1000, Train Loss: 0.6900963187217712, Test Loss: 0.6902763247489929\n",
      "Epoch 954/1000, Train Loss: 0.6900953650474548, Test Loss: 0.6902753710746765\n",
      "Epoch 955/1000, Train Loss: 0.6900942921638489, Test Loss: 0.6902743577957153\n",
      "Epoch 956/1000, Train Loss: 0.6900932788848877, Test Loss: 0.6902734637260437\n",
      "Epoch 957/1000, Train Loss: 0.6900922656059265, Test Loss: 0.6902724504470825\n",
      "Epoch 958/1000, Train Loss: 0.6900911927223206, Test Loss: 0.6902714371681213\n",
      "Epoch 959/1000, Train Loss: 0.6900902390480042, Test Loss: 0.6902704238891602\n",
      "Epoch 960/1000, Train Loss: 0.6900891661643982, Test Loss: 0.6902694702148438\n",
      "Epoch 961/1000, Train Loss: 0.6900882124900818, Test Loss: 0.6902685165405273\n",
      "Epoch 962/1000, Train Loss: 0.6900872588157654, Test Loss: 0.6902675628662109\n",
      "Epoch 963/1000, Train Loss: 0.6900862455368042, Test Loss: 0.6902665495872498\n",
      "Epoch 964/1000, Train Loss: 0.6900851130485535, Test Loss: 0.6902655959129333\n",
      "Epoch 965/1000, Train Loss: 0.6900840997695923, Test Loss: 0.6902645826339722\n",
      "Epoch 966/1000, Train Loss: 0.6900831460952759, Test Loss: 0.6902636289596558\n",
      "Epoch 967/1000, Train Loss: 0.6900821924209595, Test Loss: 0.6902625560760498\n",
      "Epoch 968/1000, Train Loss: 0.6900812387466431, Test Loss: 0.6902616620063782\n",
      "Epoch 969/1000, Train Loss: 0.6900801658630371, Test Loss: 0.690260648727417\n",
      "Epoch 970/1000, Train Loss: 0.6900791525840759, Test Loss: 0.6902597546577454\n",
      "Epoch 971/1000, Train Loss: 0.6900781989097595, Test Loss: 0.690258800983429\n",
      "Epoch 972/1000, Train Loss: 0.6900771856307983, Test Loss: 0.690257728099823\n",
      "Epoch 973/1000, Train Loss: 0.6900761127471924, Test Loss: 0.6902567744255066\n",
      "Epoch 974/1000, Train Loss: 0.6900750398635864, Test Loss: 0.6902558207511902\n",
      "Epoch 975/1000, Train Loss: 0.69007408618927, Test Loss: 0.690254807472229\n",
      "Epoch 976/1000, Train Loss: 0.6900730729103088, Test Loss: 0.6902537941932678\n",
      "Epoch 977/1000, Train Loss: 0.6900720596313477, Test Loss: 0.6902528405189514\n",
      "Epoch 978/1000, Train Loss: 0.6900711059570312, Test Loss: 0.690251886844635\n",
      "Epoch 979/1000, Train Loss: 0.6900700330734253, Test Loss: 0.6902509927749634\n",
      "Epoch 980/1000, Train Loss: 0.6900690793991089, Test Loss: 0.6902499198913574\n",
      "Epoch 981/1000, Train Loss: 0.6900680065155029, Test Loss: 0.690248966217041\n",
      "Epoch 982/1000, Train Loss: 0.6900671124458313, Test Loss: 0.6902479529380798\n",
      "Epoch 983/1000, Train Loss: 0.6900660991668701, Test Loss: 0.6902469992637634\n",
      "Epoch 984/1000, Train Loss: 0.6900649666786194, Test Loss: 0.690246045589447\n",
      "Epoch 985/1000, Train Loss: 0.6900639533996582, Test Loss: 0.6902449727058411\n",
      "Epoch 986/1000, Train Loss: 0.6900630593299866, Test Loss: 0.6902440190315247\n",
      "Epoch 987/1000, Train Loss: 0.6900620460510254, Test Loss: 0.6902430653572083\n",
      "Epoch 988/1000, Train Loss: 0.6900609135627747, Test Loss: 0.6902420520782471\n",
      "Epoch 989/1000, Train Loss: 0.6900599598884583, Test Loss: 0.6902410984039307\n",
      "Epoch 990/1000, Train Loss: 0.6900590062141418, Test Loss: 0.6902401447296143\n",
      "Epoch 991/1000, Train Loss: 0.6900579333305359, Test Loss: 0.6902390718460083\n",
      "Epoch 992/1000, Train Loss: 0.6900569796562195, Test Loss: 0.6902380585670471\n",
      "Epoch 993/1000, Train Loss: 0.6900558471679688, Test Loss: 0.6902371048927307\n",
      "Epoch 994/1000, Train Loss: 0.6900548934936523, Test Loss: 0.6902361512184143\n",
      "Epoch 995/1000, Train Loss: 0.6900539398193359, Test Loss: 0.6902351975440979\n",
      "Epoch 996/1000, Train Loss: 0.69005286693573, Test Loss: 0.6902341842651367\n",
      "Epoch 997/1000, Train Loss: 0.6900518536567688, Test Loss: 0.6902332305908203\n",
      "Epoch 998/1000, Train Loss: 0.6900508999824524, Test Loss: 0.6902322769165039\n",
      "Epoch 999/1000, Train Loss: 0.6900497674942017, Test Loss: 0.6902311444282532\n",
      "Epoch 1000/1000, Train Loss: 0.6900488138198853, Test Loss: 0.6902302503585815\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:0.691115671530004\n",
      "Accuracy:0.5313522727272727\n",
      "Confusion Matrix:\n",
      "[[ 1243 81631]\n",
      " [  851 92275]]\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "hidden_size = 30\n",
    "model = mlp.MLP_mach1(28, hidden_size)\n",
    "# Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = .01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "n_epochs = 1000\n",
    "# Train the model using our function\n",
    "train_losses, test_losses = mlp.train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n",
    "# Make predictions on the validation set\n",
    "f1, acc, cm = mlp.getResults(train_losses, test_losses, model, X_val, y_val)\n",
    "# Print the results\n",
    "print(\"F1:\" + str(f1))\n",
    "print(\"Accuracy:\" + str(acc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.691444456577301, Test Loss: 0.691550076007843\n",
      "Epoch 2/1000, Train Loss: 0.6911982297897339, Test Loss: 0.6913080215454102\n",
      "Epoch 3/1000, Train Loss: 0.6909741759300232, Test Loss: 0.6910825967788696\n",
      "Epoch 4/1000, Train Loss: 0.6907639503479004, Test Loss: 0.6908650994300842\n",
      "Epoch 5/1000, Train Loss: 0.6905580163002014, Test Loss: 0.6906484365463257\n",
      "Epoch 6/1000, Train Loss: 0.690350353717804, Test Loss: 0.6904289722442627\n",
      "Epoch 7/1000, Train Loss: 0.6901375651359558, Test Loss: 0.6902058720588684\n",
      "Epoch 8/1000, Train Loss: 0.6899176239967346, Test Loss: 0.68997722864151\n",
      "Epoch 9/1000, Train Loss: 0.6896906495094299, Test Loss: 0.6897420287132263\n",
      "Epoch 10/1000, Train Loss: 0.6894565224647522, Test Loss: 0.6894993185997009\n",
      "Epoch 11/1000, Train Loss: 0.689214825630188, Test Loss: 0.6892493963241577\n",
      "Epoch 12/1000, Train Loss: 0.688965380191803, Test Loss: 0.6889917254447937\n",
      "Epoch 13/1000, Train Loss: 0.688707709312439, Test Loss: 0.6887239813804626\n",
      "Epoch 14/1000, Train Loss: 0.6884406805038452, Test Loss: 0.688445508480072\n",
      "Epoch 15/1000, Train Loss: 0.6881632804870605, Test Loss: 0.6881558895111084\n",
      "Epoch 16/1000, Train Loss: 0.687874972820282, Test Loss: 0.6878537535667419\n",
      "Epoch 17/1000, Train Loss: 0.6875750422477722, Test Loss: 0.687537670135498\n",
      "Epoch 18/1000, Train Loss: 0.687262773513794, Test Loss: 0.6872080564498901\n",
      "Epoch 19/1000, Train Loss: 0.6869366765022278, Test Loss: 0.6868644952774048\n",
      "Epoch 20/1000, Train Loss: 0.6865966320037842, Test Loss: 0.6865054965019226\n",
      "Epoch 21/1000, Train Loss: 0.6862420439720154, Test Loss: 0.6861329078674316\n",
      "Epoch 22/1000, Train Loss: 0.685872495174408, Test Loss: 0.6857463717460632\n",
      "Epoch 23/1000, Train Loss: 0.6854883432388306, Test Loss: 0.6853465437889099\n",
      "Epoch 24/1000, Train Loss: 0.6850895285606384, Test Loss: 0.6849339604377747\n",
      "Epoch 25/1000, Train Loss: 0.6846750974655151, Test Loss: 0.6845072507858276\n",
      "Epoch 26/1000, Train Loss: 0.6842451691627502, Test Loss: 0.6840663552284241\n",
      "Epoch 27/1000, Train Loss: 0.6837994456291199, Test Loss: 0.6836103200912476\n",
      "Epoch 28/1000, Train Loss: 0.6833376884460449, Test Loss: 0.6831399202346802\n",
      "Epoch 29/1000, Train Loss: 0.6828592419624329, Test Loss: 0.6826536059379578\n",
      "Epoch 30/1000, Train Loss: 0.6823632121086121, Test Loss: 0.6821515560150146\n",
      "Epoch 31/1000, Train Loss: 0.6818490028381348, Test Loss: 0.6816326975822449\n",
      "Epoch 32/1000, Train Loss: 0.6813174486160278, Test Loss: 0.6810963749885559\n",
      "Epoch 33/1000, Train Loss: 0.6807689666748047, Test Loss: 0.680541455745697\n",
      "Epoch 34/1000, Train Loss: 0.6802025437355042, Test Loss: 0.6799676418304443\n",
      "Epoch 35/1000, Train Loss: 0.6796181797981262, Test Loss: 0.6793765425682068\n",
      "Epoch 36/1000, Train Loss: 0.6790151000022888, Test Loss: 0.6787678003311157\n",
      "Epoch 37/1000, Train Loss: 0.678394079208374, Test Loss: 0.6781408190727234\n",
      "Epoch 38/1000, Train Loss: 0.6777545809745789, Test Loss: 0.6774976253509521\n",
      "Epoch 39/1000, Train Loss: 0.6770972609519958, Test Loss: 0.6768390536308289\n",
      "Epoch 40/1000, Train Loss: 0.6764225363731384, Test Loss: 0.6761655211448669\n",
      "Epoch 41/1000, Train Loss: 0.6757319569587708, Test Loss: 0.6754777431488037\n",
      "Epoch 42/1000, Train Loss: 0.6750259399414062, Test Loss: 0.6747748851776123\n",
      "Epoch 43/1000, Train Loss: 0.6743045449256897, Test Loss: 0.6740577816963196\n",
      "Epoch 44/1000, Train Loss: 0.6735696792602539, Test Loss: 0.6733275651931763\n",
      "Epoch 45/1000, Train Loss: 0.6728225350379944, Test Loss: 0.6725864410400391\n",
      "Epoch 46/1000, Train Loss: 0.6720649600028992, Test Loss: 0.6718361377716064\n",
      "Epoch 47/1000, Train Loss: 0.6712973117828369, Test Loss: 0.6710765957832336\n",
      "Epoch 48/1000, Train Loss: 0.6705217957496643, Test Loss: 0.6703089475631714\n",
      "Epoch 49/1000, Train Loss: 0.6697394847869873, Test Loss: 0.6695371866226196\n",
      "Epoch 50/1000, Train Loss: 0.6689521074295044, Test Loss: 0.6687611937522888\n",
      "Epoch 51/1000, Train Loss: 0.6681610941886902, Test Loss: 0.6679831147193909\n",
      "Epoch 52/1000, Train Loss: 0.6673682332038879, Test Loss: 0.6672021746635437\n",
      "Epoch 53/1000, Train Loss: 0.6665741801261902, Test Loss: 0.6664184927940369\n",
      "Epoch 54/1000, Train Loss: 0.6657780408859253, Test Loss: 0.6656317114830017\n",
      "Epoch 55/1000, Train Loss: 0.6649806499481201, Test Loss: 0.6648419499397278\n",
      "Epoch 56/1000, Train Loss: 0.664181649684906, Test Loss: 0.664052426815033\n",
      "Epoch 57/1000, Train Loss: 0.663382351398468, Test Loss: 0.6632621884346008\n",
      "Epoch 58/1000, Train Loss: 0.6625824570655823, Test Loss: 0.6624734997749329\n",
      "Epoch 59/1000, Train Loss: 0.6617841124534607, Test Loss: 0.6616855263710022\n",
      "Epoch 60/1000, Train Loss: 0.6609882712364197, Test Loss: 0.6608970761299133\n",
      "Epoch 61/1000, Train Loss: 0.6601937413215637, Test Loss: 0.6601098775863647\n",
      "Epoch 62/1000, Train Loss: 0.6594012975692749, Test Loss: 0.6593260169029236\n",
      "Epoch 63/1000, Train Loss: 0.6586135625839233, Test Loss: 0.6585447788238525\n",
      "Epoch 64/1000, Train Loss: 0.6578314304351807, Test Loss: 0.6577692031860352\n",
      "Epoch 65/1000, Train Loss: 0.6570558547973633, Test Loss: 0.6570014357566833\n",
      "Epoch 66/1000, Train Loss: 0.6562881469726562, Test Loss: 0.6562400460243225\n",
      "Epoch 67/1000, Train Loss: 0.6555289626121521, Test Loss: 0.6554882526397705\n",
      "Epoch 68/1000, Train Loss: 0.6547785401344299, Test Loss: 0.6547468304634094\n",
      "Epoch 69/1000, Train Loss: 0.6540378928184509, Test Loss: 0.6540180444717407\n",
      "Epoch 70/1000, Train Loss: 0.6533087491989136, Test Loss: 0.6533017754554749\n",
      "Epoch 71/1000, Train Loss: 0.6525920629501343, Test Loss: 0.6526007652282715\n",
      "Epoch 72/1000, Train Loss: 0.6518892645835876, Test Loss: 0.6519151329994202\n",
      "Epoch 73/1000, Train Loss: 0.6512014865875244, Test Loss: 0.6512459516525269\n",
      "Epoch 74/1000, Train Loss: 0.6505305171012878, Test Loss: 0.6505957841873169\n",
      "Epoch 75/1000, Train Loss: 0.6498759984970093, Test Loss: 0.6499649882316589\n",
      "Epoch 76/1000, Train Loss: 0.6492396593093872, Test Loss: 0.6493529081344604\n",
      "Epoch 77/1000, Train Loss: 0.6486217975616455, Test Loss: 0.6487618684768677\n",
      "Epoch 78/1000, Train Loss: 0.6480242013931274, Test Loss: 0.6481913328170776\n",
      "Epoch 79/1000, Train Loss: 0.6474465727806091, Test Loss: 0.6476404070854187\n",
      "Epoch 80/1000, Train Loss: 0.6468880772590637, Test Loss: 0.6471095085144043\n",
      "Epoch 81/1000, Train Loss: 0.646348237991333, Test Loss: 0.646597683429718\n",
      "Epoch 82/1000, Train Loss: 0.6458264589309692, Test Loss: 0.6461039185523987\n",
      "Epoch 83/1000, Train Loss: 0.6453220248222351, Test Loss: 0.6456282734870911\n",
      "Epoch 84/1000, Train Loss: 0.6448355913162231, Test Loss: 0.6451699137687683\n",
      "Epoch 85/1000, Train Loss: 0.6443663835525513, Test Loss: 0.6447272300720215\n",
      "Epoch 86/1000, Train Loss: 0.6439124941825867, Test Loss: 0.6442984938621521\n",
      "Epoch 87/1000, Train Loss: 0.6434739232063293, Test Loss: 0.6438837051391602\n",
      "Epoch 88/1000, Train Loss: 0.643050491809845, Test Loss: 0.64348304271698\n",
      "Epoch 89/1000, Train Loss: 0.642641007900238, Test Loss: 0.6430956125259399\n",
      "Epoch 90/1000, Train Loss: 0.6422452330589294, Test Loss: 0.6427198648452759\n",
      "Epoch 91/1000, Train Loss: 0.641861617565155, Test Loss: 0.6423542499542236\n",
      "Epoch 92/1000, Train Loss: 0.6414885520935059, Test Loss: 0.6419996619224548\n",
      "Epoch 93/1000, Train Loss: 0.6411257982254028, Test Loss: 0.6416531801223755\n",
      "Epoch 94/1000, Train Loss: 0.6407734155654907, Test Loss: 0.641315758228302\n",
      "Epoch 95/1000, Train Loss: 0.6404301524162292, Test Loss: 0.6409856081008911\n",
      "Epoch 96/1000, Train Loss: 0.6400956511497498, Test Loss: 0.6406619548797607\n",
      "Epoch 97/1000, Train Loss: 0.6397701501846313, Test Loss: 0.6403464674949646\n",
      "Epoch 98/1000, Train Loss: 0.6394535303115845, Test Loss: 0.640039324760437\n",
      "Epoch 99/1000, Train Loss: 0.6391451358795166, Test Loss: 0.6397383809089661\n",
      "Epoch 100/1000, Train Loss: 0.6388440132141113, Test Loss: 0.6394436359405518\n",
      "Epoch 101/1000, Train Loss: 0.6385500431060791, Test Loss: 0.6391569972038269\n",
      "Epoch 102/1000, Train Loss: 0.6382619738578796, Test Loss: 0.638875424861908\n",
      "Epoch 103/1000, Train Loss: 0.6379793286323547, Test Loss: 0.6385985612869263\n",
      "Epoch 104/1000, Train Loss: 0.6377015709877014, Test Loss: 0.6383245587348938\n",
      "Epoch 105/1000, Train Loss: 0.6374291777610779, Test Loss: 0.6380547881126404\n",
      "Epoch 106/1000, Train Loss: 0.6371610164642334, Test Loss: 0.637789249420166\n",
      "Epoch 107/1000, Train Loss: 0.6368969678878784, Test Loss: 0.6375271081924438\n",
      "Epoch 108/1000, Train Loss: 0.6366364359855652, Test Loss: 0.6372684836387634\n",
      "Epoch 109/1000, Train Loss: 0.6363784074783325, Test Loss: 0.6370134949684143\n",
      "Epoch 110/1000, Train Loss: 0.6361232399940491, Test Loss: 0.6367613673210144\n",
      "Epoch 111/1000, Train Loss: 0.6358714699745178, Test Loss: 0.6365124583244324\n",
      "Epoch 112/1000, Train Loss: 0.6356229186058044, Test Loss: 0.6362659931182861\n",
      "Epoch 113/1000, Train Loss: 0.6353766322135925, Test Loss: 0.6360230445861816\n",
      "Epoch 114/1000, Train Loss: 0.635132372379303, Test Loss: 0.6357836127281189\n",
      "Epoch 115/1000, Train Loss: 0.6348903179168701, Test Loss: 0.6355477571487427\n",
      "Epoch 116/1000, Train Loss: 0.6346502900123596, Test Loss: 0.6353139877319336\n",
      "Epoch 117/1000, Train Loss: 0.6344119310379028, Test Loss: 0.6350817084312439\n",
      "Epoch 118/1000, Train Loss: 0.6341747641563416, Test Loss: 0.6348512768745422\n",
      "Epoch 119/1000, Train Loss: 0.6339393258094788, Test Loss: 0.6346200704574585\n",
      "Epoch 120/1000, Train Loss: 0.6337048411369324, Test Loss: 0.6343891024589539\n",
      "Epoch 121/1000, Train Loss: 0.6334718465805054, Test Loss: 0.6341604590415955\n",
      "Epoch 122/1000, Train Loss: 0.6332400441169739, Test Loss: 0.6339333057403564\n",
      "Epoch 123/1000, Train Loss: 0.6330090761184692, Test Loss: 0.6337057948112488\n",
      "Epoch 124/1000, Train Loss: 0.6327787041664124, Test Loss: 0.6334773898124695\n",
      "Epoch 125/1000, Train Loss: 0.6325488090515137, Test Loss: 0.6332491636276245\n",
      "Epoch 126/1000, Train Loss: 0.6323199272155762, Test Loss: 0.6330208778381348\n",
      "Epoch 127/1000, Train Loss: 0.632091760635376, Test Loss: 0.6327928900718689\n",
      "Epoch 128/1000, Train Loss: 0.6318640112876892, Test Loss: 0.6325647234916687\n",
      "Epoch 129/1000, Train Loss: 0.6316366195678711, Test Loss: 0.6323384642601013\n",
      "Epoch 130/1000, Train Loss: 0.6314102411270142, Test Loss: 0.6321092247962952\n",
      "Epoch 131/1000, Train Loss: 0.6311848759651184, Test Loss: 0.6318884491920471\n",
      "Epoch 132/1000, Train Loss: 0.6309613585472107, Test Loss: 0.6316639184951782\n",
      "Epoch 133/1000, Train Loss: 0.6307426691055298, Test Loss: 0.6314700841903687\n",
      "Epoch 134/1000, Train Loss: 0.6305361986160278, Test Loss: 0.6312723159790039\n",
      "Epoch 135/1000, Train Loss: 0.6303561329841614, Test Loss: 0.6311150789260864\n",
      "Epoch 136/1000, Train Loss: 0.6301735639572144, Test Loss: 0.6308502554893494\n",
      "Epoch 137/1000, Train Loss: 0.6299272179603577, Test Loss: 0.6306353211402893\n",
      "Epoch 138/1000, Train Loss: 0.6297023892402649, Test Loss: 0.630496621131897\n",
      "Epoch 139/1000, Train Loss: 0.6295499205589294, Test Loss: 0.630285918712616\n",
      "Epoch 140/1000, Train Loss: 0.6293537616729736, Test Loss: 0.6300746202468872\n",
      "Epoch 141/1000, Train Loss: 0.6291322112083435, Test Loss: 0.6299213171005249\n",
      "Epoch 142/1000, Train Loss: 0.6289722919464111, Test Loss: 0.6297298073768616\n",
      "Epoch 143/1000, Train Loss: 0.628791868686676, Test Loss: 0.6295322775840759\n",
      "Epoch 144/1000, Train Loss: 0.6285846829414368, Test Loss: 0.6293778419494629\n",
      "Epoch 145/1000, Train Loss: 0.628422737121582, Test Loss: 0.6291999220848083\n",
      "Epoch 146/1000, Train Loss: 0.6282488107681274, Test Loss: 0.6290144920349121\n",
      "Epoch 147/1000, Train Loss: 0.6280539631843567, Test Loss: 0.62885981798172\n",
      "Epoch 148/1000, Train Loss: 0.6278924345970154, Test Loss: 0.6286888718605042\n",
      "Epoch 149/1000, Train Loss: 0.6277238130569458, Test Loss: 0.6285118460655212\n",
      "Epoch 150/1000, Train Loss: 0.6275393962860107, Test Loss: 0.628357470035553\n",
      "Epoch 151/1000, Train Loss: 0.6273794174194336, Test Loss: 0.6281943917274475\n",
      "Epoch 152/1000, Train Loss: 0.6272155046463013, Test Loss: 0.6280260682106018\n",
      "Epoch 153/1000, Train Loss: 0.6270387768745422, Test Loss: 0.6278727054595947\n",
      "Epoch 154/1000, Train Loss: 0.626879096031189, Test Loss: 0.6277164816856384\n",
      "Epoch 155/1000, Train Loss: 0.6267196536064148, Test Loss: 0.6275545358657837\n",
      "Epoch 156/1000, Train Loss: 0.6265494227409363, Test Loss: 0.6274020671844482\n",
      "Epoch 157/1000, Train Loss: 0.6263901591300964, Test Loss: 0.6272513270378113\n",
      "Epoch 158/1000, Train Loss: 0.6262351274490356, Test Loss: 0.6270942091941833\n",
      "Epoch 159/1000, Train Loss: 0.626071572303772, Test Loss: 0.6269394159317017\n",
      "Epoch 160/1000, Train Loss: 0.6259135007858276, Test Loss: 0.6267902255058289\n",
      "Epoch 161/1000, Train Loss: 0.6257625222206116, Test Loss: 0.6266365647315979\n",
      "Epoch 162/1000, Train Loss: 0.6256067156791687, Test Loss: 0.626479983329773\n",
      "Epoch 163/1000, Train Loss: 0.6254497766494751, Test Loss: 0.6263310313224792\n",
      "Epoch 164/1000, Train Loss: 0.6252995729446411, Test Loss: 0.6261829733848572\n",
      "Epoch 165/1000, Train Loss: 0.6251488924026489, Test Loss: 0.6260291337966919\n",
      "Epoch 166/1000, Train Loss: 0.6249939799308777, Test Loss: 0.6258785128593445\n",
      "Epoch 167/1000, Train Loss: 0.6248419880867004, Test Loss: 0.6257316470146179\n",
      "Epoch 168/1000, Train Loss: 0.6246936321258545, Test Loss: 0.6255806088447571\n",
      "Epoch 169/1000, Train Loss: 0.6245434880256653, Test Loss: 0.6254293322563171\n",
      "Epoch 170/1000, Train Loss: 0.6243923306465149, Test Loss: 0.6252793073654175\n",
      "Epoch 171/1000, Train Loss: 0.6242440342903137, Test Loss: 0.6251312494277954\n",
      "Epoch 172/1000, Train Loss: 0.624098539352417, Test Loss: 0.6249824166297913\n",
      "Epoch 173/1000, Train Loss: 0.6239520311355591, Test Loss: 0.6248319149017334\n",
      "Epoch 174/1000, Train Loss: 0.6238041520118713, Test Loss: 0.624682605266571\n",
      "Epoch 175/1000, Train Loss: 0.6236574053764343, Test Loss: 0.6245356202125549\n",
      "Epoch 176/1000, Train Loss: 0.6235124468803406, Test Loss: 0.6243887543678284\n",
      "Epoch 177/1000, Train Loss: 0.6233675479888916, Test Loss: 0.6242413520812988\n",
      "Epoch 178/1000, Train Loss: 0.6232221722602844, Test Loss: 0.6240929961204529\n",
      "Epoch 179/1000, Train Loss: 0.6230769753456116, Test Loss: 0.6239467859268188\n",
      "Epoch 180/1000, Train Loss: 0.6229332089424133, Test Loss: 0.6238026022911072\n",
      "Epoch 181/1000, Train Loss: 0.6227905750274658, Test Loss: 0.6236585378646851\n",
      "Epoch 182/1000, Train Loss: 0.6226484179496765, Test Loss: 0.6235142350196838\n",
      "Epoch 183/1000, Train Loss: 0.6225064396858215, Test Loss: 0.6233697533607483\n",
      "Epoch 184/1000, Train Loss: 0.62236487865448, Test Loss: 0.6232259273529053\n",
      "Epoch 185/1000, Train Loss: 0.6222235560417175, Test Loss: 0.6230821013450623\n",
      "Epoch 186/1000, Train Loss: 0.6220827698707581, Test Loss: 0.6229397058486938\n",
      "Epoch 187/1000, Train Loss: 0.6219424605369568, Test Loss: 0.6227973699569702\n",
      "Epoch 188/1000, Train Loss: 0.6218022108078003, Test Loss: 0.6226559281349182\n",
      "Epoch 189/1000, Train Loss: 0.6216621994972229, Test Loss: 0.6225144267082214\n",
      "Epoch 190/1000, Train Loss: 0.6215226054191589, Test Loss: 0.6223741769790649\n",
      "Epoch 191/1000, Train Loss: 0.6213840246200562, Test Loss: 0.6222352981567383\n",
      "Epoch 192/1000, Train Loss: 0.6212460398674011, Test Loss: 0.6220982670783997\n",
      "Epoch 193/1000, Train Loss: 0.6211084723472595, Test Loss: 0.6219605207443237\n",
      "Epoch 194/1000, Train Loss: 0.6209716796875, Test Loss: 0.6218244433403015\n",
      "Epoch 195/1000, Train Loss: 0.6208361983299255, Test Loss: 0.6216888427734375\n",
      "Epoch 196/1000, Train Loss: 0.6207026839256287, Test Loss: 0.6215583682060242\n",
      "Epoch 197/1000, Train Loss: 0.6205726265907288, Test Loss: 0.6214310526847839\n",
      "Epoch 198/1000, Train Loss: 0.6204482913017273, Test Loss: 0.6213139295578003\n",
      "Epoch 199/1000, Train Loss: 0.6203315258026123, Test Loss: 0.621198296546936\n",
      "Epoch 200/1000, Train Loss: 0.6202195882797241, Test Loss: 0.6210711598396301\n",
      "Epoch 201/1000, Train Loss: 0.6200921535491943, Test Loss: 0.6209158897399902\n",
      "Epoch 202/1000, Train Loss: 0.6199396252632141, Test Loss: 0.6207549571990967\n",
      "Epoch 203/1000, Train Loss: 0.6197803020477295, Test Loss: 0.6206220388412476\n",
      "Epoch 204/1000, Train Loss: 0.619650661945343, Test Loss: 0.6205147504806519\n",
      "Epoch 205/1000, Train Loss: 0.6195467114448547, Test Loss: 0.620400071144104\n",
      "Epoch 206/1000, Train Loss: 0.6194350123405457, Test Loss: 0.6202594637870789\n",
      "Epoch 207/1000, Train Loss: 0.6192971467971802, Test Loss: 0.6201089024543762\n",
      "Epoch 208/1000, Train Loss: 0.6191516518592834, Test Loss: 0.6199788451194763\n",
      "Epoch 209/1000, Train Loss: 0.6190271377563477, Test Loss: 0.6198654770851135\n",
      "Epoch 210/1000, Train Loss: 0.6189179420471191, Test Loss: 0.6197399497032166\n",
      "Epoch 211/1000, Train Loss: 0.6187987923622131, Test Loss: 0.6195999979972839\n",
      "Epoch 212/1000, Train Loss: 0.6186625957489014, Test Loss: 0.619457483291626\n",
      "Epoch 213/1000, Train Loss: 0.6185255646705627, Test Loss: 0.619329571723938\n",
      "Epoch 214/1000, Train Loss: 0.618402898311615, Test Loss: 0.6192101240158081\n",
      "Epoch 215/1000, Train Loss: 0.618287205696106, Test Loss: 0.6190803647041321\n",
      "Epoch 216/1000, Train Loss: 0.6181637048721313, Test Loss: 0.6189436316490173\n",
      "Epoch 217/1000, Train Loss: 0.6180306673049927, Test Loss: 0.618804395198822\n",
      "Epoch 218/1000, Train Loss: 0.6178975701332092, Test Loss: 0.6186742186546326\n",
      "Epoch 219/1000, Train Loss: 0.6177721619606018, Test Loss: 0.6185508966445923\n",
      "Epoch 220/1000, Train Loss: 0.6176514625549316, Test Loss: 0.6184230446815491\n",
      "Epoch 221/1000, Train Loss: 0.6175276637077332, Test Loss: 0.6182913184165955\n",
      "Epoch 222/1000, Train Loss: 0.6173974275588989, Test Loss: 0.6181535124778748\n",
      "Epoch 223/1000, Train Loss: 0.6172639727592468, Test Loss: 0.6180203557014465\n",
      "Epoch 224/1000, Train Loss: 0.6171325445175171, Test Loss: 0.6178913712501526\n",
      "Epoch 225/1000, Train Loss: 0.6170051097869873, Test Loss: 0.6177616119384766\n",
      "Epoch 226/1000, Train Loss: 0.6168795228004456, Test Loss: 0.6176326274871826\n",
      "Epoch 227/1000, Train Loss: 0.6167526245117188, Test Loss: 0.6174970269203186\n",
      "Epoch 228/1000, Train Loss: 0.6166231036186218, Test Loss: 0.617362380027771\n",
      "Epoch 229/1000, Train Loss: 0.6164911985397339, Test Loss: 0.6172216534614563\n",
      "Epoch 230/1000, Train Loss: 0.6163582801818848, Test Loss: 0.6170836091041565\n",
      "Epoch 231/1000, Train Loss: 0.6162256598472595, Test Loss: 0.6169471144676208\n",
      "Epoch 232/1000, Train Loss: 0.6160940527915955, Test Loss: 0.6168121099472046\n",
      "Epoch 233/1000, Train Loss: 0.6159629821777344, Test Loss: 0.6166802048683167\n",
      "Epoch 234/1000, Train Loss: 0.6158314943313599, Test Loss: 0.6165454387664795\n",
      "Epoch 235/1000, Train Loss: 0.615699827671051, Test Loss: 0.6164155006408691\n",
      "Epoch 236/1000, Train Loss: 0.6155685186386108, Test Loss: 0.6162811517715454\n",
      "Epoch 237/1000, Train Loss: 0.6154371500015259, Test Loss: 0.6161523461341858\n",
      "Epoch 238/1000, Train Loss: 0.6153060793876648, Test Loss: 0.6160174012184143\n",
      "Epoch 239/1000, Train Loss: 0.615175724029541, Test Loss: 0.6158930063247681\n",
      "Epoch 240/1000, Train Loss: 0.61504727602005, Test Loss: 0.615760087966919\n",
      "Epoch 241/1000, Train Loss: 0.6149207949638367, Test Loss: 0.6156406402587891\n",
      "Epoch 242/1000, Train Loss: 0.614795982837677, Test Loss: 0.6155030131340027\n",
      "Epoch 243/1000, Train Loss: 0.6146677732467651, Test Loss: 0.6153749227523804\n",
      "Epoch 244/1000, Train Loss: 0.6145344376564026, Test Loss: 0.6152251362800598\n",
      "Epoch 245/1000, Train Loss: 0.6143907308578491, Test Loss: 0.6150829792022705\n",
      "Epoch 246/1000, Train Loss: 0.6142438650131226, Test Loss: 0.6149355173110962\n",
      "Epoch 247/1000, Train Loss: 0.6141014099121094, Test Loss: 0.6148013472557068\n",
      "Epoch 248/1000, Train Loss: 0.6139674782752991, Test Loss: 0.6146764159202576\n",
      "Epoch 249/1000, Train Loss: 0.6138414144515991, Test Loss: 0.614549994468689\n",
      "Epoch 250/1000, Train Loss: 0.6137186288833618, Test Loss: 0.6144329905509949\n",
      "Epoch 251/1000, Train Loss: 0.613595724105835, Test Loss: 0.6143010258674622\n",
      "Epoch 252/1000, Train Loss: 0.6134688854217529, Test Loss: 0.6141787767410278\n",
      "Epoch 253/1000, Train Loss: 0.6133373379707336, Test Loss: 0.61403489112854\n",
      "Epoch 254/1000, Train Loss: 0.6131994128227234, Test Loss: 0.6139015555381775\n",
      "Epoch 255/1000, Train Loss: 0.6130583882331848, Test Loss: 0.6137579679489136\n",
      "Epoch 256/1000, Train Loss: 0.6129182577133179, Test Loss: 0.6136245131492615\n",
      "Epoch 257/1000, Train Loss: 0.6127815842628479, Test Loss: 0.6134938597679138\n",
      "Epoch 258/1000, Train Loss: 0.6126499176025391, Test Loss: 0.6133648157119751\n",
      "Epoch 259/1000, Train Loss: 0.6125221252441406, Test Loss: 0.6132448315620422\n",
      "Epoch 260/1000, Train Loss: 0.6123966574668884, Test Loss: 0.6131147146224976\n",
      "Epoch 261/1000, Train Loss: 0.6122725009918213, Test Loss: 0.6130022406578064\n",
      "Epoch 262/1000, Train Loss: 0.6121503114700317, Test Loss: 0.6128737330436707\n",
      "Epoch 263/1000, Train Loss: 0.6120290756225586, Test Loss: 0.6127678751945496\n",
      "Epoch 264/1000, Train Loss: 0.6119093894958496, Test Loss: 0.612631618976593\n",
      "Epoch 265/1000, Train Loss: 0.6117851138114929, Test Loss: 0.6125168800354004\n",
      "Epoch 266/1000, Train Loss: 0.6116536855697632, Test Loss: 0.6123641133308411\n",
      "Epoch 267/1000, Train Loss: 0.6115111112594604, Test Loss: 0.6122311949729919\n",
      "Epoch 268/1000, Train Loss: 0.6113662719726562, Test Loss: 0.6120892763137817\n",
      "Epoch 269/1000, Train Loss: 0.6112273335456848, Test Loss: 0.6119648218154907\n",
      "Epoch 270/1000, Train Loss: 0.6110987067222595, Test Loss: 0.6118506789207458\n",
      "Epoch 271/1000, Train Loss: 0.6109786629676819, Test Loss: 0.6117279529571533\n",
      "Epoch 272/1000, Train Loss: 0.6108633875846863, Test Loss: 0.611626923084259\n",
      "Epoch 273/1000, Train Loss: 0.6107493042945862, Test Loss: 0.6114968657493591\n",
      "Epoch 274/1000, Train Loss: 0.6106319427490234, Test Loss: 0.6113914251327515\n",
      "Epoch 275/1000, Train Loss: 0.6105102300643921, Test Loss: 0.6112475991249084\n",
      "Epoch 276/1000, Train Loss: 0.6103801727294922, Test Loss: 0.6111279726028442\n",
      "Epoch 277/1000, Train Loss: 0.6102461814880371, Test Loss: 0.6109853386878967\n",
      "Epoch 278/1000, Train Loss: 0.6101115345954895, Test Loss: 0.6108629703521729\n",
      "Epoch 279/1000, Train Loss: 0.6099815964698792, Test Loss: 0.6107369661331177\n",
      "Epoch 280/1000, Train Loss: 0.6098580360412598, Test Loss: 0.6106169819831848\n",
      "Epoch 281/1000, Train Loss: 0.6097397208213806, Test Loss: 0.61050945520401\n",
      "Epoch 282/1000, Train Loss: 0.6096246242523193, Test Loss: 0.6103853583335876\n",
      "Epoch 283/1000, Train Loss: 0.6095111966133118, Test Loss: 0.6102887392044067\n",
      "Epoch 284/1000, Train Loss: 0.6093994975090027, Test Loss: 0.6101635694503784\n",
      "Epoch 285/1000, Train Loss: 0.6092889308929443, Test Loss: 0.6100744605064392\n",
      "Epoch 286/1000, Train Loss: 0.6091794371604919, Test Loss: 0.6099442839622498\n",
      "Epoch 287/1000, Train Loss: 0.6090668439865112, Test Loss: 0.6098511815071106\n",
      "Epoch 288/1000, Train Loss: 0.608950138092041, Test Loss: 0.6097052693367004\n",
      "Epoch 289/1000, Train Loss: 0.6088253855705261, Test Loss: 0.6095938682556152\n",
      "Epoch 290/1000, Train Loss: 0.6086962819099426, Test Loss: 0.6094498038291931\n",
      "Epoch 291/1000, Train Loss: 0.6085668802261353, Test Loss: 0.6093320250511169\n",
      "Epoch 292/1000, Train Loss: 0.6084433794021606, Test Loss: 0.6092115044593811\n",
      "Epoch 293/1000, Train Loss: 0.6083270907402039, Test Loss: 0.6090954542160034\n",
      "Epoch 294/1000, Train Loss: 0.6082170009613037, Test Loss: 0.6089943051338196\n",
      "Epoch 295/1000, Train Loss: 0.6081107258796692, Test Loss: 0.6088753342628479\n",
      "Epoch 296/1000, Train Loss: 0.6080074310302734, Test Loss: 0.6087873578071594\n",
      "Epoch 297/1000, Train Loss: 0.6079066395759583, Test Loss: 0.6086640357971191\n",
      "Epoch 298/1000, Train Loss: 0.6078063249588013, Test Loss: 0.6085828542709351\n",
      "Epoch 299/1000, Train Loss: 0.6077064275741577, Test Loss: 0.6084493398666382\n",
      "Epoch 300/1000, Train Loss: 0.6076005697250366, Test Loss: 0.608360230922699\n",
      "Epoch 301/1000, Train Loss: 0.6074894070625305, Test Loss: 0.6082127690315247\n",
      "Epoch 302/1000, Train Loss: 0.607368528842926, Test Loss: 0.6081082224845886\n",
      "Epoch 303/1000, Train Loss: 0.6072446703910828, Test Loss: 0.6079742312431335\n",
      "Epoch 304/1000, Train Loss: 0.6071235537528992, Test Loss: 0.6078659892082214\n",
      "Epoch 305/1000, Train Loss: 0.6070117950439453, Test Loss: 0.6077658534049988\n",
      "Epoch 306/1000, Train Loss: 0.6069092750549316, Test Loss: 0.607660710811615\n",
      "Epoch 307/1000, Train Loss: 0.6068129539489746, Test Loss: 0.6075797080993652\n",
      "Epoch 308/1000, Train Loss: 0.6067193150520325, Test Loss: 0.6074681282043457\n",
      "Epoch 309/1000, Train Loss: 0.6066245436668396, Test Loss: 0.607390284538269\n",
      "Epoch 310/1000, Train Loss: 0.6065274477005005, Test Loss: 0.6072683334350586\n",
      "Epoch 311/1000, Train Loss: 0.6064252853393555, Test Loss: 0.607182502746582\n",
      "Epoch 312/1000, Train Loss: 0.6063186526298523, Test Loss: 0.6070588827133179\n",
      "Epoch 313/1000, Train Loss: 0.6062074303627014, Test Loss: 0.6069619655609131\n",
      "Epoch 314/1000, Train Loss: 0.6060964465141296, Test Loss: 0.6068490743637085\n",
      "Epoch 315/1000, Train Loss: 0.6059877872467041, Test Loss: 0.606751561164856\n",
      "Epoch 316/1000, Train Loss: 0.6058833599090576, Test Loss: 0.6066504716873169\n",
      "Epoch 317/1000, Train Loss: 0.6057826280593872, Test Loss: 0.6065499186515808\n",
      "Epoch 318/1000, Train Loss: 0.6056846380233765, Test Loss: 0.6064615249633789\n",
      "Epoch 319/1000, Train Loss: 0.6055883765220642, Test Loss: 0.6063572764396667\n",
      "Epoch 320/1000, Train Loss: 0.6054945588111877, Test Loss: 0.606281042098999\n",
      "Epoch 321/1000, Train Loss: 0.6054031252861023, Test Loss: 0.6061778664588928\n",
      "Epoch 322/1000, Train Loss: 0.6053146123886108, Test Loss: 0.606110692024231\n",
      "Epoch 323/1000, Train Loss: 0.6052306890487671, Test Loss: 0.6060104370117188\n",
      "Epoch 324/1000, Train Loss: 0.6051477193832397, Test Loss: 0.6059501767158508\n",
      "Epoch 325/1000, Train Loss: 0.6050652861595154, Test Loss: 0.6058298349380493\n",
      "Epoch 326/1000, Train Loss: 0.6049696207046509, Test Loss: 0.6057445406913757\n",
      "Epoch 327/1000, Train Loss: 0.6048614978790283, Test Loss: 0.6055976748466492\n",
      "Epoch 328/1000, Train Loss: 0.6047369241714478, Test Loss: 0.6054876446723938\n",
      "Epoch 329/1000, Train Loss: 0.6046158075332642, Test Loss: 0.6053746342658997\n",
      "Epoch 330/1000, Train Loss: 0.6045100092887878, Test Loss: 0.6052800416946411\n",
      "Epoch 331/1000, Train Loss: 0.6044222712516785, Test Loss: 0.6052114367485046\n",
      "Epoch 332/1000, Train Loss: 0.6043450236320496, Test Loss: 0.6051135063171387\n",
      "Epoch 333/1000, Train Loss: 0.6042665243148804, Test Loss: 0.6050415635108948\n",
      "Epoch 334/1000, Train Loss: 0.6041805148124695, Test Loss: 0.6049199104309082\n",
      "Epoch 335/1000, Train Loss: 0.6040806174278259, Test Loss: 0.6048275232315063\n",
      "Epoch 336/1000, Train Loss: 0.6039732098579407, Test Loss: 0.6047029495239258\n",
      "Epoch 337/1000, Train Loss: 0.6038650870323181, Test Loss: 0.6046090126037598\n",
      "Epoch 338/1000, Train Loss: 0.6037645936012268, Test Loss: 0.6045165061950684\n",
      "Epoch 339/1000, Train Loss: 0.6036734580993652, Test Loss: 0.6044266819953918\n",
      "Epoch 340/1000, Train Loss: 0.6035886406898499, Test Loss: 0.6043598651885986\n",
      "Epoch 341/1000, Train Loss: 0.6035060882568359, Test Loss: 0.6042600274085999\n",
      "Epoch 342/1000, Train Loss: 0.6034214496612549, Test Loss: 0.6041914820671082\n",
      "Epoch 343/1000, Train Loss: 0.6033331155776978, Test Loss: 0.6040800213813782\n",
      "Epoch 344/1000, Train Loss: 0.6032389402389526, Test Loss: 0.6039941310882568\n",
      "Epoch 345/1000, Train Loss: 0.6031408309936523, Test Loss: 0.6038780212402344\n",
      "Epoch 346/1000, Train Loss: 0.603040337562561, Test Loss: 0.6037874817848206\n",
      "Epoch 347/1000, Train Loss: 0.6029404401779175, Test Loss: 0.6036769151687622\n",
      "Epoch 348/1000, Train Loss: 0.6028428077697754, Test Loss: 0.6035848259925842\n",
      "Epoch 349/1000, Train Loss: 0.6027475595474243, Test Loss: 0.6034868955612183\n",
      "Epoch 350/1000, Train Loss: 0.6026538014411926, Test Loss: 0.6033915281295776\n",
      "Epoch 351/1000, Train Loss: 0.6025620698928833, Test Loss: 0.6033048033714294\n",
      "Epoch 352/1000, Train Loss: 0.6024719476699829, Test Loss: 0.6032059788703918\n",
      "Epoch 353/1000, Train Loss: 0.6023839116096497, Test Loss: 0.6031298637390137\n",
      "Epoch 354/1000, Train Loss: 0.6022989749908447, Test Loss: 0.603037416934967\n",
      "Epoch 355/1000, Train Loss: 0.6022205352783203, Test Loss: 0.6029840707778931\n",
      "Epoch 356/1000, Train Loss: 0.6021546125411987, Test Loss: 0.6029129028320312\n",
      "Epoch 357/1000, Train Loss: 0.6021034717559814, Test Loss: 0.6029165983200073\n",
      "Epoch 358/1000, Train Loss: 0.6020776033401489, Test Loss: 0.6028413772583008\n",
      "Epoch 359/1000, Train Loss: 0.6020373702049255, Test Loss: 0.6028086543083191\n",
      "Epoch 360/1000, Train Loss: 0.6019576787948608, Test Loss: 0.6025905609130859\n",
      "Epoch 361/1000, Train Loss: 0.6017788052558899, Test Loss: 0.6024451851844788\n",
      "Epoch 362/1000, Train Loss: 0.6016026139259338, Test Loss: 0.6023533344268799\n",
      "Epoch 363/1000, Train Loss: 0.6015146970748901, Test Loss: 0.6023146510124207\n",
      "Epoch 364/1000, Train Loss: 0.601498007774353, Test Loss: 0.6023088693618774\n",
      "Epoch 365/1000, Train Loss: 0.6014602780342102, Test Loss: 0.6021518707275391\n",
      "Epoch 366/1000, Train Loss: 0.6013354659080505, Test Loss: 0.6020275950431824\n",
      "Epoch 367/1000, Train Loss: 0.6011949181556702, Test Loss: 0.6019518971443176\n",
      "Epoch 368/1000, Train Loss: 0.6011145114898682, Test Loss: 0.6019008159637451\n",
      "Epoch 369/1000, Train Loss: 0.6010794639587402, Test Loss: 0.6018657684326172\n",
      "Epoch 370/1000, Train Loss: 0.6010181903839111, Test Loss: 0.6017403602600098\n",
      "Epoch 371/1000, Train Loss: 0.6009035706520081, Test Loss: 0.6016420722007751\n",
      "Epoch 372/1000, Train Loss: 0.6007957458496094, Test Loss: 0.6015902161598206\n",
      "Epoch 373/1000, Train Loss: 0.6007319688796997, Test Loss: 0.6015281677246094\n",
      "Epoch 374/1000, Train Loss: 0.6006823182106018, Test Loss: 0.6014674305915833\n",
      "Epoch 375/1000, Train Loss: 0.6006051301956177, Test Loss: 0.6013567447662354\n",
      "Epoch 376/1000, Train Loss: 0.6005030274391174, Test Loss: 0.6012687087059021\n",
      "Epoch 377/1000, Train Loss: 0.6004157066345215, Test Loss: 0.6012160181999207\n",
      "Epoch 378/1000, Train Loss: 0.6003543734550476, Test Loss: 0.6011432409286499\n",
      "Epoch 379/1000, Train Loss: 0.6002939939498901, Test Loss: 0.6010716557502747\n",
      "Epoch 380/1000, Train Loss: 0.6002143025398254, Test Loss: 0.6009779572486877\n",
      "Epoch 381/1000, Train Loss: 0.6001232862472534, Test Loss: 0.6008983254432678\n",
      "Epoch 382/1000, Train Loss: 0.6000444293022156, Test Loss: 0.6008381247520447\n",
      "Epoch 383/1000, Train Loss: 0.5999799966812134, Test Loss: 0.6007682085037231\n",
      "Epoch 384/1000, Train Loss: 0.5999140739440918, Test Loss: 0.6006930470466614\n",
      "Epoch 385/1000, Train Loss: 0.5998364090919495, Test Loss: 0.6006039977073669\n",
      "Epoch 386/1000, Train Loss: 0.5997526049613953, Test Loss: 0.6005293726921082\n",
      "Epoch 387/1000, Train Loss: 0.5996755957603455, Test Loss: 0.6004581451416016\n",
      "Epoch 388/1000, Train Loss: 0.5996074676513672, Test Loss: 0.6003865599632263\n",
      "Epoch 389/1000, Train Loss: 0.599540114402771, Test Loss: 0.6003191471099854\n",
      "Epoch 390/1000, Train Loss: 0.5994673371315002, Test Loss: 0.6002284288406372\n",
      "Epoch 391/1000, Train Loss: 0.599389374256134, Test Loss: 0.6001561880111694\n",
      "Epoch 392/1000, Train Loss: 0.5993126034736633, Test Loss: 0.6000783443450928\n",
      "Epoch 393/1000, Train Loss: 0.5992412567138672, Test Loss: 0.600001871585846\n",
      "Epoch 394/1000, Train Loss: 0.5991735458374023, Test Loss: 0.5999395251274109\n",
      "Epoch 395/1000, Train Loss: 0.599105179309845, Test Loss: 0.5998559594154358\n",
      "Epoch 396/1000, Train Loss: 0.5990334749221802, Test Loss: 0.5997840762138367\n",
      "Epoch 397/1000, Train Loss: 0.5989593863487244, Test Loss: 0.5997097492218018\n",
      "Epoch 398/1000, Train Loss: 0.5988861322402954, Test Loss: 0.5996336936950684\n",
      "Epoch 399/1000, Train Loss: 0.5988147258758545, Test Loss: 0.5995657444000244\n",
      "Epoch 400/1000, Train Loss: 0.5987454056739807, Test Loss: 0.599492609500885\n",
      "Epoch 401/1000, Train Loss: 0.5986766219139099, Test Loss: 0.5994229912757874\n",
      "Epoch 402/1000, Train Loss: 0.5986074209213257, Test Loss: 0.5993480086326599\n",
      "Epoch 403/1000, Train Loss: 0.5985367298126221, Test Loss: 0.5992772579193115\n",
      "Epoch 404/1000, Train Loss: 0.5984652042388916, Test Loss: 0.5992025136947632\n",
      "Epoch 405/1000, Train Loss: 0.5983939170837402, Test Loss: 0.599134087562561\n",
      "Epoch 406/1000, Train Loss: 0.5983232259750366, Test Loss: 0.5990623235702515\n",
      "Epoch 407/1000, Train Loss: 0.5982537269592285, Test Loss: 0.5989930629730225\n",
      "Epoch 408/1000, Train Loss: 0.5981849431991577, Test Loss: 0.5989252328872681\n",
      "Epoch 409/1000, Train Loss: 0.5981166362762451, Test Loss: 0.5988532304763794\n",
      "Epoch 410/1000, Train Loss: 0.5980484485626221, Test Loss: 0.5987881422042847\n",
      "Epoch 411/1000, Train Loss: 0.5979803204536438, Test Loss: 0.5987135767936707\n",
      "Epoch 412/1000, Train Loss: 0.597912073135376, Test Loss: 0.5986499190330505\n",
      "Epoch 413/1000, Train Loss: 0.5978437066078186, Test Loss: 0.598577082157135\n",
      "Epoch 414/1000, Train Loss: 0.5977752804756165, Test Loss: 0.5985123515129089\n",
      "Epoch 415/1000, Train Loss: 0.5977070927619934, Test Loss: 0.5984412431716919\n",
      "Epoch 416/1000, Train Loss: 0.5976392030715942, Test Loss: 0.5983754992485046\n",
      "Epoch 417/1000, Train Loss: 0.5975717902183533, Test Loss: 0.5983020067214966\n",
      "Epoch 418/1000, Train Loss: 0.5975044965744019, Test Loss: 0.5982407331466675\n",
      "Epoch 419/1000, Train Loss: 0.5974379777908325, Test Loss: 0.598163902759552\n",
      "Epoch 420/1000, Train Loss: 0.5973721146583557, Test Loss: 0.5981071591377258\n",
      "Epoch 421/1000, Train Loss: 0.5973074436187744, Test Loss: 0.5980339050292969\n",
      "Epoch 422/1000, Train Loss: 0.5972446203231812, Test Loss: 0.5979772806167603\n",
      "Epoch 423/1000, Train Loss: 0.597184419631958, Test Loss: 0.597909152507782\n",
      "Epoch 424/1000, Train Loss: 0.5971274971961975, Test Loss: 0.5978659987449646\n",
      "Epoch 425/1000, Train Loss: 0.5970770120620728, Test Loss: 0.5978026986122131\n",
      "Epoch 426/1000, Train Loss: 0.59703129529953, Test Loss: 0.597781777381897\n",
      "Epoch 427/1000, Train Loss: 0.5969929695129395, Test Loss: 0.597713828086853\n",
      "Epoch 428/1000, Train Loss: 0.5969483852386475, Test Loss: 0.5976864099502563\n",
      "Epoch 429/1000, Train Loss: 0.5968965291976929, Test Loss: 0.5975740551948547\n",
      "Epoch 430/1000, Train Loss: 0.5968117713928223, Test Loss: 0.5974981784820557\n",
      "Epoch 431/1000, Train Loss: 0.5967110991477966, Test Loss: 0.5973767042160034\n",
      "Epoch 432/1000, Train Loss: 0.5966073274612427, Test Loss: 0.5973033308982849\n",
      "Epoch 433/1000, Train Loss: 0.596527636051178, Test Loss: 0.597256064414978\n",
      "Epoch 434/1000, Train Loss: 0.596476137638092, Test Loss: 0.5972045063972473\n",
      "Epoch 435/1000, Train Loss: 0.5964388251304626, Test Loss: 0.5971820950508118\n",
      "Epoch 436/1000, Train Loss: 0.5963974595069885, Test Loss: 0.597099244594574\n",
      "Epoch 437/1000, Train Loss: 0.596335232257843, Test Loss: 0.5970378518104553\n",
      "Epoch 438/1000, Train Loss: 0.5962575674057007, Test Loss: 0.5969471335411072\n",
      "Epoch 439/1000, Train Loss: 0.5961757898330688, Test Loss: 0.5968782901763916\n",
      "Epoch 440/1000, Train Loss: 0.5961066484451294, Test Loss: 0.5968318581581116\n",
      "Epoch 441/1000, Train Loss: 0.5960529446601868, Test Loss: 0.5967735648155212\n",
      "Epoch 442/1000, Train Loss: 0.5960069894790649, Test Loss: 0.5967382192611694\n",
      "Epoch 443/1000, Train Loss: 0.5959578156471252, Test Loss: 0.596666693687439\n",
      "Epoch 444/1000, Train Loss: 0.5958982706069946, Test Loss: 0.5966092348098755\n",
      "Epoch 445/1000, Train Loss: 0.5958312749862671, Test Loss: 0.5965374708175659\n",
      "Epoch 446/1000, Train Loss: 0.5957631468772888, Test Loss: 0.5964757204055786\n",
      "Epoch 447/1000, Train Loss: 0.5957016944885254, Test Loss: 0.5964280366897583\n",
      "Epoch 448/1000, Train Loss: 0.5956481099128723, Test Loss: 0.5963705778121948\n",
      "Epoch 449/1000, Train Loss: 0.5955983996391296, Test Loss: 0.59632807970047\n",
      "Epoch 450/1000, Train Loss: 0.5955477356910706, Test Loss: 0.5962626338005066\n",
      "Epoch 451/1000, Train Loss: 0.5954918265342712, Test Loss: 0.5962085723876953\n",
      "Epoch 452/1000, Train Loss: 0.5954321622848511, Test Loss: 0.5961423516273499\n",
      "Epoch 453/1000, Train Loss: 0.5953707695007324, Test Loss: 0.5960863828659058\n",
      "Epoch 454/1000, Train Loss: 0.5953114032745361, Test Loss: 0.5960317254066467\n",
      "Epoch 455/1000, Train Loss: 0.5952555537223816, Test Loss: 0.5959798097610474\n",
      "Epoch 456/1000, Train Loss: 0.5952029228210449, Test Loss: 0.5959324240684509\n",
      "Epoch 457/1000, Train Loss: 0.5951517820358276, Test Loss: 0.5958790183067322\n",
      "Epoch 458/1000, Train Loss: 0.5950999855995178, Test Loss: 0.5958307981491089\n",
      "Epoch 459/1000, Train Loss: 0.5950469374656677, Test Loss: 0.5957704186439514\n",
      "Epoch 460/1000, Train Loss: 0.5949921011924744, Test Loss: 0.5957241654396057\n",
      "Epoch 461/1000, Train Loss: 0.5949363112449646, Test Loss: 0.5956627726554871\n",
      "Epoch 462/1000, Train Loss: 0.5948805809020996, Test Loss: 0.5956176519393921\n",
      "Epoch 463/1000, Train Loss: 0.594825804233551, Test Loss: 0.5955623388290405\n",
      "Epoch 464/1000, Train Loss: 0.5947728157043457, Test Loss: 0.5955139398574829\n",
      "Epoch 465/1000, Train Loss: 0.5947210192680359, Test Loss: 0.5954657196998596\n",
      "Epoch 466/1000, Train Loss: 0.5946703553199768, Test Loss: 0.5954139232635498\n",
      "Epoch 467/1000, Train Loss: 0.5946205854415894, Test Loss: 0.5953734517097473\n",
      "Epoch 468/1000, Train Loss: 0.5945709347724915, Test Loss: 0.5953196287155151\n",
      "Epoch 469/1000, Train Loss: 0.5945206880569458, Test Loss: 0.595281183719635\n",
      "Epoch 470/1000, Train Loss: 0.5944705009460449, Test Loss: 0.5952244997024536\n",
      "Epoch 471/1000, Train Loss: 0.5944202542304993, Test Loss: 0.5951860547065735\n",
      "Epoch 472/1000, Train Loss: 0.5943701863288879, Test Loss: 0.5951297879219055\n",
      "Epoch 473/1000, Train Loss: 0.5943201780319214, Test Loss: 0.595089316368103\n",
      "Epoch 474/1000, Train Loss: 0.5942708253860474, Test Loss: 0.5950357913970947\n",
      "Epoch 475/1000, Train Loss: 0.594221830368042, Test Loss: 0.594993531703949\n",
      "Epoch 476/1000, Train Loss: 0.5941736102104187, Test Loss: 0.5949409008026123\n",
      "Epoch 477/1000, Train Loss: 0.5941258668899536, Test Loss: 0.5948967337608337\n",
      "Epoch 478/1000, Train Loss: 0.5940784811973572, Test Loss: 0.5948449969291687\n",
      "Epoch 479/1000, Train Loss: 0.594031035900116, Test Loss: 0.5948021411895752\n",
      "Epoch 480/1000, Train Loss: 0.593984067440033, Test Loss: 0.594748318195343\n",
      "Epoch 481/1000, Train Loss: 0.5939375162124634, Test Loss: 0.5947067141532898\n",
      "Epoch 482/1000, Train Loss: 0.5938916206359863, Test Loss: 0.5946526527404785\n",
      "Epoch 483/1000, Train Loss: 0.593846321105957, Test Loss: 0.5946125984191895\n",
      "Epoch 484/1000, Train Loss: 0.5938016176223755, Test Loss: 0.5945591330528259\n",
      "Epoch 485/1000, Train Loss: 0.5937575697898865, Test Loss: 0.5945241451263428\n",
      "Epoch 486/1000, Train Loss: 0.593714714050293, Test Loss: 0.5944696664810181\n",
      "Epoch 487/1000, Train Loss: 0.5936733484268188, Test Loss: 0.5944442749023438\n",
      "Epoch 488/1000, Train Loss: 0.593634843826294, Test Loss: 0.5943929553031921\n",
      "Epoch 489/1000, Train Loss: 0.5936004519462585, Test Loss: 0.594384491443634\n",
      "Epoch 490/1000, Train Loss: 0.5935730338096619, Test Loss: 0.5943437218666077\n",
      "Epoch 491/1000, Train Loss: 0.5935530662536621, Test Loss: 0.5943641066551208\n",
      "Epoch 492/1000, Train Loss: 0.5935455560684204, Test Loss: 0.5943222045898438\n",
      "Epoch 493/1000, Train Loss: 0.5935359597206116, Test Loss: 0.5943520069122314\n",
      "Epoch 494/1000, Train Loss: 0.5935252904891968, Test Loss: 0.5942567586898804\n",
      "Epoch 495/1000, Train Loss: 0.5934715270996094, Test Loss: 0.5942104458808899\n",
      "Epoch 496/1000, Train Loss: 0.5933855772018433, Test Loss: 0.5940741896629333\n",
      "Epoch 497/1000, Train Loss: 0.5932788848876953, Test Loss: 0.5940101146697998\n",
      "Epoch 498/1000, Train Loss: 0.5932036638259888, Test Loss: 0.5939854383468628\n",
      "Epoch 499/1000, Train Loss: 0.59317547082901, Test Loss: 0.5939646363258362\n",
      "Epoch 500/1000, Train Loss: 0.5931729674339294, Test Loss: 0.5939761400222778\n",
      "Epoch 501/1000, Train Loss: 0.5931616425514221, Test Loss: 0.5939024686813354\n",
      "Epoch 502/1000, Train Loss: 0.5931125283241272, Test Loss: 0.5938509702682495\n",
      "Epoch 503/1000, Train Loss: 0.5930416584014893, Test Loss: 0.593773365020752\n",
      "Epoch 504/1000, Train Loss: 0.5929770469665527, Test Loss: 0.5937386155128479\n",
      "Epoch 505/1000, Train Loss: 0.5929418802261353, Test Loss: 0.5937311053276062\n",
      "Epoch 506/1000, Train Loss: 0.5929266214370728, Test Loss: 0.5936926007270813\n",
      "Epoch 507/1000, Train Loss: 0.5929045677185059, Test Loss: 0.5936694145202637\n",
      "Epoch 508/1000, Train Loss: 0.5928628444671631, Test Loss: 0.5935966372489929\n",
      "Epoch 509/1000, Train Loss: 0.5928062200546265, Test Loss: 0.5935551524162292\n",
      "Epoch 510/1000, Train Loss: 0.5927562713623047, Test Loss: 0.593521237373352\n",
      "Epoch 511/1000, Train Loss: 0.59272301197052, Test Loss: 0.5934867858886719\n",
      "Epoch 512/1000, Train Loss: 0.5926993489265442, Test Loss: 0.5934700965881348\n",
      "Epoch 513/1000, Train Loss: 0.5926727652549744, Test Loss: 0.5934144854545593\n",
      "Epoch 514/1000, Train Loss: 0.592634379863739, Test Loss: 0.5933730006217957\n",
      "Epoch 515/1000, Train Loss: 0.5925891995429993, Test Loss: 0.5933244228363037\n",
      "Epoch 516/1000, Train Loss: 0.5925464630126953, Test Loss: 0.5932825803756714\n",
      "Epoch 517/1000, Train Loss: 0.5925121307373047, Test Loss: 0.5932601690292358\n",
      "Epoch 518/1000, Train Loss: 0.5924842357635498, Test Loss: 0.5932225584983826\n",
      "Epoch 519/1000, Train Loss: 0.5924558043479919, Test Loss: 0.5931971073150635\n",
      "Epoch 520/1000, Train Loss: 0.5924227833747864, Test Loss: 0.5931553244590759\n",
      "Epoch 521/1000, Train Loss: 0.5923846364021301, Test Loss: 0.5931186079978943\n",
      "Epoch 522/1000, Train Loss: 0.5923460721969604, Test Loss: 0.5930875539779663\n",
      "Epoch 523/1000, Train Loss: 0.5923112630844116, Test Loss: 0.5930551290512085\n",
      "Epoch 524/1000, Train Loss: 0.5922806859016418, Test Loss: 0.5930334329605103\n",
      "Epoch 525/1000, Train Loss: 0.5922518372535706, Test Loss: 0.593000590801239\n",
      "Epoch 526/1000, Train Loss: 0.5922213196754456, Test Loss: 0.5929723978042603\n",
      "Epoch 527/1000, Train Loss: 0.5921884179115295, Test Loss: 0.5929350852966309\n",
      "Epoch 528/1000, Train Loss: 0.5921534299850464, Test Loss: 0.5929017663002014\n",
      "Epoch 529/1000, Train Loss: 0.5921188592910767, Test Loss: 0.592868447303772\n",
      "Epoch 530/1000, Train Loss: 0.5920861959457397, Test Loss: 0.592836320400238\n",
      "Epoch 531/1000, Train Loss: 0.5920555591583252, Test Loss: 0.5928066968917847\n",
      "Epoch 532/1000, Train Loss: 0.5920257568359375, Test Loss: 0.5927733182907104\n",
      "Epoch 533/1000, Train Loss: 0.5919957756996155, Test Loss: 0.5927422046661377\n",
      "Epoch 534/1000, Train Loss: 0.5919646620750427, Test Loss: 0.5927059054374695\n",
      "Epoch 535/1000, Train Loss: 0.591932475566864, Test Loss: 0.59267657995224\n",
      "Epoch 536/1000, Train Loss: 0.5918998122215271, Test Loss: 0.5926424860954285\n",
      "Epoch 537/1000, Train Loss: 0.5918676853179932, Test Loss: 0.5926129817962646\n",
      "Epoch 538/1000, Train Loss: 0.5918362140655518, Test Loss: 0.5925822257995605\n",
      "Epoch 539/1000, Train Loss: 0.5918055772781372, Test Loss: 0.5925505757331848\n",
      "Epoch 540/1000, Train Loss: 0.5917755961418152, Test Loss: 0.5925222039222717\n",
      "Epoch 541/1000, Train Loss: 0.5917456150054932, Test Loss: 0.5924896597862244\n",
      "Epoch 542/1000, Train Loss: 0.5917157530784607, Test Loss: 0.5924598574638367\n",
      "Epoch 543/1000, Train Loss: 0.5916856527328491, Test Loss: 0.5924277305603027\n",
      "Epoch 544/1000, Train Loss: 0.5916555523872375, Test Loss: 0.5923982858657837\n",
      "Epoch 545/1000, Train Loss: 0.5916253924369812, Test Loss: 0.5923673510551453\n",
      "Epoch 546/1000, Train Loss: 0.5915950536727905, Test Loss: 0.5923393368721008\n",
      "Epoch 547/1000, Train Loss: 0.5915648937225342, Test Loss: 0.5923088192939758\n",
      "Epoch 548/1000, Train Loss: 0.5915349125862122, Test Loss: 0.5922818779945374\n",
      "Epoch 549/1000, Train Loss: 0.5915051102638245, Test Loss: 0.5922533869743347\n",
      "Epoch 550/1000, Train Loss: 0.591475784778595, Test Loss: 0.592227041721344\n",
      "Epoch 551/1000, Train Loss: 0.5914466381072998, Test Loss: 0.5922018885612488\n",
      "Epoch 552/1000, Train Loss: 0.5914179682731628, Test Loss: 0.5921729803085327\n",
      "Epoch 553/1000, Train Loss: 0.5913892984390259, Test Loss: 0.5921472311019897\n",
      "Epoch 554/1000, Train Loss: 0.5913606286048889, Test Loss: 0.5921180844306946\n",
      "Epoch 555/1000, Train Loss: 0.5913321375846863, Test Loss: 0.59209144115448\n",
      "Epoch 556/1000, Train Loss: 0.5913036465644836, Test Loss: 0.59206223487854\n",
      "Epoch 557/1000, Train Loss: 0.5912752747535706, Test Loss: 0.592036247253418\n",
      "Epoch 558/1000, Train Loss: 0.5912471413612366, Test Loss: 0.5920040011405945\n",
      "Epoch 559/1000, Train Loss: 0.5912191271781921, Test Loss: 0.5919814109802246\n",
      "Epoch 560/1000, Train Loss: 0.5911915898323059, Test Loss: 0.5919492840766907\n",
      "Epoch 561/1000, Train Loss: 0.591164231300354, Test Loss: 0.5919299721717834\n",
      "Epoch 562/1000, Train Loss: 0.5911370515823364, Test Loss: 0.5919012427330017\n",
      "Epoch 563/1000, Train Loss: 0.5911099314689636, Test Loss: 0.5918799042701721\n",
      "Epoch 564/1000, Train Loss: 0.5910831689834595, Test Loss: 0.5918532609939575\n",
      "Epoch 565/1000, Train Loss: 0.5910569429397583, Test Loss: 0.5918357372283936\n",
      "Epoch 566/1000, Train Loss: 0.5910319685935974, Test Loss: 0.5918107628822327\n",
      "Epoch 567/1000, Train Loss: 0.5910083055496216, Test Loss: 0.591798722743988\n",
      "Epoch 568/1000, Train Loss: 0.590987503528595, Test Loss: 0.5917758941650391\n",
      "Epoch 569/1000, Train Loss: 0.5909687876701355, Test Loss: 0.5917694568634033\n",
      "Epoch 570/1000, Train Loss: 0.5909543037414551, Test Loss: 0.5917543172836304\n",
      "Epoch 571/1000, Train Loss: 0.5909411907196045, Test Loss: 0.5917495489120483\n",
      "Epoch 572/1000, Train Loss: 0.5909315943717957, Test Loss: 0.5917330384254456\n",
      "Epoch 573/1000, Train Loss: 0.5909145474433899, Test Loss: 0.5917145013809204\n",
      "Epoch 574/1000, Train Loss: 0.5908942818641663, Test Loss: 0.5916763544082642\n",
      "Epoch 575/1000, Train Loss: 0.5908538699150085, Test Loss: 0.5916256904602051\n",
      "Epoch 576/1000, Train Loss: 0.590805172920227, Test Loss: 0.5915725827217102\n",
      "Epoch 577/1000, Train Loss: 0.5907493829727173, Test Loss: 0.5915237069129944\n",
      "Epoch 578/1000, Train Loss: 0.5907012224197388, Test Loss: 0.5914887189865112\n",
      "Epoch 579/1000, Train Loss: 0.5906668305397034, Test Loss: 0.5914680361747742\n",
      "Epoch 580/1000, Train Loss: 0.590645968914032, Test Loss: 0.5914570093154907\n",
      "Epoch 581/1000, Train Loss: 0.590632975101471, Test Loss: 0.5914427638053894\n",
      "Epoch 582/1000, Train Loss: 0.5906195640563965, Test Loss: 0.5914272665977478\n",
      "Epoch 583/1000, Train Loss: 0.5906009078025818, Test Loss: 0.5913981199264526\n",
      "Epoch 584/1000, Train Loss: 0.5905717611312866, Test Loss: 0.5913607478141785\n",
      "Epoch 585/1000, Train Loss: 0.5905365347862244, Test Loss: 0.5913287401199341\n",
      "Epoch 586/1000, Train Loss: 0.5904980301856995, Test Loss: 0.5912885665893555\n",
      "Epoch 587/1000, Train Loss: 0.590462863445282, Test Loss: 0.5912662148475647\n",
      "Epoch 588/1000, Train Loss: 0.590433657169342, Test Loss: 0.5912407636642456\n",
      "Epoch 589/1000, Train Loss: 0.5904103517532349, Test Loss: 0.5912202596664429\n",
      "Epoch 590/1000, Train Loss: 0.5903908014297485, Test Loss: 0.5912063717842102\n",
      "Epoch 591/1000, Train Loss: 0.5903714895248413, Test Loss: 0.5911806225776672\n",
      "Epoch 592/1000, Train Loss: 0.5903507471084595, Test Loss: 0.5911652445793152\n",
      "Epoch 593/1000, Train Loss: 0.590326189994812, Test Loss: 0.5911318063735962\n",
      "Epoch 594/1000, Train Loss: 0.5902987718582153, Test Loss: 0.5911104679107666\n",
      "Epoch 595/1000, Train Loss: 0.5902688503265381, Test Loss: 0.5910770893096924\n",
      "Epoch 596/1000, Train Loss: 0.590238630771637, Test Loss: 0.5910524129867554\n",
      "Epoch 597/1000, Train Loss: 0.5902095437049866, Test Loss: 0.5910291075706482\n",
      "Epoch 598/1000, Train Loss: 0.5901830196380615, Test Loss: 0.5910026431083679\n",
      "Epoch 599/1000, Train Loss: 0.5901583433151245, Test Loss: 0.590985119342804\n",
      "Epoch 600/1000, Train Loss: 0.590135395526886, Test Loss: 0.5909616947174072\n",
      "Epoch 601/1000, Train Loss: 0.5901132822036743, Test Loss: 0.5909441709518433\n",
      "Epoch 602/1000, Train Loss: 0.5900914669036865, Test Loss: 0.5909220576286316\n",
      "Epoch 603/1000, Train Loss: 0.5900694131851196, Test Loss: 0.5909043550491333\n",
      "Epoch 604/1000, Train Loss: 0.5900471806526184, Test Loss: 0.5908798575401306\n",
      "Epoch 605/1000, Train Loss: 0.5900245308876038, Test Loss: 0.5908603072166443\n",
      "Epoch 606/1000, Train Loss: 0.5900013446807861, Test Loss: 0.5908365845680237\n",
      "Epoch 607/1000, Train Loss: 0.5899778008460999, Test Loss: 0.5908151268959045\n",
      "Epoch 608/1000, Train Loss: 0.5899533033370972, Test Loss: 0.5907913446426392\n",
      "Epoch 609/1000, Train Loss: 0.589928388595581, Test Loss: 0.5907697081565857\n",
      "Epoch 610/1000, Train Loss: 0.5899024605751038, Test Loss: 0.5907423496246338\n",
      "Epoch 611/1000, Train Loss: 0.589876651763916, Test Loss: 0.5907233953475952\n",
      "Epoch 612/1000, Train Loss: 0.5898504257202148, Test Loss: 0.5906935930252075\n",
      "Epoch 613/1000, Train Loss: 0.5898244976997375, Test Loss: 0.5906747579574585\n",
      "Epoch 614/1000, Train Loss: 0.5897985100746155, Test Loss: 0.590645968914032\n",
      "Epoch 615/1000, Train Loss: 0.5897731184959412, Test Loss: 0.5906228423118591\n",
      "Epoch 616/1000, Train Loss: 0.5897480249404907, Test Loss: 0.5905967354774475\n",
      "Epoch 617/1000, Train Loss: 0.5897232890129089, Test Loss: 0.5905713438987732\n",
      "Epoch 618/1000, Train Loss: 0.5896989703178406, Test Loss: 0.5905480980873108\n",
      "Epoch 619/1000, Train Loss: 0.5896748304367065, Test Loss: 0.5905231833457947\n",
      "Epoch 620/1000, Train Loss: 0.5896506309509277, Test Loss: 0.590501070022583\n",
      "Epoch 621/1000, Train Loss: 0.5896269679069519, Test Loss: 0.5904818177223206\n",
      "Epoch 622/1000, Train Loss: 0.5896037220954895, Test Loss: 0.5904566049575806\n",
      "Epoch 623/1000, Train Loss: 0.5895809531211853, Test Loss: 0.5904420018196106\n",
      "Epoch 624/1000, Train Loss: 0.5895588397979736, Test Loss: 0.5904149413108826\n",
      "Epoch 625/1000, Train Loss: 0.5895375609397888, Test Loss: 0.5904021859169006\n",
      "Epoch 626/1000, Train Loss: 0.5895177125930786, Test Loss: 0.5903780460357666\n",
      "Epoch 627/1000, Train Loss: 0.589500367641449, Test Loss: 0.590374231338501\n",
      "Epoch 628/1000, Train Loss: 0.5894861817359924, Test Loss: 0.5903574228286743\n",
      "Epoch 629/1000, Train Loss: 0.5894775390625, Test Loss: 0.5903674364089966\n",
      "Epoch 630/1000, Train Loss: 0.5894747376441956, Test Loss: 0.5903631448745728\n",
      "Epoch 631/1000, Train Loss: 0.5894817113876343, Test Loss: 0.5903782248497009\n",
      "Epoch 632/1000, Train Loss: 0.5894845128059387, Test Loss: 0.5903715491294861\n",
      "Epoch 633/1000, Train Loss: 0.5894865989685059, Test Loss: 0.5903483033180237\n",
      "Epoch 634/1000, Train Loss: 0.5894551277160645, Test Loss: 0.5902895927429199\n",
      "Epoch 635/1000, Train Loss: 0.5894029140472412, Test Loss: 0.5902177691459656\n",
      "Epoch 636/1000, Train Loss: 0.5893252491950989, Test Loss: 0.5901504755020142\n",
      "Epoch 637/1000, Train Loss: 0.5892618894577026, Test Loss: 0.5901203155517578\n",
      "Epoch 638/1000, Train Loss: 0.5892306566238403, Test Loss: 0.5901213884353638\n",
      "Epoch 639/1000, Train Loss: 0.5892297029495239, Test Loss: 0.5901228785514832\n",
      "Epoch 640/1000, Train Loss: 0.5892379879951477, Test Loss: 0.5901222825050354\n",
      "Epoch 641/1000, Train Loss: 0.5892278552055359, Test Loss: 0.5900833606719971\n",
      "Epoch 642/1000, Train Loss: 0.5891957879066467, Test Loss: 0.5900400280952454\n",
      "Epoch 643/1000, Train Loss: 0.5891454219818115, Test Loss: 0.5899975299835205\n",
      "Epoch 644/1000, Train Loss: 0.5891023874282837, Test Loss: 0.5899689197540283\n",
      "Epoch 645/1000, Train Loss: 0.589077889919281, Test Loss: 0.589966356754303\n",
      "Epoch 646/1000, Train Loss: 0.5890690088272095, Test Loss: 0.5899537205696106\n",
      "Epoch 647/1000, Train Loss: 0.5890628099441528, Test Loss: 0.5899426937103271\n",
      "Epoch 648/1000, Train Loss: 0.5890461802482605, Test Loss: 0.5899111032485962\n",
      "Epoch 649/1000, Train Loss: 0.5890175700187683, Test Loss: 0.5898751020431519\n",
      "Epoch 650/1000, Train Loss: 0.5889817476272583, Test Loss: 0.5898439884185791\n",
      "Epoch 651/1000, Train Loss: 0.5889499187469482, Test Loss: 0.5898208618164062\n",
      "Epoch 652/1000, Train Loss: 0.5889278054237366, Test Loss: 0.589806854724884\n",
      "Epoch 653/1000, Train Loss: 0.5889135003089905, Test Loss: 0.5897918939590454\n",
      "Epoch 654/1000, Train Loss: 0.5888998508453369, Test Loss: 0.5897730588912964\n",
      "Epoch 655/1000, Train Loss: 0.588880717754364, Test Loss: 0.5897429585456848\n",
      "Epoch 656/1000, Train Loss: 0.5888562202453613, Test Loss: 0.5897185802459717\n",
      "Epoch 657/1000, Train Loss: 0.5888281464576721, Test Loss: 0.5896859169006348\n",
      "Epoch 658/1000, Train Loss: 0.5888009071350098, Test Loss: 0.5896626710891724\n",
      "Epoch 659/1000, Train Loss: 0.5887773633003235, Test Loss: 0.5896425247192383\n",
      "Epoch 660/1000, Train Loss: 0.5887578129768372, Test Loss: 0.5896215438842773\n",
      "Epoch 661/1000, Train Loss: 0.5887400507926941, Test Loss: 0.589608371257782\n",
      "Epoch 662/1000, Train Loss: 0.5887213349342346, Test Loss: 0.5895830392837524\n",
      "Epoch 663/1000, Train Loss: 0.5886998772621155, Test Loss: 0.5895622372627258\n",
      "Epoch 664/1000, Train Loss: 0.5886755585670471, Test Loss: 0.5895341634750366\n",
      "Epoch 665/1000, Train Loss: 0.5886502265930176, Test Loss: 0.5895086526870728\n",
      "Epoch 666/1000, Train Loss: 0.5886250734329224, Test Loss: 0.5894843339920044\n",
      "Epoch 667/1000, Train Loss: 0.5886009335517883, Test Loss: 0.5894613265991211\n",
      "Epoch 668/1000, Train Loss: 0.5885779857635498, Test Loss: 0.5894370079040527\n",
      "Epoch 669/1000, Train Loss: 0.5885555744171143, Test Loss: 0.589412271976471\n",
      "Epoch 670/1000, Train Loss: 0.5885331630706787, Test Loss: 0.5893846154212952\n",
      "Epoch 671/1000, Train Loss: 0.5885101556777954, Test Loss: 0.5893545150756836\n",
      "Epoch 672/1000, Train Loss: 0.5884865522384644, Test Loss: 0.5893245339393616\n",
      "Epoch 673/1000, Train Loss: 0.5884608626365662, Test Loss: 0.5892881155014038\n",
      "Epoch 674/1000, Train Loss: 0.5884339213371277, Test Loss: 0.5892578959465027\n",
      "Epoch 675/1000, Train Loss: 0.5884057879447937, Test Loss: 0.589217483997345\n",
      "Epoch 676/1000, Train Loss: 0.5883764624595642, Test Loss: 0.5891827940940857\n",
      "Epoch 677/1000, Train Loss: 0.5883463621139526, Test Loss: 0.5891503691673279\n",
      "Epoch 678/1000, Train Loss: 0.5883156657218933, Test Loss: 0.5891121625900269\n",
      "Epoch 679/1000, Train Loss: 0.5882835984230042, Test Loss: 0.5890873074531555\n",
      "Epoch 680/1000, Train Loss: 0.5882524251937866, Test Loss: 0.5890527367591858\n",
      "Epoch 681/1000, Train Loss: 0.588222324848175, Test Loss: 0.5890334248542786\n",
      "Epoch 682/1000, Train Loss: 0.5881975889205933, Test Loss: 0.5890145301818848\n",
      "Epoch 683/1000, Train Loss: 0.5881815552711487, Test Loss: 0.5890219211578369\n",
      "Epoch 684/1000, Train Loss: 0.5881791114807129, Test Loss: 0.589037299156189\n",
      "Epoch 685/1000, Train Loss: 0.5881912708282471, Test Loss: 0.5890613198280334\n",
      "Epoch 686/1000, Train Loss: 0.5882205367088318, Test Loss: 0.5890795588493347\n",
      "Epoch 687/1000, Train Loss: 0.5882355570793152, Test Loss: 0.5890651345252991\n",
      "Epoch 688/1000, Train Loss: 0.5882357954978943, Test Loss: 0.5890055298805237\n",
      "Epoch 689/1000, Train Loss: 0.5881590247154236, Test Loss: 0.5888867378234863\n",
      "Epoch 690/1000, Train Loss: 0.5880563259124756, Test Loss: 0.5888092517852783\n",
      "Epoch 691/1000, Train Loss: 0.5879666209220886, Test Loss: 0.5887762904167175\n",
      "Epoch 692/1000, Train Loss: 0.587937593460083, Test Loss: 0.5887851715087891\n",
      "Epoch 693/1000, Train Loss: 0.5879566669464111, Test Loss: 0.588813304901123\n",
      "Epoch 694/1000, Train Loss: 0.5879741907119751, Test Loss: 0.5887829661369324\n",
      "Epoch 695/1000, Train Loss: 0.5879599452018738, Test Loss: 0.5887333154678345\n",
      "Epoch 696/1000, Train Loss: 0.5878970623016357, Test Loss: 0.5886638164520264\n",
      "Epoch 697/1000, Train Loss: 0.5878307819366455, Test Loss: 0.5886270999908447\n",
      "Epoch 698/1000, Train Loss: 0.5877951979637146, Test Loss: 0.588631272315979\n",
      "Epoch 699/1000, Train Loss: 0.5877934098243713, Test Loss: 0.5886335968971252\n",
      "Epoch 700/1000, Train Loss: 0.5877987146377563, Test Loss: 0.5886211395263672\n",
      "Epoch 701/1000, Train Loss: 0.58777916431427, Test Loss: 0.5885717272758484\n",
      "Epoch 702/1000, Train Loss: 0.5877372026443481, Test Loss: 0.5885314345359802\n",
      "Epoch 703/1000, Train Loss: 0.5876901149749756, Test Loss: 0.5884949564933777\n",
      "Epoch 704/1000, Train Loss: 0.5876606702804565, Test Loss: 0.5884854197502136\n",
      "Epoch 705/1000, Train Loss: 0.5876500010490417, Test Loss: 0.5884819626808167\n",
      "Epoch 706/1000, Train Loss: 0.5876429677009583, Test Loss: 0.5884566903114319\n",
      "Epoch 707/1000, Train Loss: 0.5876259803771973, Test Loss: 0.5884392261505127\n",
      "Epoch 708/1000, Train Loss: 0.5875940918922424, Test Loss: 0.5883969068527222\n",
      "Epoch 709/1000, Train Loss: 0.5875588059425354, Test Loss: 0.588374674320221\n",
      "Epoch 710/1000, Train Loss: 0.5875300168991089, Test Loss: 0.5883583426475525\n",
      "Epoch 711/1000, Train Loss: 0.587511420249939, Test Loss: 0.5883408784866333\n",
      "Epoch 712/1000, Train Loss: 0.5874981880187988, Test Loss: 0.5883325934410095\n",
      "Epoch 713/1000, Train Loss: 0.5874817371368408, Test Loss: 0.5883053541183472\n",
      "Epoch 714/1000, Train Loss: 0.5874588489532471, Test Loss: 0.5882886052131653\n",
      "Epoch 715/1000, Train Loss: 0.587430477142334, Test Loss: 0.5882577300071716\n",
      "Epoch 716/1000, Train Loss: 0.5874025225639343, Test Loss: 0.5882418155670166\n",
      "Epoch 717/1000, Train Loss: 0.5873783826828003, Test Loss: 0.5882226824760437\n",
      "Epoch 718/1000, Train Loss: 0.5873587131500244, Test Loss: 0.5882049202919006\n",
      "Epoch 719/1000, Train Loss: 0.5873409509658813, Test Loss: 0.5881953239440918\n",
      "Epoch 720/1000, Train Loss: 0.5873218774795532, Test Loss: 0.5881673097610474\n",
      "Epoch 721/1000, Train Loss: 0.587300181388855, Test Loss: 0.5881552696228027\n",
      "Epoch 722/1000, Train Loss: 0.5872758626937866, Test Loss: 0.5881232619285583\n",
      "Epoch 723/1000, Train Loss: 0.5872505903244019, Test Loss: 0.5881059765815735\n",
      "Epoch 724/1000, Train Loss: 0.5872260928153992, Test Loss: 0.5880846977233887\n",
      "Epoch 725/1000, Train Loss: 0.5872035622596741, Test Loss: 0.5880643725395203\n",
      "Epoch 726/1000, Train Loss: 0.5871827006340027, Test Loss: 0.5880504846572876\n",
      "Epoch 727/1000, Train Loss: 0.5871623754501343, Test Loss: 0.5880267024040222\n",
      "Epoch 728/1000, Train Loss: 0.5871416330337524, Test Loss: 0.5880123376846313\n",
      "Epoch 729/1000, Train Loss: 0.5871203541755676, Test Loss: 0.587983250617981\n",
      "Epoch 730/1000, Train Loss: 0.587098240852356, Test Loss: 0.5879663228988647\n",
      "Epoch 731/1000, Train Loss: 0.5870752930641174, Test Loss: 0.5879369378089905\n",
      "Epoch 732/1000, Train Loss: 0.5870522260665894, Test Loss: 0.587917149066925\n",
      "Epoch 733/1000, Train Loss: 0.5870292782783508, Test Loss: 0.587891161441803\n",
      "Epoch 734/1000, Train Loss: 0.5870065689086914, Test Loss: 0.5878674387931824\n",
      "Epoch 735/1000, Train Loss: 0.5869841575622559, Test Loss: 0.5878469944000244\n",
      "Epoch 736/1000, Train Loss: 0.5869618654251099, Test Loss: 0.5878252387046814\n",
      "Epoch 737/1000, Train Loss: 0.5869399309158325, Test Loss: 0.5878075957298279\n",
      "Epoch 738/1000, Train Loss: 0.5869181752204895, Test Loss: 0.5877861380577087\n",
      "Epoch 739/1000, Train Loss: 0.586896538734436, Test Loss: 0.5877667665481567\n",
      "Epoch 740/1000, Train Loss: 0.5868752598762512, Test Loss: 0.5877431035041809\n",
      "Epoch 741/1000, Train Loss: 0.5868539810180664, Test Loss: 0.5877255797386169\n",
      "Epoch 742/1000, Train Loss: 0.5868328213691711, Test Loss: 0.5877054929733276\n",
      "Epoch 743/1000, Train Loss: 0.5868120193481445, Test Loss: 0.5876909494400024\n",
      "Epoch 744/1000, Train Loss: 0.5867915749549866, Test Loss: 0.5876682996749878\n",
      "Epoch 745/1000, Train Loss: 0.5867714881896973, Test Loss: 0.5876540541648865\n",
      "Epoch 746/1000, Train Loss: 0.5867519974708557, Test Loss: 0.5876293778419495\n",
      "Epoch 747/1000, Train Loss: 0.5867339372634888, Test Loss: 0.587622880935669\n",
      "Epoch 748/1000, Train Loss: 0.5867176651954651, Test Loss: 0.5875962376594543\n",
      "Epoch 749/1000, Train Loss: 0.5867039561271667, Test Loss: 0.5875981450080872\n",
      "Epoch 750/1000, Train Loss: 0.586692214012146, Test Loss: 0.5875753164291382\n",
      "Epoch 751/1000, Train Loss: 0.586683988571167, Test Loss: 0.5875808000564575\n",
      "Epoch 752/1000, Train Loss: 0.5866750478744507, Test Loss: 0.587559163570404\n",
      "Epoch 753/1000, Train Loss: 0.5866685509681702, Test Loss: 0.5875612497329712\n",
      "Epoch 754/1000, Train Loss: 0.5866552591323853, Test Loss: 0.5875263810157776\n",
      "Epoch 755/1000, Train Loss: 0.5866389870643616, Test Loss: 0.5875151753425598\n",
      "Epoch 756/1000, Train Loss: 0.5866075158119202, Test Loss: 0.5874547362327576\n",
      "Epoch 757/1000, Train Loss: 0.5865696668624878, Test Loss: 0.5874270796775818\n",
      "Epoch 758/1000, Train Loss: 0.5865241885185242, Test Loss: 0.5873750448226929\n",
      "Epoch 759/1000, Train Loss: 0.5864846706390381, Test Loss: 0.5873503684997559\n",
      "Epoch 760/1000, Train Loss: 0.586455225944519, Test Loss: 0.5873369574546814\n",
      "Epoch 761/1000, Train Loss: 0.5864375233650208, Test Loss: 0.5873175859451294\n",
      "Epoch 762/1000, Train Loss: 0.5864278078079224, Test Loss: 0.5873243808746338\n",
      "Epoch 763/1000, Train Loss: 0.5864203572273254, Test Loss: 0.5872997045516968\n",
      "Epoch 764/1000, Train Loss: 0.5864111185073853, Test Loss: 0.5873004794120789\n",
      "Epoch 765/1000, Train Loss: 0.5863940715789795, Test Loss: 0.5872640013694763\n",
      "Epoch 766/1000, Train Loss: 0.5863713026046753, Test Loss: 0.5872488617897034\n",
      "Epoch 767/1000, Train Loss: 0.5863410234451294, Test Loss: 0.5872083902359009\n",
      "Epoch 768/1000, Train Loss: 0.586310088634491, Test Loss: 0.5871884226799011\n",
      "Epoch 769/1000, Train Loss: 0.5862811803817749, Test Loss: 0.5871629118919373\n",
      "Epoch 770/1000, Train Loss: 0.5862572193145752, Test Loss: 0.5871443152427673\n",
      "Epoch 771/1000, Train Loss: 0.5862383842468262, Test Loss: 0.5871356725692749\n",
      "Epoch 772/1000, Train Loss: 0.5862230062484741, Test Loss: 0.5871133804321289\n",
      "Epoch 773/1000, Train Loss: 0.5862094759941101, Test Loss: 0.5871137380599976\n",
      "Epoch 774/1000, Train Loss: 0.5861956477165222, Test Loss: 0.5870875716209412\n",
      "Epoch 775/1000, Train Loss: 0.5861811637878418, Test Loss: 0.5870869755744934\n",
      "Epoch 776/1000, Train Loss: 0.5861639380455017, Test Loss: 0.5870555639266968\n",
      "Epoch 777/1000, Train Loss: 0.5861449837684631, Test Loss: 0.5870488286018372\n",
      "Epoch 778/1000, Train Loss: 0.5861228704452515, Test Loss: 0.5870100259780884\n",
      "Epoch 779/1000, Train Loss: 0.5860996246337891, Test Loss: 0.586999773979187\n",
      "Epoch 780/1000, Train Loss: 0.5860746502876282, Test Loss: 0.5869616270065308\n",
      "Epoch 781/1000, Train Loss: 0.5860505104064941, Test Loss: 0.5869485139846802\n",
      "Epoch 782/1000, Train Loss: 0.5860272645950317, Test Loss: 0.5869215726852417\n",
      "Epoch 783/1000, Train Loss: 0.5860056281089783, Test Loss: 0.5869035720825195\n",
      "Epoch 784/1000, Train Loss: 0.5859853029251099, Test Loss: 0.5868898630142212\n",
      "Epoch 785/1000, Train Loss: 0.5859660506248474, Test Loss: 0.5868672728538513\n",
      "Epoch 786/1000, Train Loss: 0.5859479308128357, Test Loss: 0.5868566632270813\n",
      "Epoch 787/1000, Train Loss: 0.5859301686286926, Test Loss: 0.5868327021598816\n",
      "Epoch 788/1000, Train Loss: 0.5859130024909973, Test Loss: 0.5868224501609802\n",
      "Epoch 789/1000, Train Loss: 0.5858967304229736, Test Loss: 0.5867996215820312\n",
      "Epoch 790/1000, Train Loss: 0.5858811140060425, Test Loss: 0.5867955088615417\n",
      "Epoch 791/1000, Train Loss: 0.5858662128448486, Test Loss: 0.5867711305618286\n",
      "Epoch 792/1000, Train Loss: 0.5858526229858398, Test Loss: 0.5867752432823181\n",
      "Epoch 793/1000, Train Loss: 0.5858392715454102, Test Loss: 0.5867454409599304\n",
      "Epoch 794/1000, Train Loss: 0.5858270525932312, Test Loss: 0.5867534279823303\n",
      "Epoch 795/1000, Train Loss: 0.5858142971992493, Test Loss: 0.5867209434509277\n",
      "Epoch 796/1000, Train Loss: 0.5858032703399658, Test Loss: 0.5867273211479187\n",
      "Epoch 797/1000, Train Loss: 0.5857880115509033, Test Loss: 0.586689293384552\n",
      "Epoch 798/1000, Train Loss: 0.585772693157196, Test Loss: 0.5866910219192505\n",
      "Epoch 799/1000, Train Loss: 0.5857508778572083, Test Loss: 0.5866449475288391\n",
      "Epoch 800/1000, Train Loss: 0.5857276320457458, Test Loss: 0.5866407752037048\n",
      "Epoch 801/1000, Train Loss: 0.5856992602348328, Test Loss: 0.5865920782089233\n",
      "Epoch 802/1000, Train Loss: 0.5856702327728271, Test Loss: 0.5865806937217712\n",
      "Epoch 803/1000, Train Loss: 0.585640013217926, Test Loss: 0.5865405201911926\n",
      "Epoch 804/1000, Train Loss: 0.5856124758720398, Test Loss: 0.5865252017974854\n",
      "Epoch 805/1000, Train Loss: 0.5855880975723267, Test Loss: 0.5865007042884827\n",
      "Epoch 806/1000, Train Loss: 0.5855673551559448, Test Loss: 0.58648282289505\n",
      "Epoch 807/1000, Train Loss: 0.5855494737625122, Test Loss: 0.586469292640686\n",
      "Epoch 808/1000, Train Loss: 0.5855337381362915, Test Loss: 0.5864480137825012\n",
      "Epoch 809/1000, Train Loss: 0.585519552230835, Test Loss: 0.5864437222480774\n",
      "Epoch 810/1000, Train Loss: 0.5855067372322083, Test Loss: 0.5864190459251404\n",
      "Epoch 811/1000, Train Loss: 0.5854949951171875, Test Loss: 0.5864229798316956\n",
      "Epoch 812/1000, Train Loss: 0.5854834914207458, Test Loss: 0.5863931775093079\n",
      "Epoch 813/1000, Train Loss: 0.5854737758636475, Test Loss: 0.5864039063453674\n",
      "Epoch 814/1000, Train Loss: 0.5854617357254028, Test Loss: 0.5863690972328186\n",
      "Epoch 815/1000, Train Loss: 0.585449755191803, Test Loss: 0.5863792300224304\n",
      "Epoch 816/1000, Train Loss: 0.5854321122169495, Test Loss: 0.5863355398178101\n",
      "Epoch 817/1000, Train Loss: 0.5854134559631348, Test Loss: 0.5863358378410339\n",
      "Epoch 818/1000, Train Loss: 0.5853878855705261, Test Loss: 0.5862880349159241\n",
      "Epoch 819/1000, Train Loss: 0.5853615999221802, Test Loss: 0.5862768888473511\n",
      "Epoch 820/1000, Train Loss: 0.5853322744369507, Test Loss: 0.5862334370613098\n",
      "Epoch 821/1000, Train Loss: 0.585304319858551, Test Loss: 0.5862183570861816\n",
      "Epoch 822/1000, Train Loss: 0.585278332233429, Test Loss: 0.5861876010894775\n",
      "Epoch 823/1000, Train Loss: 0.585256040096283, Test Loss: 0.5861712694168091\n",
      "Epoch 824/1000, Train Loss: 0.5852366089820862, Test Loss: 0.5861557722091675\n",
      "Epoch 825/1000, Train Loss: 0.5852194428443909, Test Loss: 0.5861347913742065\n",
      "Epoch 826/1000, Train Loss: 0.5852041840553284, Test Loss: 0.5861314535140991\n",
      "Epoch 827/1000, Train Loss: 0.5851901769638062, Test Loss: 0.5861014723777771\n",
      "Epoch 828/1000, Train Loss: 0.585177481174469, Test Loss: 0.5861064791679382\n",
      "Epoch 829/1000, Train Loss: 0.585164487361908, Test Loss: 0.5860692262649536\n",
      "Epoch 830/1000, Train Loss: 0.5851526260375977, Test Loss: 0.5860801935195923\n",
      "Epoch 831/1000, Train Loss: 0.5851398706436157, Test Loss: 0.5860394239425659\n",
      "Epoch 832/1000, Train Loss: 0.5851280093193054, Test Loss: 0.5860487222671509\n",
      "Epoch 833/1000, Train Loss: 0.5851132273674011, Test Loss: 0.5860078930854797\n",
      "Epoch 834/1000, Train Loss: 0.5850983262062073, Test Loss: 0.5860090851783752\n",
      "Epoch 835/1000, Train Loss: 0.5850791335105896, Test Loss: 0.5859705805778503\n",
      "Epoch 836/1000, Train Loss: 0.5850594639778137, Test Loss: 0.5859603881835938\n",
      "Epoch 837/1000, Train Loss: 0.5850345492362976, Test Loss: 0.5859209299087524\n",
      "Epoch 838/1000, Train Loss: 0.5850094556808472, Test Loss: 0.5859031081199646\n",
      "Epoch 839/1000, Train Loss: 0.5849814414978027, Test Loss: 0.5858663320541382\n",
      "Epoch 840/1000, Train Loss: 0.5849554538726807, Test Loss: 0.5858543515205383\n",
      "Epoch 841/1000, Train Loss: 0.5849312543869019, Test Loss: 0.5858226418495178\n",
      "Epoch 842/1000, Train Loss: 0.5849099159240723, Test Loss: 0.585813045501709\n",
      "Epoch 843/1000, Train Loss: 0.5848906636238098, Test Loss: 0.585788905620575\n",
      "Epoch 844/1000, Train Loss: 0.5848727226257324, Test Loss: 0.5857716798782349\n",
      "Epoch 845/1000, Train Loss: 0.5848559737205505, Test Loss: 0.5857621431350708\n",
      "Epoch 846/1000, Train Loss: 0.5848401784896851, Test Loss: 0.5857352018356323\n",
      "Epoch 847/1000, Train Loss: 0.5848257541656494, Test Loss: 0.5857369303703308\n",
      "Epoch 848/1000, Train Loss: 0.5848119258880615, Test Loss: 0.585702657699585\n",
      "Epoch 849/1000, Train Loss: 0.58479905128479, Test Loss: 0.585710346698761\n",
      "Epoch 850/1000, Train Loss: 0.5847868323326111, Test Loss: 0.5856802463531494\n",
      "Epoch 851/1000, Train Loss: 0.5847766399383545, Test Loss: 0.5856907963752747\n",
      "Epoch 852/1000, Train Loss: 0.5847663283348083, Test Loss: 0.5856598615646362\n",
      "Epoch 853/1000, Train Loss: 0.5847578644752502, Test Loss: 0.5856704711914062\n",
      "Epoch 854/1000, Train Loss: 0.584747314453125, Test Loss: 0.5856351852416992\n",
      "Epoch 855/1000, Train Loss: 0.5847377181053162, Test Loss: 0.5856431722640991\n",
      "Epoch 856/1000, Train Loss: 0.5847209095954895, Test Loss: 0.5855954885482788\n",
      "Epoch 857/1000, Train Loss: 0.5847011208534241, Test Loss: 0.5855912566184998\n",
      "Epoch 858/1000, Train Loss: 0.5846720933914185, Test Loss: 0.5855348110198975\n",
      "Epoch 859/1000, Train Loss: 0.5846408009529114, Test Loss: 0.5855214595794678\n",
      "Epoch 860/1000, Train Loss: 0.5846051573753357, Test Loss: 0.5854736566543579\n",
      "Epoch 861/1000, Train Loss: 0.5845726132392883, Test Loss: 0.5854571461677551\n",
      "Epoch 862/1000, Train Loss: 0.5845446586608887, Test Loss: 0.5854312181472778\n",
      "Epoch 863/1000, Train Loss: 0.5845234990119934, Test Loss: 0.5854136943817139\n",
      "Epoch 864/1000, Train Loss: 0.5845074653625488, Test Loss: 0.5854077339172363\n",
      "Epoch 865/1000, Train Loss: 0.5844946503639221, Test Loss: 0.5853845477104187\n",
      "Epoch 866/1000, Train Loss: 0.5844836235046387, Test Loss: 0.5853906869888306\n",
      "Epoch 867/1000, Train Loss: 0.58447265625, Test Loss: 0.5853613018989563\n",
      "Epoch 868/1000, Train Loss: 0.5844619870185852, Test Loss: 0.5853661894798279\n",
      "Epoch 869/1000, Train Loss: 0.5844482183456421, Test Loss: 0.5853307843208313\n",
      "Epoch 870/1000, Train Loss: 0.5844327807426453, Test Loss: 0.5853279232978821\n",
      "Epoch 871/1000, Train Loss: 0.5844119787216187, Test Loss: 0.5852875709533691\n",
      "Epoch 872/1000, Train Loss: 0.5843890309333801, Test Loss: 0.5852774977684021\n",
      "Epoch 873/1000, Train Loss: 0.5843638181686401, Test Loss: 0.5852369070053101\n",
      "Epoch 874/1000, Train Loss: 0.5843390226364136, Test Loss: 0.5852247476577759\n",
      "Epoch 875/1000, Train Loss: 0.5843139290809631, Test Loss: 0.5851888060569763\n",
      "Epoch 876/1000, Train Loss: 0.5842902064323425, Test Loss: 0.5851733684539795\n",
      "Epoch 877/1000, Train Loss: 0.5842679142951965, Test Loss: 0.585144579410553\n",
      "Epoch 878/1000, Train Loss: 0.5842472314834595, Test Loss: 0.5851246118545532\n",
      "Epoch 879/1000, Train Loss: 0.5842276215553284, Test Loss: 0.585105299949646\n",
      "Epoch 880/1000, Train Loss: 0.5842089056968689, Test Loss: 0.5850837230682373\n",
      "Epoch 881/1000, Train Loss: 0.5841907262802124, Test Loss: 0.5850697755813599\n",
      "Epoch 882/1000, Train Loss: 0.5841728448867798, Test Loss: 0.5850455164909363\n",
      "Epoch 883/1000, Train Loss: 0.5841554999351501, Test Loss: 0.5850347876548767\n",
      "Epoch 884/1000, Train Loss: 0.5841386318206787, Test Loss: 0.585010826587677\n",
      "Epoch 885/1000, Train Loss: 0.5841231346130371, Test Loss: 0.5850077271461487\n",
      "Epoch 886/1000, Train Loss: 0.5841084718704224, Test Loss: 0.5849835872650146\n",
      "Epoch 887/1000, Train Loss: 0.5840957760810852, Test Loss: 0.5849854350090027\n",
      "Epoch 888/1000, Train Loss: 0.5840839147567749, Test Loss: 0.5849595069885254\n",
      "Epoch 889/1000, Train Loss: 0.5840752124786377, Test Loss: 0.5849711298942566\n",
      "Epoch 890/1000, Train Loss: 0.5840667486190796, Test Loss: 0.5849381685256958\n",
      "Epoch 891/1000, Train Loss: 0.5840619206428528, Test Loss: 0.5849591493606567\n",
      "Epoch 892/1000, Train Loss: 0.5840549468994141, Test Loss: 0.5849170684814453\n",
      "Epoch 893/1000, Train Loss: 0.5840503573417664, Test Loss: 0.5849399566650391\n",
      "Epoch 894/1000, Train Loss: 0.5840355753898621, Test Loss: 0.58488529920578\n",
      "Epoch 895/1000, Train Loss: 0.5840178728103638, Test Loss: 0.5848881006240845\n",
      "Epoch 896/1000, Train Loss: 0.583984375, Test Loss: 0.5848209857940674\n",
      "Epoch 897/1000, Train Loss: 0.5839474201202393, Test Loss: 0.5848004817962646\n",
      "Epoch 898/1000, Train Loss: 0.5839031338691711, Test Loss: 0.5847457051277161\n",
      "Epoch 899/1000, Train Loss: 0.5838635563850403, Test Loss: 0.5847228169441223\n",
      "Epoch 900/1000, Train Loss: 0.5838313102722168, Test Loss: 0.5846977233886719\n",
      "Epoch 901/1000, Train Loss: 0.5838090181350708, Test Loss: 0.5846810936927795\n",
      "Epoch 902/1000, Train Loss: 0.5837946534156799, Test Loss: 0.5846817493438721\n",
      "Epoch 903/1000, Train Loss: 0.5837859511375427, Test Loss: 0.584665060043335\n",
      "Epoch 904/1000, Train Loss: 0.5837805271148682, Test Loss: 0.5846794247627258\n",
      "Epoch 905/1000, Train Loss: 0.5837736129760742, Test Loss: 0.5846453309059143\n",
      "Epoch 906/1000, Train Loss: 0.5837647318840027, Test Loss: 0.5846582651138306\n",
      "Epoch 907/1000, Train Loss: 0.5837470293045044, Test Loss: 0.5846041440963745\n",
      "Epoch 908/1000, Train Loss: 0.5837246179580688, Test Loss: 0.5846040844917297\n",
      "Epoch 909/1000, Train Loss: 0.5836937427520752, Test Loss: 0.58454829454422\n",
      "Epoch 910/1000, Train Loss: 0.5836604237556458, Test Loss: 0.5845292806625366\n",
      "Epoch 911/1000, Train Loss: 0.5836262106895447, Test Loss: 0.5844947099685669\n",
      "Epoch 912/1000, Train Loss: 0.5835962891578674, Test Loss: 0.584466814994812\n",
      "Epoch 913/1000, Train Loss: 0.5835714340209961, Test Loss: 0.5844565629959106\n",
      "Epoch 914/1000, Train Loss: 0.5835512280464172, Test Loss: 0.5844288468360901\n",
      "Epoch 915/1000, Train Loss: 0.5835348963737488, Test Loss: 0.5844272971153259\n",
      "Epoch 916/1000, Train Loss: 0.5835206508636475, Test Loss: 0.5844036936759949\n",
      "Epoch 917/1000, Train Loss: 0.5835075974464417, Test Loss: 0.5844002366065979\n",
      "Epoch 918/1000, Train Loss: 0.5834931135177612, Test Loss: 0.584375262260437\n",
      "Epoch 919/1000, Train Loss: 0.583477258682251, Test Loss: 0.5843686461448669\n",
      "Epoch 920/1000, Train Loss: 0.5834581255912781, Test Loss: 0.5843337774276733\n",
      "Epoch 921/1000, Train Loss: 0.5834381580352783, Test Loss: 0.5843284726142883\n",
      "Epoch 922/1000, Train Loss: 0.5834158658981323, Test Loss: 0.5842883586883545\n",
      "Epoch 923/1000, Train Loss: 0.5833933353424072, Test Loss: 0.5842851400375366\n",
      "Epoch 924/1000, Train Loss: 0.583368182182312, Test Loss: 0.5842465162277222\n",
      "Epoch 925/1000, Train Loss: 0.5833437442779541, Test Loss: 0.5842382907867432\n",
      "Epoch 926/1000, Train Loss: 0.5833186507225037, Test Loss: 0.584205150604248\n",
      "Epoch 927/1000, Train Loss: 0.5832940340042114, Test Loss: 0.5841914415359497\n",
      "Epoch 928/1000, Train Loss: 0.5832701921463013, Test Loss: 0.5841665863990784\n",
      "Epoch 929/1000, Train Loss: 0.5832472443580627, Test Loss: 0.5841484665870667\n",
      "Epoch 930/1000, Train Loss: 0.5832249522209167, Test Loss: 0.5841288566589355\n",
      "Epoch 931/1000, Train Loss: 0.5832033753395081, Test Loss: 0.5841090679168701\n",
      "Epoch 932/1000, Train Loss: 0.5831820964813232, Test Loss: 0.5840942859649658\n",
      "Epoch 933/1000, Train Loss: 0.5831612944602966, Test Loss: 0.5840709209442139\n",
      "Epoch 934/1000, Train Loss: 0.5831404328346252, Test Loss: 0.5840550065040588\n",
      "Epoch 935/1000, Train Loss: 0.5831193327903748, Test Loss: 0.5840285420417786\n",
      "Epoch 936/1000, Train Loss: 0.5830984711647034, Test Loss: 0.5840145945549011\n",
      "Epoch 937/1000, Train Loss: 0.5830778479576111, Test Loss: 0.5839909911155701\n",
      "Epoch 938/1000, Train Loss: 0.5830577611923218, Test Loss: 0.5839822888374329\n",
      "Epoch 939/1000, Train Loss: 0.5830388069152832, Test Loss: 0.583958625793457\n",
      "Epoch 940/1000, Train Loss: 0.5830219388008118, Test Loss: 0.5839598774909973\n",
      "Epoch 941/1000, Train Loss: 0.5830076336860657, Test Loss: 0.5839395523071289\n",
      "Epoch 942/1000, Train Loss: 0.5829991102218628, Test Loss: 0.5839582681655884\n",
      "Epoch 943/1000, Train Loss: 0.5829975008964539, Test Loss: 0.5839484333992004\n",
      "Epoch 944/1000, Train Loss: 0.583009660243988, Test Loss: 0.5839990377426147\n",
      "Epoch 945/1000, Train Loss: 0.5830301642417908, Test Loss: 0.5840023756027222\n",
      "Epoch 946/1000, Train Loss: 0.5830721259117126, Test Loss: 0.5840744972229004\n",
      "Epoch 947/1000, Train Loss: 0.583093523979187, Test Loss: 0.5840321779251099\n",
      "Epoch 948/1000, Train Loss: 0.5831093788146973, Test Loss: 0.5840228796005249\n",
      "Epoch 949/1000, Train Loss: 0.5830338001251221, Test Loss: 0.5838654041290283\n",
      "Epoch 950/1000, Train Loss: 0.5829312205314636, Test Loss: 0.5837954878807068\n",
      "Epoch 951/1000, Train Loss: 0.5828129053115845, Test Loss: 0.5837153196334839\n",
      "Epoch 952/1000, Train Loss: 0.5827505588531494, Test Loss: 0.5837202072143555\n",
      "Epoch 953/1000, Train Loss: 0.5827549695968628, Test Loss: 0.5837835073471069\n",
      "Epoch 954/1000, Train Loss: 0.58278888463974, Test Loss: 0.5837647318840027\n",
      "Epoch 955/1000, Train Loss: 0.582809567451477, Test Loss: 0.5837801694869995\n",
      "Epoch 956/1000, Train Loss: 0.5827709436416626, Test Loss: 0.5836670398712158\n",
      "Epoch 957/1000, Train Loss: 0.5827021598815918, Test Loss: 0.5836309790611267\n",
      "Epoch 958/1000, Train Loss: 0.5826319456100464, Test Loss: 0.583591639995575\n",
      "Epoch 959/1000, Train Loss: 0.5826006531715393, Test Loss: 0.5835861563682556\n",
      "Epoch 960/1000, Train Loss: 0.5826048851013184, Test Loss: 0.583628237247467\n",
      "Epoch 961/1000, Train Loss: 0.5826134085655212, Test Loss: 0.5835772752761841\n",
      "Epoch 962/1000, Train Loss: 0.5826019048690796, Test Loss: 0.5835755467414856\n",
      "Epoch 963/1000, Train Loss: 0.5825581550598145, Test Loss: 0.583495020866394\n",
      "Epoch 964/1000, Train Loss: 0.5825070738792419, Test Loss: 0.5834718942642212\n",
      "Epoch 965/1000, Train Loss: 0.5824687480926514, Test Loss: 0.583462655544281\n",
      "Epoch 966/1000, Train Loss: 0.5824522972106934, Test Loss: 0.5834391713142395\n",
      "Epoch 967/1000, Train Loss: 0.5824480652809143, Test Loss: 0.5834606289863586\n",
      "Epoch 968/1000, Train Loss: 0.5824383497238159, Test Loss: 0.58340984582901\n",
      "Epoch 969/1000, Train Loss: 0.5824154019355774, Test Loss: 0.5834041237831116\n",
      "Epoch 970/1000, Train Loss: 0.5823791027069092, Test Loss: 0.5833557844161987\n",
      "Epoch 971/1000, Train Loss: 0.5823420286178589, Test Loss: 0.5833321213722229\n",
      "Epoch 972/1000, Train Loss: 0.5823122262954712, Test Loss: 0.5833206176757812\n",
      "Epoch 973/1000, Train Loss: 0.5822926163673401, Test Loss: 0.5832939147949219\n",
      "Epoch 974/1000, Train Loss: 0.5822785496711731, Test Loss: 0.5833005309104919\n",
      "Epoch 975/1000, Train Loss: 0.5822628736495972, Test Loss: 0.5832617282867432\n",
      "Epoch 976/1000, Train Loss: 0.5822415947914124, Test Loss: 0.5832557678222656\n",
      "Epoch 977/1000, Train Loss: 0.5822132229804993, Test Loss: 0.5832133293151855\n",
      "Epoch 978/1000, Train Loss: 0.5821825265884399, Test Loss: 0.5831986665725708\n",
      "Epoch 979/1000, Train Loss: 0.5821534395217896, Test Loss: 0.5831750631332397\n",
      "Epoch 980/1000, Train Loss: 0.5821287035942078, Test Loss: 0.5831551551818848\n",
      "Epoch 981/1000, Train Loss: 0.5821074843406677, Test Loss: 0.5831487774848938\n",
      "Epoch 982/1000, Train Loss: 0.5820879936218262, Test Loss: 0.5831167101860046\n",
      "Epoch 983/1000, Train Loss: 0.582068145275116, Test Loss: 0.5831174254417419\n",
      "Epoch 984/1000, Train Loss: 0.5820463299751282, Test Loss: 0.5830755233764648\n",
      "Epoch 985/1000, Train Loss: 0.5820225477218628, Test Loss: 0.5830695629119873\n",
      "Epoch 986/1000, Train Loss: 0.5819962024688721, Test Loss: 0.5830285549163818\n",
      "Epoch 987/1000, Train Loss: 0.5819690823554993, Test Loss: 0.583010196685791\n",
      "Epoch 988/1000, Train Loss: 0.5819422602653503, Test Loss: 0.5829824805259705\n",
      "Epoch 989/1000, Train Loss: 0.5819162726402283, Test Loss: 0.5829566121101379\n",
      "Epoch 990/1000, Train Loss: 0.5818918943405151, Test Loss: 0.5829407572746277\n",
      "Epoch 991/1000, Train Loss: 0.5818684697151184, Test Loss: 0.5829115509986877\n",
      "Epoch 992/1000, Train Loss: 0.5818459987640381, Test Loss: 0.5829030275344849\n",
      "Epoch 993/1000, Train Loss: 0.5818236470222473, Test Loss: 0.5828713774681091\n",
      "Epoch 994/1000, Train Loss: 0.581801176071167, Test Loss: 0.582862913608551\n",
      "Epoch 995/1000, Train Loss: 0.581778347492218, Test Loss: 0.5828274488449097\n",
      "Epoch 996/1000, Train Loss: 0.5817552804946899, Test Loss: 0.5828198194503784\n",
      "Epoch 997/1000, Train Loss: 0.581731379032135, Test Loss: 0.5827791690826416\n",
      "Epoch 998/1000, Train Loss: 0.5817070603370667, Test Loss: 0.5827729105949402\n",
      "Epoch 999/1000, Train Loss: 0.5816822648048401, Test Loss: 0.5827312469482422\n",
      "Epoch 1000/1000, Train Loss: 0.5816571712493896, Test Loss: 0.582724392414093\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:0.726635588450797\n",
      "Accuracy:0.7074659090909091\n",
      "Confusion Matrix:\n",
      "[[56086 26788]\n",
      " [24698 68428]]\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "hidden_size = 30\n",
    "model = mlp.MLP_mach1(28, hidden_size)\n",
    "# Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = .001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 1000\n",
    "# Train the model using our function\n",
    "train_losses, test_losses = mlp.train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n",
    "# Make predictions on the validation set\n",
    "f1, acc, cm = mlp.getResults(train_losses, test_losses, model, X_val, y_val)\n",
    "# Print the results\n",
    "print(\"F1:\" + str(f1))\n",
    "print(\"Accuracy:\" + str(acc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6917847394943237, Test Loss: 0.690656304359436\n",
      "Epoch 2/100, Train Loss: 0.6907647252082825, Test Loss: 0.6890964508056641\n",
      "Epoch 3/100, Train Loss: 0.6894295811653137, Test Loss: 0.6874492168426514\n",
      "Epoch 4/100, Train Loss: 0.6881676316261292, Test Loss: 0.685097336769104\n",
      "Epoch 5/100, Train Loss: 0.6862054467201233, Test Loss: 0.682166337966919\n",
      "Epoch 6/100, Train Loss: 0.6837232708930969, Test Loss: 0.6786639094352722\n",
      "Epoch 7/100, Train Loss: 0.681274950504303, Test Loss: 0.674452543258667\n",
      "Epoch 8/100, Train Loss: 0.6781654953956604, Test Loss: 0.6697661280632019\n",
      "Epoch 9/100, Train Loss: 0.6748629808425903, Test Loss: 0.6648595929145813\n",
      "Epoch 10/100, Train Loss: 0.6715644001960754, Test Loss: 0.6611517667770386\n",
      "Epoch 11/100, Train Loss: 0.6696828603744507, Test Loss: 0.6611748337745667\n",
      "Epoch 12/100, Train Loss: 0.6697683930397034, Test Loss: 0.6561025381088257\n",
      "Epoch 13/100, Train Loss: 0.6661772727966309, Test Loss: 0.6549273729324341\n",
      "Epoch 14/100, Train Loss: 0.6656155586242676, Test Loss: 0.6538956761360168\n",
      "Epoch 15/100, Train Loss: 0.664246141910553, Test Loss: 0.6515455842018127\n",
      "Epoch 16/100, Train Loss: 0.6621712446212769, Test Loss: 0.6509861350059509\n",
      "Epoch 17/100, Train Loss: 0.6619515419006348, Test Loss: 0.6491063237190247\n",
      "Epoch 18/100, Train Loss: 0.6600360870361328, Test Loss: 0.6492072939872742\n",
      "Epoch 19/100, Train Loss: 0.6594839096069336, Test Loss: 0.6478953957557678\n",
      "Epoch 20/100, Train Loss: 0.6577968597412109, Test Loss: 0.6465917229652405\n",
      "Epoch 21/100, Train Loss: 0.6570606231689453, Test Loss: 0.6457769274711609\n",
      "Epoch 22/100, Train Loss: 0.6561189889907837, Test Loss: 0.6450191140174866\n",
      "Epoch 23/100, Train Loss: 0.6545449495315552, Test Loss: 0.6448136568069458\n",
      "Epoch 24/100, Train Loss: 0.6539570689201355, Test Loss: 0.6425724625587463\n",
      "Epoch 25/100, Train Loss: 0.6526123881340027, Test Loss: 0.6414370536804199\n",
      "Epoch 26/100, Train Loss: 0.6518659591674805, Test Loss: 0.6401830911636353\n",
      "Epoch 27/100, Train Loss: 0.6503115296363831, Test Loss: 0.6402214765548706\n",
      "Epoch 28/100, Train Loss: 0.6501362323760986, Test Loss: 0.6378259062767029\n",
      "Epoch 29/100, Train Loss: 0.6486725807189941, Test Loss: 0.6363901495933533\n",
      "Epoch 30/100, Train Loss: 0.6483935713768005, Test Loss: 0.6360365748405457\n",
      "Epoch 31/100, Train Loss: 0.6469403505325317, Test Loss: 0.6362534165382385\n",
      "Epoch 32/100, Train Loss: 0.6460195183753967, Test Loss: 0.6341844797134399\n",
      "Epoch 33/100, Train Loss: 0.6450027823448181, Test Loss: 0.6334689259529114\n",
      "Epoch 34/100, Train Loss: 0.6443418860435486, Test Loss: 0.6338433623313904\n",
      "Epoch 35/100, Train Loss: 0.6433809995651245, Test Loss: 0.6333490014076233\n",
      "Epoch 36/100, Train Loss: 0.6428138613700867, Test Loss: 0.6313857436180115\n",
      "Epoch 37/100, Train Loss: 0.6418364644050598, Test Loss: 0.6304734945297241\n",
      "Epoch 38/100, Train Loss: 0.6406489014625549, Test Loss: 0.6304735541343689\n",
      "Epoch 39/100, Train Loss: 0.6399613618850708, Test Loss: 0.62907475233078\n",
      "Epoch 40/100, Train Loss: 0.6392923593521118, Test Loss: 0.6276576519012451\n",
      "Epoch 41/100, Train Loss: 0.6385464668273926, Test Loss: 0.6276490688323975\n",
      "Epoch 42/100, Train Loss: 0.6377832293510437, Test Loss: 0.6273926496505737\n",
      "Epoch 43/100, Train Loss: 0.6369946002960205, Test Loss: 0.6259304881095886\n",
      "Epoch 44/100, Train Loss: 0.6363466382026672, Test Loss: 0.6253556609153748\n",
      "Epoch 45/100, Train Loss: 0.6358016729354858, Test Loss: 0.6253710389137268\n",
      "Epoch 46/100, Train Loss: 0.6349511742591858, Test Loss: 0.6238718032836914\n",
      "Epoch 47/100, Train Loss: 0.633952260017395, Test Loss: 0.6227666735649109\n",
      "Epoch 48/100, Train Loss: 0.6337774991989136, Test Loss: 0.6229157447814941\n",
      "Epoch 49/100, Train Loss: 0.6331228613853455, Test Loss: 0.6217296719551086\n",
      "Epoch 50/100, Train Loss: 0.6319364309310913, Test Loss: 0.6206866502761841\n",
      "Epoch 51/100, Train Loss: 0.6315010786056519, Test Loss: 0.6206980347633362\n",
      "Epoch 52/100, Train Loss: 0.6311377286911011, Test Loss: 0.6194663643836975\n",
      "Epoch 53/100, Train Loss: 0.629936158657074, Test Loss: 0.6183209419250488\n",
      "Epoch 54/100, Train Loss: 0.6294251680374146, Test Loss: 0.6181145906448364\n",
      "Epoch 55/100, Train Loss: 0.6284439563751221, Test Loss: 0.6168952584266663\n",
      "Epoch 56/100, Train Loss: 0.6283376216888428, Test Loss: 0.6159764528274536\n",
      "Epoch 57/100, Train Loss: 0.6272146105766296, Test Loss: 0.6157178282737732\n",
      "Epoch 58/100, Train Loss: 0.6271502375602722, Test Loss: 0.6144477128982544\n",
      "Epoch 59/100, Train Loss: 0.6260245442390442, Test Loss: 0.6135690212249756\n",
      "Epoch 60/100, Train Loss: 0.6250669956207275, Test Loss: 0.6128848195075989\n",
      "Epoch 61/100, Train Loss: 0.6239352226257324, Test Loss: 0.6114962100982666\n",
      "Epoch 62/100, Train Loss: 0.6235218644142151, Test Loss: 0.6108057498931885\n",
      "Epoch 63/100, Train Loss: 0.622713565826416, Test Loss: 0.6097875237464905\n",
      "Epoch 64/100, Train Loss: 0.6221646666526794, Test Loss: 0.6087178587913513\n",
      "Epoch 65/100, Train Loss: 0.6212563514709473, Test Loss: 0.6082040071487427\n",
      "Epoch 66/100, Train Loss: 0.6204745769500732, Test Loss: 0.6067995429039001\n",
      "Epoch 67/100, Train Loss: 0.6195034980773926, Test Loss: 0.606480062007904\n",
      "Epoch 68/100, Train Loss: 0.6190356016159058, Test Loss: 0.6049575805664062\n",
      "Epoch 69/100, Train Loss: 0.6184779405593872, Test Loss: 0.6041664481163025\n",
      "Epoch 70/100, Train Loss: 0.6174575090408325, Test Loss: 0.6031408905982971\n",
      "Epoch 71/100, Train Loss: 0.6167309880256653, Test Loss: 0.6020246148109436\n",
      "Epoch 72/100, Train Loss: 0.6159058213233948, Test Loss: 0.6016174554824829\n",
      "Epoch 73/100, Train Loss: 0.6151557564735413, Test Loss: 0.6002326011657715\n",
      "Epoch 74/100, Train Loss: 0.613872766494751, Test Loss: 0.5999894738197327\n",
      "Epoch 75/100, Train Loss: 0.613667368888855, Test Loss: 0.5983787178993225\n",
      "Epoch 76/100, Train Loss: 0.6127263307571411, Test Loss: 0.5982419848442078\n",
      "Epoch 77/100, Train Loss: 0.6118980050086975, Test Loss: 0.5966956615447998\n",
      "Epoch 78/100, Train Loss: 0.6113801598548889, Test Loss: 0.5959331393241882\n",
      "Epoch 79/100, Train Loss: 0.6106288433074951, Test Loss: 0.5954734086990356\n",
      "Epoch 80/100, Train Loss: 0.6096891164779663, Test Loss: 0.5941463112831116\n",
      "Epoch 81/100, Train Loss: 0.6096429824829102, Test Loss: 0.5939511656761169\n",
      "Epoch 82/100, Train Loss: 0.6085171699523926, Test Loss: 0.5927319526672363\n",
      "Epoch 83/100, Train Loss: 0.6079972982406616, Test Loss: 0.592250645160675\n",
      "Epoch 84/100, Train Loss: 0.6070359349250793, Test Loss: 0.5917356610298157\n",
      "Epoch 85/100, Train Loss: 0.6063601970672607, Test Loss: 0.5906141996383667\n",
      "Epoch 86/100, Train Loss: 0.605731189250946, Test Loss: 0.5902241468429565\n",
      "Epoch 87/100, Train Loss: 0.6054133176803589, Test Loss: 0.5892354249954224\n",
      "Epoch 88/100, Train Loss: 0.6045658588409424, Test Loss: 0.5884822607040405\n",
      "Epoch 89/100, Train Loss: 0.6039206385612488, Test Loss: 0.5880998373031616\n",
      "Epoch 90/100, Train Loss: 0.603657066822052, Test Loss: 0.587269127368927\n",
      "Epoch 91/100, Train Loss: 0.6028344631195068, Test Loss: 0.5871146321296692\n",
      "Epoch 92/100, Train Loss: 0.6023938655853271, Test Loss: 0.5859289169311523\n",
      "Epoch 93/100, Train Loss: 0.6016440987586975, Test Loss: 0.5853053331375122\n",
      "Epoch 94/100, Train Loss: 0.6005659699440002, Test Loss: 0.5850344300270081\n",
      "Epoch 95/100, Train Loss: 0.5999258160591125, Test Loss: 0.5838198661804199\n",
      "Epoch 96/100, Train Loss: 0.5996277928352356, Test Loss: 0.5838433504104614\n",
      "Epoch 97/100, Train Loss: 0.5994457006454468, Test Loss: 0.5829434394836426\n",
      "Epoch 98/100, Train Loss: 0.5985754132270813, Test Loss: 0.5823934078216553\n",
      "Epoch 99/100, Train Loss: 0.597724974155426, Test Loss: 0.5818504095077515\n",
      "Epoch 100/100, Train Loss: 0.597386360168457, Test Loss: 0.5807114243507385\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:0.7100413529848372\n",
      "Accuracy:0.68925\n",
      "Confusion Matrix:\n",
      "[[54344 28530]\n",
      " [26162 66964]]\n"
     ]
    }
   ],
   "source": [
    "# Now lets see how our mach2 model does\n",
    "# Set the size for four hidden layers and the dropout rate\n",
    "model2 = mlp.MLP_mach2(28, 300, 220, 140, 80, 0.2)\n",
    "# Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = .001\n",
    "# Lower learning rate with the adam optimizeer made the models accuracy better by over 10%\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=lr)\n",
    "# Only trained for 100 epochs because it will be too slow otherwise. We can see that the training and testing losses are still decreasing\n",
    "# so, we can use the same hyperparameters and train for more epochs on the GPU.\n",
    "n_epochs = 100\n",
    "# Train the model using our function\n",
    "train_losses, test_losses = mlp.train_model(model2, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n",
    "# Make predictions on the validation set\n",
    "f1, acc, cm = mlp.getResults(train_losses, test_losses, model2, X_val, y_val)\n",
    "print(\"F1:\" + str(f1))\n",
    "print(\"Accuracy:\" + str(acc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models I was testing got as big as the one above I moved to training using slurm jobs on City's Hyperion. This also allowed me to use a larger amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
