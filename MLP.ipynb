{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP testing notebook\n",
    "I trained the smaller models on a smaller amount of data to find optimum hyperparameters. Once the best settings were found I used this on bigger models trained with more data using a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "(563200, 28)\n",
      "(140800, 28)\n",
      "(176000, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import MLPfunctions as mlp\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a dataframe from the csv file\n",
    "dataset = pd.read_csv('HIGGS_train.csv')\n",
    "# Make the dataset much smaller\n",
    "dataset_small = dataset.sample(frac=0.1)\n",
    "# Set aside a validation set\n",
    "data_val = dataset_small.sample(frac=0.2)\n",
    "dataset_small.drop(data_val.index, inplace=True)\n",
    "X_val = data_val.iloc[:, 1:].values\n",
    "y_val = data_val.iloc[:, 0].values\n",
    "# Split the data into features and labels\n",
    "X = dataset_small.iloc[:, 1:].values\n",
    "y = dataset_small.iloc[:, 0].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Print all the sizes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train,)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.694962739944458, Test Loss: 0.6940897703170776\n",
      "Epoch 2/1000, Train Loss: 0.694060206413269, Test Loss: 0.6935638785362244\n",
      "Epoch 3/1000, Train Loss: 0.6935591101646423, Test Loss: 0.6931754946708679\n",
      "Epoch 4/1000, Train Loss: 0.6931883692741394, Test Loss: 0.6928584575653076\n",
      "Epoch 5/1000, Train Loss: 0.6928859949111938, Test Loss: 0.6925862431526184\n",
      "Epoch 6/1000, Train Loss: 0.6926265358924866, Test Loss: 0.692345380783081\n",
      "Epoch 7/1000, Train Loss: 0.6923965811729431, Test Loss: 0.692126989364624\n",
      "Epoch 8/1000, Train Loss: 0.6921877861022949, Test Loss: 0.691925585269928\n",
      "Epoch 9/1000, Train Loss: 0.6919949054718018, Test Loss: 0.6917383074760437\n",
      "Epoch 10/1000, Train Loss: 0.6918149590492249, Test Loss: 0.6915619373321533\n",
      "Epoch 11/1000, Train Loss: 0.6916449069976807, Test Loss: 0.6913941502571106\n",
      "Epoch 12/1000, Train Loss: 0.6914828419685364, Test Loss: 0.6912335753440857\n",
      "Epoch 13/1000, Train Loss: 0.6913277506828308, Test Loss: 0.6910790205001831\n",
      "Epoch 14/1000, Train Loss: 0.6911782026290894, Test Loss: 0.690929651260376\n",
      "Epoch 15/1000, Train Loss: 0.6910336017608643, Test Loss: 0.690784752368927\n",
      "Epoch 16/1000, Train Loss: 0.6908930540084839, Test Loss: 0.6906437277793884\n",
      "Epoch 17/1000, Train Loss: 0.69075608253479, Test Loss: 0.6905059814453125\n",
      "Epoch 18/1000, Train Loss: 0.6906219720840454, Test Loss: 0.6903713941574097\n",
      "Epoch 19/1000, Train Loss: 0.6904904842376709, Test Loss: 0.6902396082878113\n",
      "Epoch 20/1000, Train Loss: 0.6903617978096008, Test Loss: 0.6901103854179382\n",
      "Epoch 21/1000, Train Loss: 0.6902356147766113, Test Loss: 0.6899832487106323\n",
      "Epoch 22/1000, Train Loss: 0.6901112794876099, Test Loss: 0.6898580193519592\n",
      "Epoch 23/1000, Train Loss: 0.6899889707565308, Test Loss: 0.6897346377372742\n",
      "Epoch 24/1000, Train Loss: 0.6898679733276367, Test Loss: 0.6896126866340637\n",
      "Epoch 25/1000, Train Loss: 0.6897485852241516, Test Loss: 0.6894918084144592\n",
      "Epoch 26/1000, Train Loss: 0.6896302700042725, Test Loss: 0.689372181892395\n",
      "Epoch 27/1000, Train Loss: 0.6895129680633545, Test Loss: 0.6892538666725159\n",
      "Epoch 28/1000, Train Loss: 0.6893967390060425, Test Loss: 0.6891364455223083\n",
      "Epoch 29/1000, Train Loss: 0.6892816424369812, Test Loss: 0.6890201568603516\n",
      "Epoch 30/1000, Train Loss: 0.6891674995422363, Test Loss: 0.688904345035553\n",
      "Epoch 31/1000, Train Loss: 0.689054012298584, Test Loss: 0.6887893080711365\n",
      "Epoch 32/1000, Train Loss: 0.688941240310669, Test Loss: 0.6886751055717468\n",
      "Epoch 33/1000, Train Loss: 0.6888291835784912, Test Loss: 0.6885616183280945\n",
      "Epoch 34/1000, Train Loss: 0.6887177228927612, Test Loss: 0.6884485483169556\n",
      "Epoch 35/1000, Train Loss: 0.6886066794395447, Test Loss: 0.688336193561554\n",
      "Epoch 36/1000, Train Loss: 0.6884961724281311, Test Loss: 0.6882242560386658\n",
      "Epoch 37/1000, Train Loss: 0.6883862614631653, Test Loss: 0.6881129145622253\n",
      "Epoch 38/1000, Train Loss: 0.6882768869400024, Test Loss: 0.6880019307136536\n",
      "Epoch 39/1000, Train Loss: 0.6881676912307739, Test Loss: 0.6878911852836609\n",
      "Epoch 40/1000, Train Loss: 0.6880590319633484, Test Loss: 0.6877809762954712\n",
      "Epoch 41/1000, Train Loss: 0.687950849533081, Test Loss: 0.6876711249351501\n",
      "Epoch 42/1000, Train Loss: 0.6878429651260376, Test Loss: 0.6875614523887634\n",
      "Epoch 43/1000, Train Loss: 0.6877353191375732, Test Loss: 0.6874520182609558\n",
      "Epoch 44/1000, Train Loss: 0.6876280903816223, Test Loss: 0.6873428225517273\n",
      "Epoch 45/1000, Train Loss: 0.6875209808349609, Test Loss: 0.6872338652610779\n",
      "Epoch 46/1000, Train Loss: 0.6874141693115234, Test Loss: 0.6871249675750732\n",
      "Epoch 47/1000, Train Loss: 0.6873074173927307, Test Loss: 0.6870162487030029\n",
      "Epoch 48/1000, Train Loss: 0.6872008442878723, Test Loss: 0.6869076490402222\n",
      "Epoch 49/1000, Train Loss: 0.6870944499969482, Test Loss: 0.6867993474006653\n",
      "Epoch 50/1000, Train Loss: 0.6869882941246033, Test Loss: 0.6866911053657532\n",
      "Epoch 51/1000, Train Loss: 0.6868821978569031, Test Loss: 0.6865831613540649\n",
      "Epoch 52/1000, Train Loss: 0.686776340007782, Test Loss: 0.6864752173423767\n",
      "Epoch 53/1000, Train Loss: 0.6866706013679504, Test Loss: 0.6863674521446228\n",
      "Epoch 54/1000, Train Loss: 0.6865649223327637, Test Loss: 0.6862599849700928\n",
      "Epoch 55/1000, Train Loss: 0.6864593029022217, Test Loss: 0.6861527562141418\n",
      "Epoch 56/1000, Train Loss: 0.6863538026809692, Test Loss: 0.6860458254814148\n",
      "Epoch 57/1000, Train Loss: 0.6862486600875854, Test Loss: 0.6859388947486877\n",
      "Epoch 58/1000, Train Loss: 0.6861433982849121, Test Loss: 0.6858322024345398\n",
      "Epoch 59/1000, Train Loss: 0.6860383152961731, Test Loss: 0.6857255697250366\n",
      "Epoch 60/1000, Train Loss: 0.6859336495399475, Test Loss: 0.6856188178062439\n",
      "Epoch 61/1000, Train Loss: 0.6858288645744324, Test Loss: 0.685512363910675\n",
      "Epoch 62/1000, Train Loss: 0.685724139213562, Test Loss: 0.6854059100151062\n",
      "Epoch 63/1000, Train Loss: 0.6856195330619812, Test Loss: 0.6852996945381165\n",
      "Epoch 64/1000, Train Loss: 0.6855149269104004, Test Loss: 0.6851934790611267\n",
      "Epoch 65/1000, Train Loss: 0.6854103803634644, Test Loss: 0.6850875020027161\n",
      "Epoch 66/1000, Train Loss: 0.6853058934211731, Test Loss: 0.6849817037582397\n",
      "Epoch 67/1000, Train Loss: 0.6852016448974609, Test Loss: 0.6848759651184082\n",
      "Epoch 68/1000, Train Loss: 0.6850974559783936, Test Loss: 0.6847703456878662\n",
      "Epoch 69/1000, Train Loss: 0.6849933862686157, Test Loss: 0.6846649050712585\n",
      "Epoch 70/1000, Train Loss: 0.6848893761634827, Test Loss: 0.6845595240592957\n",
      "Epoch 71/1000, Train Loss: 0.6847856044769287, Test Loss: 0.6844541430473328\n",
      "Epoch 72/1000, Train Loss: 0.68468177318573, Test Loss: 0.6843488216400146\n",
      "Epoch 73/1000, Train Loss: 0.6845778822898865, Test Loss: 0.6842435002326965\n",
      "Epoch 74/1000, Train Loss: 0.6844740509986877, Test Loss: 0.6841385364532471\n",
      "Epoch 75/1000, Train Loss: 0.6843702793121338, Test Loss: 0.6840334534645081\n",
      "Epoch 76/1000, Train Loss: 0.6842665076255798, Test Loss: 0.6839283108711243\n",
      "Epoch 77/1000, Train Loss: 0.6841627359390259, Test Loss: 0.6838233470916748\n",
      "Epoch 78/1000, Train Loss: 0.6840589046478271, Test Loss: 0.6837185025215149\n",
      "Epoch 79/1000, Train Loss: 0.683955192565918, Test Loss: 0.6836135983467102\n",
      "Epoch 80/1000, Train Loss: 0.6838516592979431, Test Loss: 0.6835087537765503\n",
      "Epoch 81/1000, Train Loss: 0.6837482452392578, Test Loss: 0.6834039092063904\n",
      "Epoch 82/1000, Train Loss: 0.6836447715759277, Test Loss: 0.6832993030548096\n",
      "Epoch 83/1000, Train Loss: 0.6835414171218872, Test Loss: 0.6831948757171631\n",
      "Epoch 84/1000, Train Loss: 0.6834383606910706, Test Loss: 0.6830903887748718\n",
      "Epoch 85/1000, Train Loss: 0.6833353042602539, Test Loss: 0.6829860806465149\n",
      "Epoch 86/1000, Train Loss: 0.683232307434082, Test Loss: 0.6828818917274475\n",
      "Epoch 87/1000, Train Loss: 0.6831292510032654, Test Loss: 0.6827778220176697\n",
      "Epoch 88/1000, Train Loss: 0.6830264329910278, Test Loss: 0.6826738715171814\n",
      "Epoch 89/1000, Train Loss: 0.6829236745834351, Test Loss: 0.6825700402259827\n",
      "Epoch 90/1000, Train Loss: 0.6828210949897766, Test Loss: 0.6824663877487183\n",
      "Epoch 91/1000, Train Loss: 0.6827185750007629, Test Loss: 0.6823627948760986\n",
      "Epoch 92/1000, Train Loss: 0.682616114616394, Test Loss: 0.6822594404220581\n",
      "Epoch 93/1000, Train Loss: 0.6825137734413147, Test Loss: 0.6821560859680176\n",
      "Epoch 94/1000, Train Loss: 0.6824113130569458, Test Loss: 0.6820529103279114\n",
      "Epoch 95/1000, Train Loss: 0.6823089718818665, Test Loss: 0.6819496750831604\n",
      "Epoch 96/1000, Train Loss: 0.6822065114974976, Test Loss: 0.6818467378616333\n",
      "Epoch 97/1000, Train Loss: 0.6821043491363525, Test Loss: 0.681743860244751\n",
      "Epoch 98/1000, Train Loss: 0.6820020079612732, Test Loss: 0.681641161441803\n",
      "Epoch 99/1000, Train Loss: 0.6818997859954834, Test Loss: 0.6815384030342102\n",
      "Epoch 100/1000, Train Loss: 0.6817975044250488, Test Loss: 0.6814358234405518\n",
      "Epoch 101/1000, Train Loss: 0.6816953420639038, Test Loss: 0.6813334226608276\n",
      "Epoch 102/1000, Train Loss: 0.681593120098114, Test Loss: 0.6812310218811035\n",
      "Epoch 103/1000, Train Loss: 0.6814908981323242, Test Loss: 0.6811286807060242\n",
      "Epoch 104/1000, Train Loss: 0.6813888549804688, Test Loss: 0.6810263991355896\n",
      "Epoch 105/1000, Train Loss: 0.6812867522239685, Test Loss: 0.6809240579605103\n",
      "Epoch 106/1000, Train Loss: 0.6811847686767578, Test Loss: 0.6808218359947205\n",
      "Epoch 107/1000, Train Loss: 0.6810827851295471, Test Loss: 0.6807196140289307\n",
      "Epoch 108/1000, Train Loss: 0.6809808611869812, Test Loss: 0.68061763048172\n",
      "Epoch 109/1000, Train Loss: 0.6808788776397705, Test Loss: 0.6805155873298645\n",
      "Epoch 110/1000, Train Loss: 0.6807770133018494, Test Loss: 0.6804136633872986\n",
      "Epoch 111/1000, Train Loss: 0.6806750893592834, Test Loss: 0.6803116798400879\n",
      "Epoch 112/1000, Train Loss: 0.6805732250213623, Test Loss: 0.6802099347114563\n",
      "Epoch 113/1000, Train Loss: 0.6804714798927307, Test Loss: 0.6801081895828247\n",
      "Epoch 114/1000, Train Loss: 0.6803697347640991, Test Loss: 0.6800065636634827\n",
      "Epoch 115/1000, Train Loss: 0.6802679896354675, Test Loss: 0.6799049973487854\n",
      "Epoch 116/1000, Train Loss: 0.6801663637161255, Test Loss: 0.6798035502433777\n",
      "Epoch 117/1000, Train Loss: 0.6800647377967834, Test Loss: 0.6797021627426147\n",
      "Epoch 118/1000, Train Loss: 0.679963231086731, Test Loss: 0.679600715637207\n",
      "Epoch 119/1000, Train Loss: 0.679861843585968, Test Loss: 0.6794995069503784\n",
      "Epoch 120/1000, Train Loss: 0.6797605156898499, Test Loss: 0.6793982982635498\n",
      "Epoch 121/1000, Train Loss: 0.6796591877937317, Test Loss: 0.6792970299720764\n",
      "Epoch 122/1000, Train Loss: 0.6795578002929688, Test Loss: 0.6791959404945374\n",
      "Epoch 123/1000, Train Loss: 0.6794565320014954, Test Loss: 0.6790947914123535\n",
      "Epoch 124/1000, Train Loss: 0.6793551445007324, Test Loss: 0.6789936423301697\n",
      "Epoch 125/1000, Train Loss: 0.6792536973953247, Test Loss: 0.6788927912712097\n",
      "Epoch 126/1000, Train Loss: 0.6791524291038513, Test Loss: 0.6787917017936707\n",
      "Epoch 127/1000, Train Loss: 0.6790511012077332, Test Loss: 0.6786907911300659\n",
      "Epoch 128/1000, Train Loss: 0.678949773311615, Test Loss: 0.6785898804664612\n",
      "Epoch 129/1000, Train Loss: 0.6788485050201416, Test Loss: 0.678489089012146\n",
      "Epoch 130/1000, Train Loss: 0.678747296333313, Test Loss: 0.6783882975578308\n",
      "Epoch 131/1000, Train Loss: 0.6786460876464844, Test Loss: 0.6782876253128052\n",
      "Epoch 132/1000, Train Loss: 0.6785449385643005, Test Loss: 0.6781870126724243\n",
      "Epoch 133/1000, Train Loss: 0.6784438490867615, Test Loss: 0.6780864000320435\n",
      "Epoch 134/1000, Train Loss: 0.6783426403999329, Test Loss: 0.6779859662055969\n",
      "Epoch 135/1000, Train Loss: 0.678241491317749, Test Loss: 0.6778854727745056\n",
      "Epoch 136/1000, Train Loss: 0.6781403422355652, Test Loss: 0.6777849793434143\n",
      "Epoch 137/1000, Train Loss: 0.6780393719673157, Test Loss: 0.6776846647262573\n",
      "Epoch 138/1000, Train Loss: 0.6779385805130005, Test Loss: 0.6775845289230347\n",
      "Epoch 139/1000, Train Loss: 0.6778376698493958, Test Loss: 0.677484393119812\n",
      "Epoch 140/1000, Train Loss: 0.6777369379997253, Test Loss: 0.6773843765258789\n",
      "Epoch 141/1000, Train Loss: 0.6776360869407654, Test Loss: 0.6772843599319458\n",
      "Epoch 142/1000, Train Loss: 0.677535355091095, Test Loss: 0.677184522151947\n",
      "Epoch 143/1000, Train Loss: 0.6774346828460693, Test Loss: 0.6770846843719482\n",
      "Epoch 144/1000, Train Loss: 0.6773340702056885, Test Loss: 0.676984965801239\n",
      "Epoch 145/1000, Train Loss: 0.6772335767745972, Test Loss: 0.6768854260444641\n",
      "Epoch 146/1000, Train Loss: 0.6771330833435059, Test Loss: 0.6767858266830444\n",
      "Epoch 147/1000, Train Loss: 0.6770326495170593, Test Loss: 0.6766863465309143\n",
      "Epoch 148/1000, Train Loss: 0.6769323348999023, Test Loss: 0.6765868067741394\n",
      "Epoch 149/1000, Train Loss: 0.6768319606781006, Test Loss: 0.6764873266220093\n",
      "Epoch 150/1000, Train Loss: 0.6767316460609436, Test Loss: 0.6763878464698792\n",
      "Epoch 151/1000, Train Loss: 0.6766313314437866, Test Loss: 0.6762884259223938\n",
      "Epoch 152/1000, Train Loss: 0.6765310168266296, Test Loss: 0.6761889457702637\n",
      "Epoch 153/1000, Train Loss: 0.6764307022094727, Test Loss: 0.6760897636413574\n",
      "Epoch 154/1000, Train Loss: 0.6763304471969604, Test Loss: 0.6759905815124512\n",
      "Epoch 155/1000, Train Loss: 0.6762303709983826, Test Loss: 0.6758915185928345\n",
      "Epoch 156/1000, Train Loss: 0.6761302351951599, Test Loss: 0.6757925748825073\n",
      "Epoch 157/1000, Train Loss: 0.6760303974151611, Test Loss: 0.675693690776825\n",
      "Epoch 158/1000, Train Loss: 0.6759304404258728, Test Loss: 0.6755950450897217\n",
      "Epoch 159/1000, Train Loss: 0.6758307814598083, Test Loss: 0.6754965782165527\n",
      "Epoch 160/1000, Train Loss: 0.6757311820983887, Test Loss: 0.675398051738739\n",
      "Epoch 161/1000, Train Loss: 0.6756316423416138, Test Loss: 0.6752997040748596\n",
      "Epoch 162/1000, Train Loss: 0.6755321621894836, Test Loss: 0.6752012968063354\n",
      "Epoch 163/1000, Train Loss: 0.6754326224327087, Test Loss: 0.6751030683517456\n",
      "Epoch 164/1000, Train Loss: 0.6753334403038025, Test Loss: 0.6750050187110901\n",
      "Epoch 165/1000, Train Loss: 0.6752341389656067, Test Loss: 0.6749070286750793\n",
      "Epoch 166/1000, Train Loss: 0.67513507604599, Test Loss: 0.6748091578483582\n",
      "Epoch 167/1000, Train Loss: 0.6750360131263733, Test Loss: 0.6747115254402161\n",
      "Epoch 168/1000, Train Loss: 0.6749370694160461, Test Loss: 0.6746139526367188\n",
      "Epoch 169/1000, Train Loss: 0.6748381853103638, Test Loss: 0.674516499042511\n",
      "Epoch 170/1000, Train Loss: 0.6747393608093262, Test Loss: 0.6744192242622375\n",
      "Epoch 171/1000, Train Loss: 0.6746407747268677, Test Loss: 0.6743219494819641\n",
      "Epoch 172/1000, Train Loss: 0.674542248249054, Test Loss: 0.6742246150970459\n",
      "Epoch 173/1000, Train Loss: 0.6744437217712402, Test Loss: 0.6741276979446411\n",
      "Epoch 174/1000, Train Loss: 0.6743453145027161, Test Loss: 0.6740306615829468\n",
      "Epoch 175/1000, Train Loss: 0.6742470264434814, Test Loss: 0.6739338040351868\n",
      "Epoch 176/1000, Train Loss: 0.6741489171981812, Test Loss: 0.6738371253013611\n",
      "Epoch 177/1000, Train Loss: 0.6740508675575256, Test Loss: 0.673740565776825\n",
      "Epoch 178/1000, Train Loss: 0.6739528179168701, Test Loss: 0.6736442446708679\n",
      "Epoch 179/1000, Train Loss: 0.6738549470901489, Test Loss: 0.6735478639602661\n",
      "Epoch 180/1000, Train Loss: 0.6737571954727173, Test Loss: 0.6734516620635986\n",
      "Epoch 181/1000, Train Loss: 0.6736593842506409, Test Loss: 0.6733554601669312\n",
      "Epoch 182/1000, Train Loss: 0.6735618114471436, Test Loss: 0.6732596158981323\n",
      "Epoch 183/1000, Train Loss: 0.673464298248291, Test Loss: 0.6731638312339783\n",
      "Epoch 184/1000, Train Loss: 0.6733669638633728, Test Loss: 0.6730682253837585\n",
      "Epoch 185/1000, Train Loss: 0.6732697486877441, Test Loss: 0.6729727387428284\n",
      "Epoch 186/1000, Train Loss: 0.6731727719306946, Test Loss: 0.6728774309158325\n",
      "Epoch 187/1000, Train Loss: 0.673075795173645, Test Loss: 0.6727822422981262\n",
      "Epoch 188/1000, Train Loss: 0.672978937625885, Test Loss: 0.6726873517036438\n",
      "Epoch 189/1000, Train Loss: 0.6728822588920593, Test Loss: 0.6725924611091614\n",
      "Epoch 190/1000, Train Loss: 0.6727856397628784, Test Loss: 0.6724978089332581\n",
      "Epoch 191/1000, Train Loss: 0.6726891398429871, Test Loss: 0.6724032759666443\n",
      "Epoch 192/1000, Train Loss: 0.6725929379463196, Test Loss: 0.6723090410232544\n",
      "Epoch 193/1000, Train Loss: 0.6724968552589417, Test Loss: 0.672214686870575\n",
      "Epoch 194/1000, Train Loss: 0.6724007725715637, Test Loss: 0.6721208095550537\n",
      "Epoch 195/1000, Train Loss: 0.6723049283027649, Test Loss: 0.6720268726348877\n",
      "Epoch 196/1000, Train Loss: 0.6722092032432556, Test Loss: 0.6719332337379456\n",
      "Epoch 197/1000, Train Loss: 0.6721135377883911, Test Loss: 0.6718395352363586\n",
      "Epoch 198/1000, Train Loss: 0.6720179915428162, Test Loss: 0.6717460751533508\n",
      "Epoch 199/1000, Train Loss: 0.6719226241111755, Test Loss: 0.6716527342796326\n",
      "Epoch 200/1000, Train Loss: 0.6718273758888245, Test Loss: 0.6715595126152039\n",
      "Epoch 201/1000, Train Loss: 0.6717323064804077, Test Loss: 0.6714665293693542\n",
      "Epoch 202/1000, Train Loss: 0.6716374158859253, Test Loss: 0.6713736653327942\n",
      "Epoch 203/1000, Train Loss: 0.6715426445007324, Test Loss: 0.6712809801101685\n",
      "Epoch 204/1000, Train Loss: 0.6714481115341187, Test Loss: 0.6711884140968323\n",
      "Epoch 205/1000, Train Loss: 0.6713535189628601, Test Loss: 0.6710962057113647\n",
      "Epoch 206/1000, Train Loss: 0.6712592244148254, Test Loss: 0.6710039377212524\n",
      "Epoch 207/1000, Train Loss: 0.6711650490760803, Test Loss: 0.6709118485450745\n",
      "Epoch 208/1000, Train Loss: 0.6710709929466248, Test Loss: 0.6708199977874756\n",
      "Epoch 209/1000, Train Loss: 0.6709771156311035, Test Loss: 0.6707282066345215\n",
      "Epoch 210/1000, Train Loss: 0.6708834171295166, Test Loss: 0.6706365346908569\n",
      "Epoch 211/1000, Train Loss: 0.6707897186279297, Test Loss: 0.6705449819564819\n",
      "Epoch 212/1000, Train Loss: 0.6706963181495667, Test Loss: 0.6704536080360413\n",
      "Epoch 213/1000, Train Loss: 0.6706029772758484, Test Loss: 0.6703623533248901\n",
      "Epoch 214/1000, Train Loss: 0.6705098748207092, Test Loss: 0.6702712178230286\n",
      "Epoch 215/1000, Train Loss: 0.6704167723655701, Test Loss: 0.6701802611351013\n",
      "Epoch 216/1000, Train Loss: 0.6703239679336548, Test Loss: 0.6700895428657532\n",
      "Epoch 217/1000, Train Loss: 0.670231282711029, Test Loss: 0.6699989438056946\n",
      "Epoch 218/1000, Train Loss: 0.6701387763023376, Test Loss: 0.6699085235595703\n",
      "Epoch 219/1000, Train Loss: 0.6700462698936462, Test Loss: 0.6698182821273804\n",
      "Epoch 220/1000, Train Loss: 0.6699540615081787, Test Loss: 0.6697280406951904\n",
      "Epoch 221/1000, Train Loss: 0.6698620319366455, Test Loss: 0.6696381568908691\n",
      "Epoch 222/1000, Train Loss: 0.6697703003883362, Test Loss: 0.6695484519004822\n",
      "Epoch 223/1000, Train Loss: 0.6696785688400269, Test Loss: 0.6694588661193848\n",
      "Epoch 224/1000, Train Loss: 0.6695870161056519, Test Loss: 0.6693695187568665\n",
      "Epoch 225/1000, Train Loss: 0.6694955825805664, Test Loss: 0.6692802906036377\n",
      "Epoch 226/1000, Train Loss: 0.6694044470787048, Test Loss: 0.6691913604736328\n",
      "Epoch 227/1000, Train Loss: 0.6693134903907776, Test Loss: 0.6691026091575623\n",
      "Epoch 228/1000, Train Loss: 0.6692227721214294, Test Loss: 0.6690139770507812\n",
      "Epoch 229/1000, Train Loss: 0.6691320538520813, Test Loss: 0.6689257025718689\n",
      "Epoch 230/1000, Train Loss: 0.6690415740013123, Test Loss: 0.6688376665115356\n",
      "Epoch 231/1000, Train Loss: 0.6689514517784119, Test Loss: 0.6687497496604919\n",
      "Epoch 232/1000, Train Loss: 0.6688616275787354, Test Loss: 0.6686621308326721\n",
      "Epoch 233/1000, Train Loss: 0.6687718629837036, Test Loss: 0.6685746908187866\n",
      "Epoch 234/1000, Train Loss: 0.668682336807251, Test Loss: 0.6684874892234802\n",
      "Epoch 235/1000, Train Loss: 0.6685931086540222, Test Loss: 0.6684004068374634\n",
      "Epoch 236/1000, Train Loss: 0.668503999710083, Test Loss: 0.66831374168396\n",
      "Epoch 237/1000, Train Loss: 0.6684150099754333, Test Loss: 0.6682270765304565\n",
      "Epoch 238/1000, Train Loss: 0.6683263778686523, Test Loss: 0.6681407690048218\n",
      "Epoch 239/1000, Train Loss: 0.6682379841804504, Test Loss: 0.6680546402931213\n",
      "Epoch 240/1000, Train Loss: 0.6681497693061829, Test Loss: 0.6679686307907104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Train the model using our function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_losses, test_losses \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39;49mtrain_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Make predictions on the validation set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m f1, acc, cm \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39mgetResults(train_losses, test_losses, model, X_val, y_val)\n",
      "File \u001b[0;32m~/Desktop/higgsBosonDetection/MLPfunctions.py:273\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs, patience)\u001b[0m\n\u001b[1;32m    271\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred\u001b[39m.\u001b[39msqueeze(), y_train)\n\u001b[1;32m    272\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    274\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    275\u001b[0m train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "hidden_size = 50\n",
    "model = mlp.MLP_mach1(28, hidden_size)\n",
    "# Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = .1\n",
    "# Adam is the best optimizer. Adam uses momentum.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 1000\n",
    "# Train the model using our function\n",
    "train_losses, test_losses = mlp.train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n",
    "# Make predictions on the validation set\n",
    "f1, acc, cm = mlp.getResults(train_losses, test_losses, model, X_val, y_val)\n",
    "# Print the results\n",
    "print(\"F1:\" + str(f1))\n",
    "print(\"Accuracy:\" + str(acc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.6952064037322998, Test Loss: 0.6923283338546753\n",
      "Epoch 2/1000, Train Loss: 0.6924740076065063, Test Loss: 0.6915485262870789\n",
      "Epoch 3/1000, Train Loss: 0.6916215419769287, Test Loss: 0.6910988688468933\n",
      "Epoch 4/1000, Train Loss: 0.6911607384681702, Test Loss: 0.6906971335411072\n",
      "Epoch 5/1000, Train Loss: 0.6908056735992432, Test Loss: 0.6902922987937927\n",
      "Epoch 6/1000, Train Loss: 0.6906963586807251, Test Loss: 0.689930260181427\n",
      "Epoch 7/1000, Train Loss: 0.690493106842041, Test Loss: 0.6895027160644531\n",
      "Epoch 8/1000, Train Loss: 0.6901252269744873, Test Loss: 0.6889672875404358\n",
      "Epoch 9/1000, Train Loss: 0.689578115940094, Test Loss: 0.6884041428565979\n",
      "Epoch 10/1000, Train Loss: 0.6889495253562927, Test Loss: 0.6877419948577881\n",
      "Epoch 11/1000, Train Loss: 0.6883424520492554, Test Loss: 0.686891496181488\n",
      "Epoch 12/1000, Train Loss: 0.6875312924385071, Test Loss: 0.6858280897140503\n",
      "Epoch 13/1000, Train Loss: 0.6866482496261597, Test Loss: 0.6845870018005371\n",
      "Epoch 14/1000, Train Loss: 0.6856147646903992, Test Loss: 0.6832220554351807\n",
      "Epoch 15/1000, Train Loss: 0.6845471858978271, Test Loss: 0.6817702651023865\n",
      "Epoch 16/1000, Train Loss: 0.6832342743873596, Test Loss: 0.6802722215652466\n",
      "Epoch 17/1000, Train Loss: 0.681874692440033, Test Loss: 0.6785726547241211\n",
      "Epoch 18/1000, Train Loss: 0.6803520917892456, Test Loss: 0.6765124201774597\n",
      "Epoch 19/1000, Train Loss: 0.6787199378013611, Test Loss: 0.6742309331893921\n",
      "Epoch 20/1000, Train Loss: 0.6770214438438416, Test Loss: 0.6719540357589722\n",
      "Epoch 21/1000, Train Loss: 0.6750324368476868, Test Loss: 0.6696175932884216\n",
      "Epoch 22/1000, Train Loss: 0.6729639768600464, Test Loss: 0.6668593883514404\n",
      "Epoch 23/1000, Train Loss: 0.6709306240081787, Test Loss: 0.6641963124275208\n",
      "Epoch 24/1000, Train Loss: 0.6691498160362244, Test Loss: 0.6620063781738281\n",
      "Epoch 25/1000, Train Loss: 0.6670928597450256, Test Loss: 0.6590294241905212\n",
      "Epoch 26/1000, Train Loss: 0.6651780009269714, Test Loss: 0.6563419699668884\n",
      "Epoch 27/1000, Train Loss: 0.6636809706687927, Test Loss: 0.6549702882766724\n",
      "Epoch 28/1000, Train Loss: 0.6615774035453796, Test Loss: 0.6510592103004456\n",
      "Epoch 29/1000, Train Loss: 0.6608890891075134, Test Loss: 0.6517876982688904\n",
      "Epoch 30/1000, Train Loss: 0.6597084999084473, Test Loss: 0.6479150652885437\n",
      "Epoch 31/1000, Train Loss: 0.6578283309936523, Test Loss: 0.6458207368850708\n",
      "Epoch 32/1000, Train Loss: 0.6571530103683472, Test Loss: 0.6472136974334717\n",
      "Epoch 33/1000, Train Loss: 0.6566766500473022, Test Loss: 0.6430562138557434\n",
      "Epoch 34/1000, Train Loss: 0.6551989912986755, Test Loss: 0.6420383453369141\n",
      "Epoch 35/1000, Train Loss: 0.6546689867973328, Test Loss: 0.6430044770240784\n",
      "Epoch 36/1000, Train Loss: 0.6538555026054382, Test Loss: 0.640110969543457\n",
      "Epoch 37/1000, Train Loss: 0.6521120071411133, Test Loss: 0.6390872597694397\n",
      "Epoch 38/1000, Train Loss: 0.6507170796394348, Test Loss: 0.6399555802345276\n",
      "Epoch 39/1000, Train Loss: 0.6502810716629028, Test Loss: 0.6386229395866394\n",
      "Epoch 40/1000, Train Loss: 0.6494137048721313, Test Loss: 0.6373513340950012\n",
      "Epoch 41/1000, Train Loss: 0.6483868360519409, Test Loss: 0.6374419331550598\n",
      "Epoch 42/1000, Train Loss: 0.6475610136985779, Test Loss: 0.6369917392730713\n",
      "Epoch 43/1000, Train Loss: 0.6468017101287842, Test Loss: 0.6350767612457275\n",
      "Epoch 44/1000, Train Loss: 0.6460468769073486, Test Loss: 0.6340816020965576\n",
      "Epoch 45/1000, Train Loss: 0.6444867253303528, Test Loss: 0.6338629126548767\n",
      "Epoch 46/1000, Train Loss: 0.6437819600105286, Test Loss: 0.63241046667099\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Train the model using our function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_losses, test_losses \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39;49mtrain_model(model2, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Make predictions on the validation set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m f1, acc, cm \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39mgetResults(train_losses, test_losses, model2, X_val, y_val)\n",
      "File \u001b[0;32m~/Desktop/higgsBosonDetection/MLPfunctions.py:376\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs, patience)\u001b[0m\n\u001b[1;32m    374\u001b[0m train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    375\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 376\u001b[0m y_pred \u001b[39m=\u001b[39m model(X_test)\n\u001b[1;32m    377\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred\u001b[39m.\u001b[39msqueeze(), y_test)\n\u001b[1;32m    378\u001b[0m test_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/higgsBosonDetection/MLPfunctions.py:64\u001b[0m, in \u001b[0;36mMLP_mach2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[1;32m     63\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(out)\n\u001b[0;32m---> 64\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout1(out)\n\u001b[1;32m     66\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(out)\n\u001b[1;32m     67\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu2(out)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[39m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[39m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[39m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[39m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[39m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now lets see how our mach2 model does\n",
    "# Set the size for four hidden layers and the dropout rate\n",
    "model2 = mlp.MLP_mach2(28, 300, 220, 100, 30, 0.2)\n",
    "# Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = .001\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=lr)\n",
    "n_epochs = 1000\n",
    "# Train the model using our function\n",
    "train_losses, test_losses = mlp.train_model(model2, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n",
    "# Make predictions on the validation set\n",
    "f1, acc, cm = mlp.getResults(train_losses, test_losses, model2, X_val, y_val)\n",
    "print(\"F1:\" + str(f1))\n",
    "print(\"Accuracy:\" + str(acc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.692429780960083, Test Loss: 0.6921226382255554\n",
      "Epoch 2/100, Train Loss: 0.692003071308136, Test Loss: 0.6918318271636963\n",
      "Epoch 3/100, Train Loss: 0.6916661262512207, Test Loss: 0.6915619969367981\n",
      "Epoch 4/100, Train Loss: 0.691385805606842, Test Loss: 0.6913436651229858\n",
      "Epoch 5/100, Train Loss: 0.691175639629364, Test Loss: 0.6910753846168518\n",
      "Epoch 6/100, Train Loss: 0.6908677816390991, Test Loss: 0.6904028058052063\n",
      "Epoch 7/100, Train Loss: 0.6903401017189026, Test Loss: 0.6894748210906982\n",
      "Epoch 8/100, Train Loss: 0.6895642280578613, Test Loss: 0.6885915994644165\n",
      "Epoch 9/100, Train Loss: 0.6888293623924255, Test Loss: 0.6871492266654968\n",
      "Epoch 10/100, Train Loss: 0.6875709295272827, Test Loss: 0.6849786043167114\n",
      "Epoch 11/100, Train Loss: 0.6858924627304077, Test Loss: 0.6827300190925598\n",
      "Epoch 12/100, Train Loss: 0.684144139289856, Test Loss: 0.6800819039344788\n",
      "Epoch 13/100, Train Loss: 0.6816034913063049, Test Loss: 0.6781975030899048\n",
      "Epoch 14/100, Train Loss: 0.6796761155128479, Test Loss: 0.6747370958328247\n",
      "Epoch 15/100, Train Loss: 0.677074134349823, Test Loss: 0.6722671985626221\n",
      "Epoch 16/100, Train Loss: 0.6752662658691406, Test Loss: 0.6726656556129456\n",
      "Epoch 17/100, Train Loss: 0.6741607785224915, Test Loss: 0.6696013808250427\n",
      "Epoch 18/100, Train Loss: 0.6723849773406982, Test Loss: 0.6681142449378967\n",
      "Epoch 19/100, Train Loss: 0.6717822551727295, Test Loss: 0.6692076325416565\n",
      "Epoch 20/100, Train Loss: 0.6710118651390076, Test Loss: 0.6657798886299133\n",
      "Epoch 21/100, Train Loss: 0.6691287159919739, Test Loss: 0.6637853980064392\n",
      "Epoch 22/100, Train Loss: 0.6684761643409729, Test Loss: 0.6644547581672668\n",
      "Epoch 23/100, Train Loss: 0.6673941612243652, Test Loss: 0.6611163020133972\n",
      "Epoch 24/100, Train Loss: 0.6654225587844849, Test Loss: 0.6588115096092224\n",
      "Epoch 25/100, Train Loss: 0.6653587818145752, Test Loss: 0.6579550504684448\n",
      "Epoch 26/100, Train Loss: 0.6635228395462036, Test Loss: 0.6553599238395691\n",
      "Epoch 27/100, Train Loss: 0.6622856855392456, Test Loss: 0.6534762978553772\n",
      "Epoch 28/100, Train Loss: 0.6621459126472473, Test Loss: 0.6514853835105896\n",
      "Epoch 29/100, Train Loss: 0.6594722867012024, Test Loss: 0.650684654712677\n",
      "Epoch 30/100, Train Loss: 0.6590421795845032, Test Loss: 0.6486386656761169\n",
      "Epoch 31/100, Train Loss: 0.6583946347236633, Test Loss: 0.6467188596725464\n",
      "Epoch 32/100, Train Loss: 0.6565164923667908, Test Loss: 0.646965742111206\n",
      "Epoch 33/100, Train Loss: 0.6563844680786133, Test Loss: 0.6448427438735962\n",
      "Epoch 34/100, Train Loss: 0.6546534895896912, Test Loss: 0.6443068385124207\n",
      "Epoch 35/100, Train Loss: 0.6538154482841492, Test Loss: 0.643482506275177\n",
      "Epoch 36/100, Train Loss: 0.652949869632721, Test Loss: 0.6424877643585205\n",
      "Epoch 37/100, Train Loss: 0.6521214842796326, Test Loss: 0.6421909928321838\n",
      "Epoch 38/100, Train Loss: 0.6512929797172546, Test Loss: 0.6409857869148254\n",
      "Epoch 39/100, Train Loss: 0.650245726108551, Test Loss: 0.6404825448989868\n",
      "Epoch 40/100, Train Loss: 0.6499510407447815, Test Loss: 0.639698326587677\n",
      "Epoch 41/100, Train Loss: 0.6488442420959473, Test Loss: 0.6395646333694458\n",
      "Epoch 42/100, Train Loss: 0.6482715010643005, Test Loss: 0.6384895443916321\n",
      "Epoch 43/100, Train Loss: 0.6472919583320618, Test Loss: 0.6380056142807007\n",
      "Epoch 44/100, Train Loss: 0.6468879580497742, Test Loss: 0.6379449367523193\n",
      "Epoch 45/100, Train Loss: 0.6461907625198364, Test Loss: 0.6373461484909058\n",
      "Epoch 46/100, Train Loss: 0.645419716835022, Test Loss: 0.6365879774093628\n",
      "Epoch 47/100, Train Loss: 0.6451820731163025, Test Loss: 0.636164665222168\n",
      "Epoch 48/100, Train Loss: 0.64447420835495, Test Loss: 0.636096715927124\n",
      "Epoch 49/100, Train Loss: 0.6440120339393616, Test Loss: 0.6352655291557312\n",
      "Epoch 50/100, Train Loss: 0.6433143019676208, Test Loss: 0.634596586227417\n",
      "Epoch 51/100, Train Loss: 0.6427216529846191, Test Loss: 0.6343098878860474\n",
      "Epoch 52/100, Train Loss: 0.6417577266693115, Test Loss: 0.6338347792625427\n",
      "Epoch 53/100, Train Loss: 0.6415359377861023, Test Loss: 0.6329600811004639\n",
      "Epoch 54/100, Train Loss: 0.6407583355903625, Test Loss: 0.6324155330657959\n",
      "Epoch 55/100, Train Loss: 0.6396795511245728, Test Loss: 0.6321449875831604\n",
      "Epoch 56/100, Train Loss: 0.6393051147460938, Test Loss: 0.6312994956970215\n",
      "Epoch 57/100, Train Loss: 0.6388477087020874, Test Loss: 0.6305792927742004\n",
      "Epoch 58/100, Train Loss: 0.6380610466003418, Test Loss: 0.6302410960197449\n",
      "Epoch 59/100, Train Loss: 0.6379175782203674, Test Loss: 0.6297308802604675\n",
      "Epoch 60/100, Train Loss: 0.6369401812553406, Test Loss: 0.6289020776748657\n",
      "Epoch 61/100, Train Loss: 0.6365568041801453, Test Loss: 0.6284842491149902\n",
      "Epoch 62/100, Train Loss: 0.63579922914505, Test Loss: 0.6280606985092163\n",
      "Epoch 63/100, Train Loss: 0.635300874710083, Test Loss: 0.6271275877952576\n",
      "Epoch 64/100, Train Loss: 0.6348286867141724, Test Loss: 0.626590371131897\n",
      "Epoch 65/100, Train Loss: 0.6337443590164185, Test Loss: 0.6260969042778015\n",
      "Epoch 66/100, Train Loss: 0.6330113410949707, Test Loss: 0.625247597694397\n",
      "Epoch 67/100, Train Loss: 0.6327264904975891, Test Loss: 0.6246482729911804\n",
      "Epoch 68/100, Train Loss: 0.6320040822029114, Test Loss: 0.624161958694458\n",
      "Epoch 69/100, Train Loss: 0.6315147280693054, Test Loss: 0.6234285235404968\n",
      "Epoch 70/100, Train Loss: 0.6309638619422913, Test Loss: 0.6227627396583557\n",
      "Epoch 71/100, Train Loss: 0.6302257180213928, Test Loss: 0.6221886873245239\n",
      "Epoch 72/100, Train Loss: 0.6294872164726257, Test Loss: 0.6211188435554504\n",
      "Epoch 73/100, Train Loss: 0.6288608908653259, Test Loss: 0.6207298636436462\n",
      "Epoch 74/100, Train Loss: 0.6285560131072998, Test Loss: 0.6196552515029907\n",
      "Epoch 75/100, Train Loss: 0.6276445984840393, Test Loss: 0.6195723414421082\n",
      "Epoch 76/100, Train Loss: 0.6272691488265991, Test Loss: 0.6183801293373108\n",
      "Epoch 77/100, Train Loss: 0.6266207695007324, Test Loss: 0.6182718276977539\n",
      "Epoch 78/100, Train Loss: 0.6256508827209473, Test Loss: 0.6167885065078735\n",
      "Epoch 79/100, Train Loss: 0.6254684329032898, Test Loss: 0.6169271469116211\n",
      "Epoch 80/100, Train Loss: 0.6245023012161255, Test Loss: 0.6153367757797241\n",
      "Epoch 81/100, Train Loss: 0.6242976188659668, Test Loss: 0.6160715818405151\n",
      "Epoch 82/100, Train Loss: 0.6236048340797424, Test Loss: 0.6142990589141846\n",
      "Epoch 83/100, Train Loss: 0.6228394508361816, Test Loss: 0.6143100261688232\n",
      "Epoch 84/100, Train Loss: 0.6224795579910278, Test Loss: 0.6133359670639038\n",
      "Epoch 85/100, Train Loss: 0.6219679713249207, Test Loss: 0.6126407384872437\n",
      "Epoch 86/100, Train Loss: 0.6211696267127991, Test Loss: 0.6126826405525208\n",
      "Epoch 87/100, Train Loss: 0.6209079623222351, Test Loss: 0.6113021969795227\n",
      "Epoch 88/100, Train Loss: 0.620444118976593, Test Loss: 0.6110782027244568\n",
      "Epoch 89/100, Train Loss: 0.6196997761726379, Test Loss: 0.6104767322540283\n",
      "Epoch 90/100, Train Loss: 0.6192618012428284, Test Loss: 0.6096840500831604\n",
      "Epoch 91/100, Train Loss: 0.6188560724258423, Test Loss: 0.6094452142715454\n",
      "Epoch 92/100, Train Loss: 0.6185111403465271, Test Loss: 0.6085337400436401\n",
      "Epoch 93/100, Train Loss: 0.6178153157234192, Test Loss: 0.608165442943573\n",
      "Epoch 94/100, Train Loss: 0.617232084274292, Test Loss: 0.6072713732719421\n",
      "Epoch 95/100, Train Loss: 0.6169661283493042, Test Loss: 0.6069736480712891\n",
      "Epoch 96/100, Train Loss: 0.6161727905273438, Test Loss: 0.6060532927513123\n",
      "Epoch 97/100, Train Loss: 0.6158789396286011, Test Loss: 0.6058593988418579\n",
      "Epoch 98/100, Train Loss: 0.6152668595314026, Test Loss: 0.6050871014595032\n",
      "Epoch 99/100, Train Loss: 0.6149109601974487, Test Loss: 0.6045699119567871\n",
      "Epoch 100/100, Train Loss: 0.6142880320549011, Test Loss: 0.6040720343589783\n"
     ]
    }
   ],
   "source": [
    "model3 = mlp.MLP_mach3(28, 250, 200, 150, 100, 50, 10, 0.2)\n",
    "# Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = .001\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=lr)\n",
    "n_epochs = 100\n",
    "# Train the model using our function\n",
    "train_losses, test_losses = mlp.train_model(model3, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n",
    "f1, acc, cm = mlp.getResults(train_losses, test_losses, model3, X_val, y_val)\n",
    "print(\"F1:\" + str(f1))\n",
    "print(\"Accuracy:\" + str(acc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.7510489821434021, Test Loss: 0.6953123807907104\n",
      "Epoch 2/100, Train Loss: 0.7364600896835327, Test Loss: 0.7018556594848633\n",
      "Epoch 3/100, Train Loss: 0.7281211614608765, Test Loss: 0.7032963633537292\n",
      "Epoch 4/100, Train Loss: 0.7225462198257446, Test Loss: 0.7007282376289368\n",
      "Epoch 5/100, Train Loss: 0.7183631658554077, Test Loss: 0.6971570253372192\n",
      "Epoch 6/100, Train Loss: 0.7127454280853271, Test Loss: 0.6939710974693298\n",
      "Epoch 7/100, Train Loss: 0.7088937163352966, Test Loss: 0.6916778683662415\n",
      "Epoch 8/100, Train Loss: 0.7048030495643616, Test Loss: 0.6903852820396423\n",
      "Epoch 9/100, Train Loss: 0.7020050287246704, Test Loss: 0.6898725628852844\n",
      "Epoch 10/100, Train Loss: 0.6996763944625854, Test Loss: 0.6897658705711365\n",
      "Epoch 11/100, Train Loss: 0.6981785893440247, Test Loss: 0.6897735595703125\n",
      "Epoch 12/100, Train Loss: 0.6964243054389954, Test Loss: 0.6897538900375366\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Train the model using our function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_losses, test_losses \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39;49mtrain_model(model4, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m f1, acc, cm \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39mgetResults(train_losses, test_losses, model4, X_val, y_val)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tylerliddell/Desktop/higgsBosonDetection/MLP.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mF1:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(f1))\n",
      "File \u001b[0;32m~/Desktop/higgsBosonDetection/MLPfunctions.py:366\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs, patience)\u001b[0m\n\u001b[1;32m    364\u001b[0m train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    365\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 366\u001b[0m y_pred \u001b[39m=\u001b[39m model(X_test)\n\u001b[1;32m    367\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred\u001b[39m.\u001b[39msqueeze(), y_test)\n\u001b[1;32m    368\u001b[0m test_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/higgsBosonDetection/MLPfunctions.py:240\u001b[0m, in \u001b[0;36mMLP_mach4.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout5:\n\u001b[1;32m    238\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout5(out)\n\u001b[0;32m--> 240\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc6(out)\n\u001b[1;32m    241\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu6(out)\n\u001b[1;32m    242\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout6:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neco/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now lets see how our mach4 model does\n",
    "# Set the size for four hidden layers and the dropout rate\n",
    "reload(mlp)\n",
    "model4 = mlp.MLP_mach4(28, 150, 120, 90, 60, 30, 10, .2)\n",
    "# Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = .001\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=lr)\n",
    "n_epochs = 100\n",
    "# Train the model using our function\n",
    "train_losses, test_losses = mlp.train_model(model4, X_train, y_train, X_test, y_test, criterion, optimizer, n_epochs)\n",
    "f1, acc, cm = mlp.getResults(train_losses, test_losses, model4, X_val, y_val)\n",
    "print(\"F1:\" + str(f1))\n",
    "print(\"Accuracy:\" + str(acc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
